<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>On the role of dopamine in motivated behavior : a neuro-computational approach - 1&nbsp; Introduction</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./2-JOCN.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./1-Introduction.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./img/tuc.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">On the role of dopamine in motivated behavior : a neuro-computational approach</a> 
        <div class="sidebar-tools-main">
    <a href="./thesis.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Abstract</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1-Introduction.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-JOCN.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Sustained activities and retrieval in a computational model of perirhinal cortex</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-FICN.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A computational model of basal ganglia and its role in memory retrieval in rewarded visual memory tasks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4-NN.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Working memory and response selection: A computational account of interactions among cortico-basal ganglio-thalamic loops</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5-FINR.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Timing and expectation of reward: a neuro-computational model of the afferents to the ventral tegmental area</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6-FINI.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">ANNarchy: a code generation approach to neural simulations on parallel hardware</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./References.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#computational-neuroscience" id="toc-computational-neuroscience" class="nav-link active" data-scroll-target="#computational-neuroscience"><span class="header-section-number">1.1</span> Computational neuroscience</a></li>
  <li><a href="#motivated-behavior" id="toc-motivated-behavior" class="nav-link" data-scroll-target="#motivated-behavior"><span class="header-section-number">1.2</span> Motivated behavior</a></li>
  <li><a href="#reward-and-the-dopaminergic-system" id="toc-reward-and-the-dopaminergic-system" class="nav-link" data-scroll-target="#reward-and-the-dopaminergic-system"><span class="header-section-number">1.3</span> Reward and the dopaminergic system</a></li>
  <li><a href="#basal-ganglia-and-reinforcement-learning" id="toc-basal-ganglia-and-reinforcement-learning" class="nav-link" data-scroll-target="#basal-ganglia-and-reinforcement-learning"><span class="header-section-number">1.4</span> Basal ganglia and reinforcement learning</a></li>
  <li><a href="#multiple-loops-and-organization-of-behavior" id="toc-multiple-loops-and-organization-of-behavior" class="nav-link" data-scroll-target="#multiple-loops-and-organization-of-behavior"><span class="header-section-number">1.5</span> Multiple loops and organization of behavior</a></li>
  <li><a href="#structure-of-the-thesis-and-contribution" id="toc-structure-of-the-thesis-and-contribution" class="nav-link" data-scroll-target="#structure-of-the-thesis-and-contribution"><span class="header-section-number">1.6</span> Structure of the thesis and contribution</a>
  <ul class="collapse">
  <li><a href="#list-of-publications-included-in-the-thesis" id="toc-list-of-publications-included-in-the-thesis" class="nav-link" data-scroll-target="#list-of-publications-included-in-the-thesis">List of publications included in the thesis</a></li>
  <li><a href="#contribution-to-each-article" id="toc-contribution-to-each-article" class="nav-link" data-scroll-target="#contribution-to-each-article">Contribution to each article</a></li>
  <li><a href="#chapter-2-perirhinal-cortex-and-dopamine" id="toc-chapter-2-perirhinal-cortex-and-dopamine" class="nav-link" data-scroll-target="#chapter-2-perirhinal-cortex-and-dopamine"><span class="header-section-number">1.6.1</span> Chapter 2 : Perirhinal cortex and dopamine</a></li>
  <li><a href="#chapter-3-basal-ganglia-and-memory-retrieval" id="toc-chapter-3-basal-ganglia-and-memory-retrieval" class="nav-link" data-scroll-target="#chapter-3-basal-ganglia-and-memory-retrieval"><span class="header-section-number">1.6.2</span> Chapter 3 : Basal ganglia and memory retrieval</a></li>
  <li><a href="#chapter-4-wm-and-multiple-basal-ganglia-loops" id="toc-chapter-4-wm-and-multiple-basal-ganglia-loops" class="nav-link" data-scroll-target="#chapter-4-wm-and-multiple-basal-ganglia-loops"><span class="header-section-number">1.6.3</span> Chapter 4 : WM and multiple basal ganglia loops</a></li>
  <li><a href="#chapter-5-timing-and-expectation-of-reward" id="toc-chapter-5-timing-and-expectation-of-reward" class="nav-link" data-scroll-target="#chapter-5-timing-and-expectation-of-reward"><span class="header-section-number">1.6.4</span> Chapter 5 : Timing and expectation of reward</a></li>
  <li><a href="#chapter-6-neural-simulator-annarchy" id="toc-chapter-6-neural-simulator-annarchy" class="nav-link" data-scroll-target="#chapter-6-neural-simulator-annarchy"><span class="header-section-number">1.6.5</span> Chapter 6 : Neural simulator ANNarchy</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">1.7</span> Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec:Introduction" class="quarto-section-identifier"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></h1>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<p><span class="citation" data-cites="Minsky1968">Minsky (<a href="References.html#ref-Minsky1968" role="doc-biblioref">1968</a>)</span> defined the field of Artificial Intelligence (AI) as “the science of making machines do things that would require intelligence if done by humans”. Over its almost 70 years of existence - John MacCarthy invented the term in 1956 -, AI has achieved a lot of progress in specialized areas such as data-mining, machine learning, computer vision, speech recognition or even single cognitive tasks such as chess playing or medical diagnosis. Weak (or applied) AI indeed focuses on methods allowing to solve specific tasks which either necessitate a limited range of human intellectual abilities (e.g.&nbsp;recognizing objects) or even have nothing to do with human intelligence (e.g.&nbsp;search engines). Although these improvements have proven very useful, especially in an industrial context, the real goal of AI - called strong AI by <span class="citation" data-cites="Searle1980">Searle (<a href="References.html#ref-Searle1980" role="doc-biblioref">1980</a>)</span> - is to obtain systems with a general form of intelligence that could be compared with human intelligence on complex behaviors. Despite recent advances in machine learning techniques <span class="citation" data-cites="LeCun2015">(e.g.&nbsp;deep learning, <a href="References.html#ref-LeCun2015" role="doc-biblioref">LeCun et al., 2015</a>)</span> and prophetic claims that the singularity is approaching <span class="citation" data-cites="Kurzweil2005">(<a href="References.html#ref-Kurzweil2005" role="doc-biblioref">Kurzweil, 2005</a>)</span>, one has to admit that strong AI has basically failed until now <span class="citation" data-cites="Velik2012">(<a href="References.html#ref-Velik2012" role="doc-biblioref">Velik, 2012</a>)</span>. As Marvin Minsky noticed, all we have is a collection of “dumb” specialists which perform single tasks very well - deep neural networks exceed for example human performance on certain visual recognition tasks - but which, when put together, do not even get close to the cognitive abilities of a rodent. Robotic competitions such as RoboCup are good demonstrators of the limits of strong AI.</p>
<p>Many cognitive architectures for strong AI have been proposed over the years <span class="citation" data-cites="Langley2009">(for a review, see <a href="References.html#ref-Langley2009" role="doc-biblioref">Langley et al., 2009</a>)</span>. They usually take the form of conversational agents, virtual reality avatars or robotic platforms, although they may also be used in specific applications. They can be classified generally into two approaches: the symbolic (or cognitivist) approach, which breaks human intelligence into functional components - e.g.&nbsp;attention, long-term memory, sensory processes - and implements each of them with particular symbolic algorithms - production rules, tree searches; and the connectionnist (or emergentist) approach which considers distributed systems of functional units (often in the form of neural networks) which interact with each other and learn to perform a task through interacting with an environment. Although the behavior of a symbolic system is easier to analyze, its suitability for real-world problems is problematic: if breaking a task into elementary components makes sense for symbolic problems such as the game of chess, it becomes much harder for recognizing a face, engaging a conversation appropriately or even playing football. The main issue here is symbol-grounding: while manipulating the concept of a cup or a ball is easy for a computer, it is much harder to relate this concept to the visual perception of a cup or ball with any possible shape, under various lightning conditions or orientations. This explains why symbolic cognitive architectures have mostly failed to produce interesting behaviors outside restricted lab settings. An additional difficulty is the amount of work required to create the cognitive architecture: each module of the system must communicate symbols adequately to the others, which in turn should be able to cope with potential failures. The resulting architecture becomes quickly tuned to a particular problem, and any significant change in the environmental conditions may require to redevelop the whole system.</p>
<p>The connectionnist approach relies heavily on learning to exhibit the desired cognitive functions. Contrary to symbolic architectures, the desired function is not hard-coded in the system but rather emerges from the interaction of multiple units after learning. An example is artificial neural networks, where neurons communicate with each other through connections whose weights evolve with learning: the function performed arises from this interaction, not from the structure of the network itself. The same network can for example learn to perform many different functions, depending on its interaction with the task. The computational properties of neural networks are heavily used in weak AI, especially in machine learning. The drawback of this decoupling between the function and the underlying structure is that it becomes complicated to create complex cognitive architectures: the communication between different modules is not symbolic anymore, but numerical - the activity of a population of neurons. Psychological models of cognition, using generic modules such as planning or long-term memory, do not map easily on a connectionnist substrate.</p>
<p>To overcome this problem, a promising direction for AI is to get inspiration from the only truly intelligent system known to date: the brain. The brain has intrinsically a connectionnist structure: it is composed of hundreds of billions of neurons, communicating with each other through synapses which undergo plasticity based on experience. The core idea of brain-like AI (or brain-inspired AI) is to study how the brain exhibits natural intelligence and extract the necessary mechanisms to reproduce it in an artificial system. <span class="citation" data-cites="Velik2012">Velik (<a href="References.html#ref-Velik2012" role="doc-biblioref">2012</a>)</span> defined the basic dogma of brain-like AI as such:</p>
<blockquote class="blockquote">
<p>It is well appreciated that the human brain is the most sophisticated, powerful, efficient, effective, flexible and intelligent information processing system known. Therefore, the functioning of the human brain, its structural organization, and information processing principles should be used as archetype for designing artificial intelligent systems instead of just emulating its behavior in a black box manner. To achieve this, approaches should not build on work from engineers only but on a close cooperation between engineers and brain scientists.</p>
</blockquote>
<p>Brain-like AI covers a variety of approaches, from top-down models simulating the functions of particular brain areas using non-brain-inspired implementations (e.g.&nbsp;Bayesian models for decision making), to bottom-up models simulating with great detail specific brain areas, but without any relationship to their function (e.g.&nbsp;Human Brain Project). The neuro-computational models presented in this thesis aim at finding a middle ground between these approaches: address the problem of intelligence at the functional level (the models should be useful at the end), while keeping the biological realism high enough to explain and predict biological neural mechanisms. This link between function and structure in the brain is the fundamental question of <em>computational neuroscience</em>, which partly overlaps with brain-like AI. Neuro-computational models in this field are by design close to the architecture of the brain; insights from neuroscience on motivated behavior can be rapidly integrated to improve both their plausibility and performance. They furthermore provide an unique way to investigate new computing paradigms.</p>
<section id="computational-neuroscience" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="computational-neuroscience"><span class="header-section-number">1.1</span> Computational neuroscience</h2>
<section id="aim" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="aim">Aim</h4>
<p>The ambition of computational neuroscience is to bring a computational modeling approach to the interdisciplinary field of neuroscience, aiming concurrently at an explicative role - explaining why the brain behaves in the observed way - and a predictive one - suggesting previously unobserved effects which can be tested. The main challenge is to integrate experimental observations from different levels of description (neuroanatomy, neurochemistry, neurophysiology, neuro-imaging, cognitive science and behavioral studies) into a biologically realistic neural network. Numerical simulations of this model allow to reproduce the underlying observations in a systematic way and to better analyze, interpret and understand the available data. Conversely, predictions can be made based on these simulations, guiding experimentalists in the design of their experiments (theory-driven neuroscience).</p>
<p>Although the term was only first coined by Eric L. Schwartz in 1985, the first examples of computational neuroscience work may be the invention of the integrate-and-fire neuron by <span class="citation" data-cites="Lapicque1907">Lapicque (<a href="References.html#ref-Lapicque1907" role="doc-biblioref">1907</a>)</span> and the complete mathematical characterization of the initiation and propagation of action potentials in the squid giant axon by <span class="citation" data-cites="Hodgkin1952">Hodgkin and Huxley (<a href="References.html#ref-Hodgkin1952" role="doc-biblioref">1952</a>)</span>. Research in computational neuroscience has long focused mostly on characterizing the dynamics of individual neurons or small assemblies. The focus has now shifted toward large-scale models, either at the systems level where functional networks involved in particular processes are investigated <span class="citation" data-cites="Hamker2004a Dranias2008">(<a href="References.html#ref-Dranias2008" role="doc-biblioref">Dranias et al., 2008</a>; <a href="References.html#ref-Hamker2004a" role="doc-biblioref">Hamker, 2004</a>)</span>, or at the detailed biological level, with the goal of simulating complete brain areas, as in the Blue Brain Project <span class="citation" data-cites="Markram2006">(<a href="References.html#ref-Markram2006" role="doc-biblioref">Markram, 2006</a>)</span>, or even the whole brain in its follower Human Brain Project (HBP). Neuro-computational models have virtually addressed over the years all brain structures and functions, including vision in the occipital and temporal lobes <span class="citation" data-cites="Rolls2001">(<a href="References.html#ref-Rolls2001" role="doc-biblioref">Rolls and Deco, 2001</a>)</span>, working memory in the prefrontal cortex and basal ganglia <span class="citation" data-cites="frank2001 Schroll2012">(<a href="References.html#ref-frank2001" role="doc-biblioref">Frank et al., 2001</a>; <a href="References.html#ref-Schroll2012" role="doc-biblioref">Schroll et al., 2012</a>)</span>, long-term memory formation in the hippocampal formation <span class="citation" data-cites="Burgess2007">(<a href="References.html#ref-Burgess2007" role="doc-biblioref">Burgess et al., 2007</a>)</span> or motor learning in the cerebellum <span class="citation" data-cites="albus1971">(<a href="References.html#ref-albus1971" role="doc-biblioref">Albus, 1971</a>)</span>.</p>
</section>
<section id="neuro-computational-models" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="neuro-computational-models">Neuro-computational models</h4>
<p>Neuro-computational models typically study the interaction of different brain areas. Each area comprises a certain number of artificial neurons, which are modeled differently depending on the physiological properties of the corresponding biological neuron (pyramidal neuron, medium spiny neuron, basket cell, etc) and the neuro-transmitter they use (e.g.&nbsp;AMPA, NMDA, GABA). Many models however use only two types of neurons: excitatory and inhibitory neurons. When active, excitatory neurons increase the firing rate of neurons receiving synapses from them, while inhibitory neurons decrease it. One of the simplest -although powerful - neuron model is the <em>rate-coded</em> neuron, which computes an instantaneous firing rate (corresponding to the frequency of spike emission at a given time <span class="math inline">t</span>) and exchange it with other neurons. A rate-coded neuron is described by an ordinary differential equation (ODE), which can be of the form:</p>
<p><span class="math display">
    \tau \cdot \frac{d r(t)}{dt} + r(t) = \sum_{i \in \text{Exc}} w_i \cdot r_i(t) - \sum_{j \in \text{Inh}} w_j \cdot r_j(t) + B
</span></p>
<p><span class="math inline">r(t)</span> is the instantaneous firing rate of a single neuron, <span class="math inline">\tau</span> the time constant defining the speed of its dynamics and <span class="math inline">B</span> its baseline activity (the firing rate it has without inputs). Inputs are represented by the weighted sums, where each connection to the neuron (a synapse) has a weight <span class="math inline">w</span> (also called the synaptic efficiency) which multiplies the firing rate of the corresponding pre-synaptic neuron. Two sums are represented here (corresponding to excitatory and inhibitory synapses, as denoted by their respective positive and negative signs), but more complex relationships can be used. For example, modulatory inputs can multiply globally a weighted sum of excitatory inputs. Additionally, a transfer function can be used to restrict the firing rate to positive values, or implement non-linear effects.</p>
<p>If the description of a single neuron is relatively simple, the computational power of a neuro-computational model comes from the interconnection of several populations of neurons, allowing the emergence of complex functions. The projection of a population on another can be dense (all-to-all, i.e.&nbsp;each neuron in the post-synaptic population has a synapse with every neuron in the pre-synaptic one) or sparse (a synapse exists according to a fixed probability or some more complex rule). Moreover, <em>synaptic plasticity</em> allows to modify the weights <span class="math inline">w</span> of a projection based on the activity of the neurons, forming the basis of learning in a neural network. The simplest and most famous rule for synaptic plasticity is the Hebbian learning rule <span class="citation" data-cites="Hebb1949">(<a href="References.html#ref-Hebb1949" role="doc-biblioref">Hebb, 1949</a>)</span>, which states that the weight of a synapse increases when both pre- and post-synaptic neurons are active at the same time (correlation-based learning rule):</p>
<p><span class="math display">
    \Delta w = \eta \cdot r^\text{pre} \cdot r^\text{post}
</span></p>
<p>where <span class="math inline">w</span> is the weight of the synapse, <span class="math inline">\eta</span> a learning rate defining the speed of learning, <span class="math inline">r^\text{pre}</span> and <span class="math inline">r^\text{post}</span> the instantaneous firing rate of the pre- and post-synaptic neurons, respectively. The disadvantage of this rule being that the weights would increase infinitely, several variants have since been introduced, among which the Oja learning rule <span class="citation" data-cites="Oja1982">(which adds a regularization term to keep the sum of weights coming to a neuron constant, <a href="References.html#ref-Oja1982" role="doc-biblioref">Oja, 1982</a>)</span> or the Bienenstock-Cooper-Munro rule<span class="citation" data-cites="Bienenstock1982">(BCM, modeling both long-term potentiation - LTP, weight increase - and long-term depression - LTD, weight decrease - <a href="References.html#ref-Bienenstock1982" role="doc-biblioref">Bienenstock et al., 1982</a>)</span>, as well as rules modeling the modulatory influence of dopamine on synaptic plasticity. Although there is flexibility in the choice of the rules, a hard constraint to obtain a biologically-realistic model is that all the information needed by the rule should be local to the synapse: the weight change can only depend on variables of the pre- and post-synaptic neurons, but not other neurons. Many classical machine learning algorithms such as backpropagation <span class="citation" data-cites="rumelhart1986">(<a href="References.html#ref-rumelhart1986" role="doc-biblioref">Rumelhart et al., 1986</a>)</span> can not be used in this context.</p>
</section>
<section id="computer-science" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="computer-science">Computer science</h4>
<p>The cross-fertilization between computational neuroscience and artificial intelligence is well documented. As explained later, reinforcement learning, a subfield of machine learning <span class="citation" data-cites="Sutton1998">(<a href="References.html#ref-Sutton1998" role="doc-biblioref">Sutton and Barto, 1998</a>)</span>, has been successfully used to interpret the patterns of activity in dopaminergic areas during classical conditioning <span class="citation" data-cites="schultz1998">(<a href="References.html#ref-schultz1998" role="doc-biblioref">Schultz, 1998</a>)</span>. As dopamine modulates processing and learning in many brain areas, including the prefrontal cortex and the basal ganglia, this theoretical consideration has radically changed the interpretation of their role in various processes such as motor learning, action selection, working memory or decision-making. This analogy is still widely used by experimentalists and clinicians to interpret their observations, although several computational neuroscientists have since proposed more detailed and realistic neuro-computational models of the dopaminergic system <span class="citation" data-cites="Brown1999 OReilly2006 Vitay2014">(<a href="References.html#ref-Brown1999" role="doc-biblioref">Brown et al., 1999</a>; <a href="References.html#ref-OReilly2006" role="doc-biblioref">O’Reilly and Frank, 2006</a>; <a href="References.html#ref-Vitay2014" role="doc-biblioref">Vitay and Hamker, 2014</a>)</span>.</p>
<p>On the other hand, deep learning networks <span class="citation" data-cites="LeCun2015">(<a href="References.html#ref-LeCun2015" role="doc-biblioref">LeCun et al., 2015</a>)</span> are directly derived from computational neuroscience research. The basic structure of a deep learning network for visual recognition is mapped onto the hierarchical organization of the visual cortex, with lower-levels areas extracting simple and local features from the retinal image (edges, gradients), and higher-level areas combining these lower features into complex shapes or even objects <span class="citation" data-cites="Lecun1998">(<a href="References.html#ref-Lecun1998" role="doc-biblioref">Lecun et al., 1998</a>)</span>. The most successful deep learning architectures make also use of sparseness as a regularization method to ensure an efficient coding of visual features, a concept which was first extensively studied by computational neuroscientists <span class="citation" data-cites="Olshausen1997 spratling1999 Wiltschut2009">(<a href="References.html#ref-Olshausen1997" role="doc-biblioref">Olshausen and Field, 1997</a>; <a href="References.html#ref-spratling1999" role="doc-biblioref">Spratling, 1999</a>; <a href="References.html#ref-Wiltschut2009" role="doc-biblioref">Wiltschut and Hamker, 2009</a>)</span>. Dropout, a regularization technique used to improve generalization in deep networks <span class="citation" data-cites="Srivastava2014">(<a href="References.html#ref-Srivastava2014" role="doc-biblioref">Srivastava et al., 2014</a>)</span>, is inspired from computational studies of stochastic synaptic transmission <span class="citation" data-cites="Maass1999">(<a href="References.html#ref-Maass1999" role="doc-biblioref">Maass and Zador, 1999</a>)</span>.</p>
</section>
<section id="challenges" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="challenges">Challenges</h4>
<p>Several issues are faced by computational neuroscience. The first one is the everlasting controversy on the adequate level of description to explain brain processes. Some models can be very detailed, using a model of the 3D morphology of specific neurons and a detailed description of chemical processes occurring inside the synapses. This <em>bottom-up</em> approach, exemplified by the Human Brain Project, relies heavily on data analysis to find the correct parameters and replicate observations. There is virtually no end to the degree of details that can be incorporated in such models. Despite its ambitious nature on this issue, one of the major criticisms addressed to HBP is that the level of description they chose will not be sufficient to capture all the properties of brain functioning. Contrary to physics or chemistry, neuroscience (including computational neuroscience) is non-paradigmatic in the sense of Thomas Kuhn <span class="citation" data-cites="Kuhn1962">(<a href="References.html#ref-Kuhn1962" role="doc-biblioref">Kuhn, 1962</a>)</span>: there is no common agreement inside the community on common axiomatic principles or models that could be used as a framework to interpret observations. Based on the enormous amount of unexplained experiments, the different schools of thought can select observations that fit into their paradigm and reject the ones that do not, leading to endless debates. Neuroscience is still in its infancy as a science, but the hope of defining a unified theory of brain functioning has to be maintained.</p>
<p>A more serious criticism to the <em>bottom-up</em> approach is that reproducing neural activity does not obligatorily mean to understand it. A complete simulated model of the brain, up to the last molecule involved, may end up as difficult to analyze and understand as an actual brain. What makes a human brain so special is not its number of neurons, nor its variety of cell types and neurotransmitters, but its different levels of organization: the complex and dynamical interaction between biological structures at different scales. The <em>top-down</em> approach to computational neuroscience starts from the behavioral function and breaks it iteratively into functional blocks that may eventually map onto the biological substrate. The corresponding models can be high-level mathematical descriptions, such as Bayesian inference <span class="citation" data-cites="Doya2006">(<a href="References.html#ref-Doya2006" role="doc-biblioref">Doya et al., 2006</a>)</span>, free-energy minimization <span class="citation" data-cites="Friston2010">(<a href="References.html#ref-Friston2010" role="doc-biblioref">Friston, 2010</a>)</span> or optimization techniques <span class="citation" data-cites="Sutton1998">(<a href="References.html#ref-Sutton1998" role="doc-biblioref">Sutton and Barto, 1998</a>)</span>, while others use simplified neural models (spiking point-neurons, rate-coded neurons) to capture essential computational properties of neural networks. Top-down computational models obviously need to make strong assumptions about the underlying biological substrates and can only explain a limited range of observations. The whole difficulty is to define precisely enough the validity of the model: what can this model explain and predict, and where are its limits. For this kind of models, the key aspect is the ability to make predictions: they usually have enough degrees of freedom to fit virtually any set of experimental data, so their plausibility can only be evaluated by their predictive power.</p>
<p>A second problem faced by computational neuroscience is scalability. As neuro-computational models grow in size, the computational load to run the simulation becomes critical. The human brain comprises around 100 billions of neurons and tenths of trillions synapses. Even when using simple neural and synaptic models, the number of operations per second and the amount of memory needed by a complete brain model exceed the power of current supercomputers. The Human Brain Project has estimated that a complete brain model would require computational power at the exascale (one exaflops - <span class="math inline">10^{18}</span> - and 100 petabytes of memory) in order to function in real-time, while the fastest supercomputer at this date only proposes 50 petaflops (<span class="math inline">5 \cdot 10^{16}</span>) of peak performance. The resulting simulation would consume 1.5 GW of energy if today’s architectures were simply scaled up (the goal is to reduce it to 20 MW by 2020), while the human brain merely requires 30W on average. On the short term, there is obviously a need for applying state-of-the-art parallel computing methods to the simulation of neuro-computational models. Several parallel neural simulators exist (NEST, GeNN, Brian, ANNarchy, etc) but they are usually limited to a particular type of neural models and on specific hardware platforms. On the longer term, one may need to rethink computer architectures: neural networks are inherently parallel, with localized processing units - the neurons - interacting through connections - the synapses - in continuous time. Simulating these networks on serial von Neumann architectures, even with many cores, is probably a waste of resources. Dedicated neuromorphic hardware solutions are being developed for the simulation of large-scale neural networks, for example the Spinnaker <span class="citation" data-cites="Rast2011">(<a href="References.html#ref-Rast2011" role="doc-biblioref">Rast et al., 2011</a>)</span> and BrainScaleS <span class="citation" data-cites="Fieres2008">(<a href="References.html#ref-Fieres2008" role="doc-biblioref">Fieres et al., 2008</a>)</span> projects, or the IBM SyNAPSE (Systems of Neuromorphic Adaptive Plastic Scalable Electronics) chip. They rely on fundamentally different concepts, such as asynchronous and event-driven computations, what leads to fast and energy-efficient simulations. However, these neuromorphic hardware platforms are not commonly available yet and require a strong programming effort.</p>
<p>Despite the different issues inherent to the youth of the field, computational neuroscience is a promising approach to artificial intelligence. It allows to bridge the gap between the quickly expanding knowledge on cognitive and emotional processes involved in behavior and the design of flexible and robust algorithms for intelligent behaving systems.</p>
</section>
</section>
<section id="motivated-behavior" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="motivated-behavior"><span class="header-section-number">1.2</span> Motivated behavior</h2>
<section id="animal-behavior" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="animal-behavior">Animal behavior</h4>
<p>Animal behavior can be decomposed into four categories: reflexes (low-level motor responses to stimuli which can not be voluntarily controlled), Pavlovian responses (the acquired association between a stimulus and an outcome, leading to conditioned responses), habits (more or less complex sequences of thoughts or actions which are routinely executed when triggered in a specific context) and goal-directed behavior (or motivated behavior, the ability to perform actions in order to achieve a particular goal) <span class="citation" data-cites="Balleine1998">(<a href="References.html#ref-Balleine1998" role="doc-biblioref">Balleine and Dickinson, 1998</a>)</span>. Pavlovian (or classical) conditioning is a passive learning process: an initially neutral stimulus (conditioned stimulus, CS) is repeatedly paired with a meaningful stimulus (unconditioned stimulus, US, which can be either positive - reward - or negative - punishment). The unconditioned response (UR) usually associated to the US becomes after a variable number of trials associated to the CS, becoming a conditioned response (CR). The classical experiment of Pavlov used a tone (CS) to predict the delivery of food (US) associated with drooling (CR). This form of conditioning does not require any action to be acquired, but can be used to adapt behavior by signaling the relevance of sensory events to higher-level functions. In appetitive conditioning, where the US is a food reward, the appearance of the CS prepares the animal to consumption, mainly through drooling but also possibly by interrupting the current behavior. In fear conditioning, where the US is a painful stimulation, the CS may trigger avoidance behaviors.</p>
<p>Oppositely, habits and goal-directed behavior are two components of instrumental (or operant) conditioning: the term covers all the processes which lead an animal to <em>learn</em> to produce actions in order to obtain rewards (positive reinforcers) or avoid punishments (negative reinforcers) <span class="citation" data-cites="Thorndike1911 Skinner1938">(<a href="References.html#ref-Skinner1938" role="doc-biblioref">Skinner, 1938</a>; <a href="References.html#ref-Thorndike1911" role="doc-biblioref">Thorndike, 1911</a>)</span>. While in Pavlovian conditioning the animal merely observes relationships in its environment, in operant conditioning it has control over the occurrence of reinforcers by adapting its behavior both during the learning phase and the exploitation phase. Operant conditioning is the key process in educating animals (for example teaching a dog new tricks by rewarding him after each successful action), but is also fundamental in free behavior: actions are directed toward the achievement of goals. Achieving a goal is a positive reinforcer for behavior, increasing the probability to achieve it again in the future, while failing to do so is a negative reinforcer which forces to adapt the current strategy or find a new one.</p>
<p>Although they are both directed toward goals, the difference between habits and goal-directed behavior is their dependency on the value of the goal. A classical experiment is the devaluation task: when the value of the reward is suddenly decreased (for example by inducing satiety before the experiment), goal-directed processes quickly avoid this outcome, while habitual behavior can persist for a long period of time <span class="citation" data-cites="Balleine1998">(<a href="References.html#ref-Balleine1998" role="doc-biblioref">Balleine and Dickinson, 1998</a>)</span>. Habits are therefore stimulus-response (S-R) mechanisms (a stimulus can trigger the behavior, even when the goal is not interesting anymore) while goal-directed processes are based on action-outcome (A-O) associations (which action do I need to perform to obtain this particular outcome?). The transfer of a goal-directed behavior to the habitual system is possible when the association is repeatedly experienced over an extended period of time. The mechanisms underlying this transfer are not yet fully understood, but they are thought to play an important role in the development of addiction <span class="citation" data-cites="Everitt2001">(<a href="References.html#ref-Everitt2001" role="doc-biblioref">Everitt et al., 2001</a>)</span>.</p>
</section>
<section id="explicit-vs.-implicit-motivation" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="explicit-vs.-implicit-motivation">Explicit vs.&nbsp;implicit motivation</h4>
<p>Goals can be extrinsically defined, for example when some food item is available in the environment. If the value of such a goal, possibly previously estimated through classical conditioning processes, exceeds sufficiently the costs associated to obtaining it, the animal engages in a series of actions that may lead to its obtainment, in which case these actions are reinforced. This form of operant conditioning is also called <em>reinforcement learning</em>, which is an important issue for both psychology and computer science. However, animals do not only produce actions which are directed toward primary reinforcers such as food, water or sexual partners: they play with their fellows or they explore their environment without any obvious reason for an external observer. The goals of such actions are called intrinsic rewards: satisfying one’s curiosity, checking if one’s beliefs are true, engaging in social interactions are as important from an evolutionary point of view as ensuring food supplies, reproduction or shelter <span class="citation" data-cites="Kaplan2007 Barto2013">(<a href="References.html#ref-Barto2013" role="doc-biblioref">Barto et al., 2013</a>; <a href="References.html#ref-Kaplan2007" role="doc-biblioref">Kaplan and Oudeyer, 2007</a>)</span>.</p>
<p>Extrinsic and intrinsic rewards are at the core of motivated behavior, as they determine the choice and intensity of motor plans to achieve them. Importantly, their value depends not only on the outcome itself, but also on its relevance for the organism: food items have an incentive (motivational) value only when the animal is hungry. This fact highlights the importance of embodiment, i.e.&nbsp;the fundamental link between the body and cognitive, emotional or motivational processes <span class="citation" data-cites="Price2012">(<a href="References.html#ref-Price2012" role="doc-biblioref">Price et al., 2012</a>)</span>. These processes are not ethereal as suggested by dualist theories of the mind but rather grounded in the body and aimed at ensuring its homeostasis <span class="citation" data-cites="Cabanac1971 damasio1994">(<a href="References.html#ref-Cabanac1971" role="doc-biblioref">Cabanac, 1971</a>; <a href="References.html#ref-damasio1994" role="doc-biblioref">Damasio, 1994</a>)</span>.</p>
<p>These fundamental properties of animal behavior, especially of human cognitive behavior, are still unaccessible to artificial systems. Current artificial systems mostly respond to specific stimuli by applying predefined or learned rules (stimulus-response associations). They are reactive structures which only seek new and relevant information when instructed to, not when they “want”, “need” or “like” it. There are only a few attempts to implement motivated behavior in such systems, e.g.&nbsp;intrinsic motivation on robotic platforms <span class="citation" data-cites="Baldassarre2013a Mirolli2013">(<a href="References.html#ref-Baldassarre2013a" role="doc-biblioref">Baldassarre et al., 2013</a>; <a href="References.html#ref-Mirolli2013" role="doc-biblioref">Mirolli et al., 2013</a>)</span>, but they are still limited to toy problems. In order to build truly intelligent and autonomous artificial systems, fundamental properties such as intrinsic motivation and transfer of learning must be understood and formalized.</p>
</section>
</section>
<section id="reward-and-the-dopaminergic-system" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="reward-and-the-dopaminergic-system"><span class="header-section-number">1.3</span> Reward and the dopaminergic system</h2>
<section id="dopaminergic-system" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="dopaminergic-system">Dopaminergic system</h4>
<div id="fig-intro:daprojection" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro:daprojection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/intro/dasystem.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro:daprojection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.1: Efferent pathways of the dopaminergic system. The nigrostriatal pathway connects SNc to the basal ganglia, especially the striatum. The mesolimbic pathway connects VTA to the nucleus accumbens (or ventral striatum), the amygdala, the hippocampus and the cingulate cortex. The mesocortical pathway connects VTA mainly to the prefrontal cortex, but also the motor cortex and temporal lobe. Adapted from <span class="citation" data-cites="Mancall2011">Mancall and Brock (<a href="References.html#ref-Mancall2011" role="doc-biblioref">2011</a>)</span>.
</figcaption>
</figure>
</div>
<p>Dopamine (DA) is a key neurotransmitter in the brain. It is primarily produced by two small nuclei of the brainstem: the substantia nigra pars compacta (SNc) and the ventral tegmental area (VTA). Dopamine levels are involved in many processes such as the facilitation of approach behavior, incentive learning, motivation, novelty and saliency detection as well as reinforcement learning and action selection <span class="citation" data-cites="Horvitz2000 Ikemoto2010 Sesack2010">(<a href="References.html#ref-Horvitz2000" role="doc-biblioref">Horvitz, 2000</a>; <a href="References.html#ref-Ikemoto2010" role="doc-biblioref">Ikemoto, 2010</a>; <a href="References.html#ref-Sesack2010" role="doc-biblioref">Sesack and Grace, 2010</a>)</span>. As shown on <a href="#fig-intro:daprojection" class="quarto-xref">Figure&nbsp;<span>1.1</span></a>, dopaminergic neurons in SNc and VTA send projections along three different pathways: the nigrostriatal pathway comprises the projections between SNc and the basal ganglia (BG), especially its input structure the striatum. While SNc projects almost entirely to the BG, VTA projects both inside and outside the BG: the mesolimbic pathway reaches subcortical or phylogenetically ancient structures such as the nucleus accumbens (NAcc, also called ventral striatum in primates), the amygdala (a key area for emotional processing), the hippocampus (long-term memory formation and spatial navigation) and the cingulate cortex (error detection, self). VTA also projects diffusely to the cerebral cortex through the mesocortical pathway, reaching primarily the prefrontal cortex (PFC, planning, working memory), but also the motor cortex (movement) and the temporal lobe (visual processing and memory).</p>
<p>Neurons in VTA exhibit a rather low baseline activity (around 5 Hz) and become transiently active in response to various stimuli: novel and salient stimuli <span class="citation" data-cites="redgrave2006">(<a href="References.html#ref-redgrave2006" role="doc-biblioref">Redgrave and Gurney, 2006</a>)</span>, painful stimulations <span class="citation" data-cites="Matsumoto2009">(<a href="References.html#ref-Matsumoto2009" role="doc-biblioref">Matsumoto and Hikosaka, 2009</a>)</span> and reward delivery <span class="citation" data-cites="Ljungberg1992">(<a href="References.html#ref-Ljungberg1992" role="doc-biblioref">Ljungberg et al., 1992</a>)</span>. Importantly, <span class="citation" data-cites="Schultz1997">Schultz et al. (<a href="References.html#ref-Schultz1997" role="doc-biblioref">1997</a>)</span> showed an interesting characteristic of neural firing in VTA during classical appetitive conditioning in the primate. A visual conditioned stimulus (CS) is repeatedly paired with a food reward (US). At the beginning of learning, reward delivery generates a burst of activation of the VTA dopaminergic neurons (top of <a href="#fig-intro:daschultz" class="quarto-xref">Figure&nbsp;<span>1.2</span></a>), but not the appearance of the CS. After a few days of training, the pattern is reversed (middle of <a href="#fig-intro:daschultz" class="quarto-xref">Figure&nbsp;<span>1.2</span></a>): the appearance of the CS provokes a DA burst, but not reward delivery anymore. Moreover, when the reward delivery is predicted by the CS but omitted (bottom of <a href="#fig-intro:daschultz" class="quarto-xref">Figure&nbsp;<span>1.2</span></a>), DA cells show a pause in firing (a <em>dip</em>) at the time reward is expected.</p>
<div id="fig-intro:daschultz" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro:daschultz-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/intro/schultz.jpg" class="img-fluid figure-img" style="width:35.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro:daschultz-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.2: Recordings of a single VTA neuron during appetitive conditioning. The raster plots depict the spikes emitted for different trials. The histogram of these spikes is displayed above. Top: reward is delivered unexpectedly. Middle: the CS predicts the delivery of reward. Bottom: the CS predicts a reward, but the reward fails to occur. Adapted from <span class="citation" data-cites="schultz1998">Schultz (<a href="References.html#ref-schultz1998" role="doc-biblioref">1998</a>)</span>.
</figcaption>
</figure>
</div>
<p>This pattern of activation suggests that VTA cells collectively encode a reward prediction error (RPE), defined as the difference between the reward actually received and the predicted reward. If more reward is received than expected, the RPE is positive, which happens when reward delivery is unexpected (not - yet - predicted) or when a CS appears (the appearance of the CS itself is unpredictable, but it signals that reward will be delivered). If less reward is received than expected, the RPE is negative, corresponding to the dip in VTA activity when reward is omitted. If reward is received as expected, the error is equal to zero. This happens when the CS fully predicts reward delivery.</p>
</section>
<section id="td-analogy" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="td-analogy">TD analogy</h4>
<p>An analogy between this RPE pattern of VTA cells during conditioning and the <em>temporal difference</em> (TD) algorithm of reinforcement learning <span class="citation" data-cites="Sutton1998">(<a href="References.html#ref-Sutton1998" role="doc-biblioref">Sutton and Barto, 1998</a>)</span> became quickly dominant. In the reinforcement learning framework, each state <span class="math inline">s</span> of a finite Markovian Decision Process (MDP) is associated with a value function <span class="math inline">V^\pi (s)</span> which represents the expectation of the sum of rewards that will be obtained after being in the state <span class="math inline">s</span> and thereafter following a policy <span class="math inline">\pi</span>:</p>
<p><span class="math display">
    V^{\pi} (s) = E_{\pi} ( R_t | s_t = s) = E_{\pi} ( \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} | s_t = s )
</span></p>
<p><span class="math inline">\gamma</span> is a discounting factor allowing to scale the relative importance of immediate rewards (which will be obtained shortly after being in the state <span class="math inline">s</span> at time <span class="math inline">t</span>) compared to rewards obtained on the longer term. In the TD algorithm, the value of a state is estimated iteratively after each transition between a state <span class="math inline">s_t</span> and a state <span class="math inline">s_{t+1}</span>:</p>
<p><span class="math display">
    V^{\pi}(s_t) \leftarrow V^{\pi}(s_t) + \alpha \cdot (r_{t+1} + \gamma\cdot V^{\pi}(s_{t+1}) - V^{\pi}(s_t) )
</span></p>
<p>The TD error signal:</p>
<p><span class="math display">
    \delta_t = R_t - V^{\pi}(s_t) = (r_{t+1} + \gamma\cdot V^{\pi}(s_{t+1}) ) - V^{\pi}(s_t)
</span></p>
<p>is a reward prediction error signal, as it compares the rewards actually received after the state <span class="math inline">s_t</span> with their prediction <span class="math inline">V^{\pi}(s_t)</span>. More precisely, the rewards actually received are decomposed into the reward immediately obtained during the transition (<span class="math inline">r_{t+1}</span>) and an estimation of the rewards that will be obtained after being in <span class="math inline">s_{t+1}</span> (<span class="math inline">V^{\pi}(s_{t+1})</span>, discounted by <span class="math inline">\gamma</span>). When more reward is obtained than predicted (either because the immediate reward <span class="math inline">r_{t+1}</span> is high, or because the transition leads to a state with a high value), the RPE signal is positive and increases the value of the state. If less reward is received than expected, the TD error signal is negative and decreases the value of the state.</p>
<p>To account for classical conditioning, states have to represent discrete time events. As by definition no action is required to obtain the rewards, transitions between states occur on a fixed schedule. At the beginning of conditioning, all states are initialized with a value of 0. The first time a reward is delivered, the TD error becomes positive for the preceding state: it was not predicting any reward but one occurred. At the next trial, if the reward arrives at the same time, its value will be slightly higher, so the TD error will be smaller. Meanwhile, the preceding state will see its value increased, because it leads to a state with a positive value. After several conditioning trials, the value of all states preceding reward delivery will be positive. The TD error is zero for the transitions between theses states, as they correctly predict reward delivery. Only the transition to the state corresponding to the appearance of the CS will have a positive TD error signal: the system was in a state where no reward is predicted (the animal is waiting for something to happen) but the transition leads to a state where reward will be delivered after a certain delay. If the reward is not delivered during the usual transition, the TD error becomes negative.</p>
<p>However, over the course of learning multiple trials, the positive TD error signal “travels” back in time, peaking first at reward delivery, then at the preceding state, until it appears at CS onset. In order to fully account for the observations of <span class="citation" data-cites="Schultz1997">Schultz et al. (<a href="References.html#ref-Schultz1997" role="doc-biblioref">1997</a>)</span>, where reward-related activation of VTA cells slowly decreases with learning while the CS-related one increases, but nothing happens in-between, one has to use a modified version of the TD algorithm called TD(<span class="math inline">\lambda</span>) <span class="citation" data-cites="Sutton1988">(<a href="References.html#ref-Sutton1988" role="doc-biblioref">Sutton, 1988</a>)</span>. In this variant, when a reward is delivered, not only the value of the preceding state is updated, but also all the preceding states, with a magnitude weighted by the decreasing series <span class="math inline">\lambda^{t-k}</span>. Consequently, the state corresponding to CS onset gets also updated the first time reward is delivered. When <span class="math inline">\lambda</span> is chosen close enough to 1, the resulting pattern of activation of the TD error signal matches the experimental observations on VTA firing <span class="citation" data-cites="schultz1998">(<a href="References.html#ref-schultz1998" role="doc-biblioref">Schultz, 1998</a>)</span>.</p>
</section>
<section id="alternative-models" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="alternative-models">Alternative models</h4>
<p>This striking analogy was immediately successful and led to many top-down models of the role of DA in both classical and operant conditioning <span class="citation" data-cites="Suri2001 daw2002 Smith2006 Samejima2007 Rao2010">(<a href="References.html#ref-daw2002" role="doc-biblioref">Daw and Touretzky, 2002</a>; <a href="References.html#ref-Rao2010" role="doc-biblioref">Rao, 2010</a>; <a href="References.html#ref-Samejima2007" role="doc-biblioref">Samejima and Doya, 2007</a>; <a href="References.html#ref-Smith2006" role="doc-biblioref">Smith et al., 2006</a>; e.g. <a href="References.html#ref-Suri2001" role="doc-biblioref">Suri and Schultz, 2001</a>)</span>. There are however many aspects of DA firing in VTA which are not explained by the TD analogy. When reward is delivered earlier than predicted, VTA cells are activated at reward delivery but stay at baseline at the usual time <span class="citation" data-cites="Hollerman1998">(<a href="References.html#ref-Hollerman1998" role="doc-biblioref">Hollerman and Schultz, 1998</a>)</span>, contrary to what is predicted by TD. When reward delivery is uncertain, dopaminergic neurons first respond phasically to CS onset and then increase their activity until reward delivery with a slope depending on its probability <span class="citation" data-cites="Fiorillo2003">(<a href="References.html#ref-Fiorillo2003" role="doc-biblioref">Fiorillo et al., 2003</a>)</span>. Moreover, DA neurons also respond to novel and salient stimuli which are not predictive of reward <span class="citation" data-cites="redgrave2006">(<a href="References.html#ref-redgrave2006" role="doc-biblioref">Redgrave and Gurney, 2006</a>)</span>.</p>
<p>The main problem is the way time is represented: transitions between states are supposed to occur at a fixed rate, determined by some internal clock. A TD model is only able to learn a single CS-US interval with a constant duration. However, classical conditioning is robust to variability in the CS-US interval <span class="citation" data-cites="kirkpatrick2000">(<a href="References.html#ref-kirkpatrick2000" role="doc-biblioref">Kirkpatrick and Church, 2000</a>)</span>. Many models have been proposed to improve the representation of time in TD models, including serial-compound representations <span class="citation" data-cites="Suri2001">(<a href="References.html#ref-Suri2001" role="doc-biblioref">Suri and Schultz, 2001</a>)</span> and Long Short-Term Memory networks <span class="citation" data-cites="Rivest2010">(LSTM, <a href="References.html#ref-Rivest2010" role="doc-biblioref">Rivest et al., 2010</a>)</span>. More sophisticated neuro-computational models separate the mechanisms responsible for CS-related and US-related activations <span class="citation" data-cites="OReilly2006 Dranias2008 Vitay2014">(<a href="References.html#ref-Dranias2008" role="doc-biblioref">Dranias et al., 2008</a>; <a href="References.html#ref-OReilly2006" role="doc-biblioref">O’Reilly and Frank, 2006</a>; <a href="References.html#ref-Vitay2014" role="doc-biblioref">Vitay and Hamker, 2014</a>)</span>. Based on the neuroanatomy of the afferents to the dopaminergic system, they distinguish the sources of excitation and inhibition signaling reward delivery to VTA from the ones signaling predictors of rewards. In <a href="5-FINR.html" class="quarto-xref"><span>Chapter 5</span></a>, we will discuss these models and explain their importance for motivated behavior.</p>
<p>Despite these limitations, it is clear that DA firing in VTA represents a RPE that can be used to reinforce actions or plans in other structures such as the BG or the prefrontal cortex. However, if the amount of evidence for the role of positive RPEs is undebatable, it is still unclear what is the effect of negative RPEs, for example when a predicted reward is omitted. VTA cells fire at a rather low baseline activity (5 Hz) and the mechanisms by which a pause in firing can influence plasticity in efferent systems are still a matter of debate <span class="citation" data-cites="Shen2008">(see <a href="References.html#ref-Shen2008" role="doc-biblioref">Shen et al., 2008</a> for an explanation on its role in the striatum)</span>. Moreover, the observed dip in VTA activity when reward is omitted has not been reproduced by other researchers <span class="citation" data-cites="Joshua2009">(<a href="References.html#ref-Joshua2009" role="doc-biblioref">Joshua et al., 2009</a>)</span>. Many efforts remain to be done to fully understand how VTA and SNc signal reward-prediction errors to the BG and prefrontal cortex. VTA is for example known to send also inhibitory projections to the ventral striatum <span class="citation" data-cites="Brown2012">(<a href="References.html#ref-Brown2012" role="doc-biblioref">Brown et al., 2012</a>)</span>, what opens new questions on the exact role of VTA in reward processing <span class="citation" data-cites="Creed2014">(<a href="References.html#ref-Creed2014" role="doc-biblioref">Creed et al., 2014</a>)</span>.</p>
</section>
</section>
<section id="basal-ganglia-and-reinforcement-learning" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="basal-ganglia-and-reinforcement-learning"><span class="header-section-number">1.4</span> Basal ganglia and reinforcement learning</h2>
<section id="anatomy" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="anatomy">Anatomy</h4>
<p>The basal ganglia are a set of nuclei in the basal forebrain (<a href="#fig-intro:bg_anatomy" class="quarto-xref">Figure&nbsp;<span>1.3</span></a>). They receive inputs from the entirety of the cerebral cortex (although the frontal lobe is dominant) and project to various sub-cortical nuclei such as the brainstem or the thalamus, where the processed information can go back to the cerebral cortex. They are involved in a variety of functions, among which the control of voluntary movements, action selection, sequence learning, habit formation, updating of working memory (WM), motivation and emotion. Their importance for behavior is emphasized by their involvement in many neurological diseases, including Parkinson’s disease, Huntington’s disease, Tourette syndrome, obsessive-compulsive disorders, addiction and schizophrenia.</p>
<p>The <em>striatum</em> (STR) is the main input structure of the BG. In the primate, it is composed of the dorsal striatum (caudate nucleus - CN - and putamen - PUT) and the ventral striatum (nucleus accumbens - NAcc - and olfactory tubercle). It receives massive inputs from the whole cerebral cortex, with the ventral striatum also receiving inputs from sub-cortical structures such as the hippocampus or the amygdala <span class="citation" data-cites="Humphries2010">(<a href="References.html#ref-Humphries2010" role="doc-biblioref">Humphries and Prescott, 2010</a>)</span>. It is principally composed of <em>medium spiny neurons</em> (MSNs) which are able to integrate cortical inputs from different areas and project inhibitorily inside the BG on the <em>globus pallidus</em> (GP). Two types of MSNs are found in the striatum depending on the dopamine receptors they exhibit: D1-mediated and D2-mediated MSNs. They contribute to different pathways within the BG depending on the part of the GP they project on: its internal part (GPi) for D1 MSNS, the external one (GPe) for D2 MSNs. The second input structure of the BG is the <em>subthalamic nucleus</em> (STN). Although much smaller, it also receives massive cortical inputs and projects excitatorily on the GP.</p>
<p>The output structures of the BG are GPi and the <em>substantia nigra pars reticulata</em> (SNr). As they are functionally similar, they are often labeled together as GPi/SNr, although they are not anatomically close. Neurons in GPi/SNr are tonically active, meaning that they have an elevated firing rate baseline (between 60 and 80 Hz). At rest, they exert a strong inhibition on target structures of the BG, including the thalamus. GPi/SNr must be themselves inhibited in order to release this inhibition and allow the target structures to get activated, a phenomenon called <em>disinhibition</em> <span class="citation" data-cites="Chevalier1990">(<a href="References.html#ref-Chevalier1990" role="doc-biblioref">Chevalier and Deniau, 1990</a>)</span>. As a whole, the BG act as a gating regulator of activity in target structures.</p>
<div id="fig-intro:bg_anatomy" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro:bg_anatomy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/intro/bg_anatomy.jpg" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro:bg_anatomy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.3: Anatomical position of the basal ganglia in the brain. The BG are composed by the striatum (caudate nucleus and putamen), the globus pallidus (internal and external), the substantia nigra (pars reticulata and pars compacta) and the subthalamic nucleus. It receives mainly inputs from the cerebral cortex and projects either directly to the brainstem (red nucleus, superior colliculus) or back to the cortex through the thalamus.
</figcaption>
</figure>
</div>
</section>
<section id="pathways" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="pathways">Pathways</h4>
<p>The internal connectivity of the BG shows a complex organization (<a href="#fig-intro:bg_structure" class="quarto-xref">Figure&nbsp;<span>1.4</span></a>). Three principal pathways can nevertheless be identified. The <em>direct</em> pathway goes directly from D1-mediated MSNs to GPi/SNr. It is the main source of disinhibition for the output of the BG. The <em>indirect</em> pathway originates in the D2-mediated MSNs and relays in GPe before targeting GPi/SNr either directly or through STN. The additional inhibitory relay on GPe makes this pathway globally excitatory on GPi/SNr: the activation of D2-mediated MSNs increases firing rates in GPi/SNr, what further prevents target structures to get activated. The opposing effects of the direct and indirect pathways led to the first models of motor processing in the BG <span class="citation" data-cites="Albin1989 DeLong1990">(<a href="References.html#ref-Albin1989" role="doc-biblioref">Albin et al., 1989</a>; <a href="References.html#ref-DeLong1990" role="doc-biblioref">DeLong, 1990</a>)</span>. The balance between their opposing effects (“Go” for the direct pathway, “No Go” for the indirect one) allows to control the initiation, vigor and termination of motor movements. Pathological imbalance between the pathways can explain neurological diseases: dopamine loss, characteristic of Parkinson’s disease (PD), weakens the direct pathway, as DA has an excitatory effect on D1-mediated MSNs and inhibitory on D2-mediated ones <span class="citation" data-cites="Gerfen1990 Surmeier2007">(<a href="References.html#ref-Gerfen1990" role="doc-biblioref">Gerfen et al., 1990</a>; <a href="References.html#ref-Surmeier2007" role="doc-biblioref">Surmeier et al., 2007</a>)</span>. The resulting increased inhibition on motor centers causes hypokinesia, the inability to initiate movements. On the contrary, excess of dopamine, as in Huntington’s disease <span class="citation" data-cites="Chen2013">(<a href="References.html#ref-Chen2013" role="doc-biblioref">Chen et al., 2013</a>)</span> or Tourette syndrome <span class="citation" data-cites="Albin2006">(<a href="References.html#ref-Albin2006" role="doc-biblioref">Albin and Mink, 2006</a>)</span>, over-activates the direct pathway and leads to hyperkinetic symptoms, such as involuntary movements and tics.</p>
<div id="fig-intro:bg_structure" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro:bg_structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/intro/bg_structure.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro:bg_structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.4: Schematic organization of the BG. The BG takes inputs from the cerebral cortex and tonically inhibits the thalamus, modulating closed or open loops between the cortex and the thalamus. The <em>direct</em> pathway starts from D1-mediated MSNs of the striatum and ends directly in the output structures GPi/SNr. The <em>indirect</em> pathway starts from D2-mediated MSNs, relays in GPe and reaches GPi/SNr either directly or through STN. The <em>hyperdirect</em> pathway starts from STN and reaches GPi/SNr either directly or through GPe. Dopaminergic cells in SNc have inputs from the striatum and modulate virtually all projections within the BG.
</figcaption>
</figure>
</div>
<p>The <em>hyperdirect</em> pathway connects directly STN to GPi/SNr through excitatory synapses, with a much lower latency than the other pathways <span class="citation" data-cites="Nambu2002">(<a href="References.html#ref-Nambu2002" role="doc-biblioref">Nambu et al., 2002</a>)</span>. It allows to send rapidly cortical information to the output nuclei of the BG, bypassing computations in the direct and indirect pathways. Because of its excitatory effect on GPi/SNr and the diffuse projection of SNr on GPi/SNr (a neuron in STN excites many neurons in GPi/SNr), it carries a “Global No Go” signal allowing to suppress involuntary movements or to terminate them prematurely. According to <span class="citation" data-cites="Nambu2002">Nambu et al. (<a href="References.html#ref-Nambu2002" role="doc-biblioref">2002</a>)</span>, the three pathways may cooperate during action selection following a center-surround model: when a voluntary movement is initiated by cortical areas, the hyperdirect pathway first inhibits large areas of the thalamus and cerebral cortex that are related to both the selected movement and its competitors. For example, before moving the arm to the left, any arm movement previously prepared will be wiped out by the increased excitation in GPi/SNr. Some milliseconds later, the direct pathway selects the appropriate motor program while the indirect pathway selectively inhibits competing movements.</p>
<p>These three pathways form a classical feedforward view of the BG which has been used in many models <span class="citation" data-cites="Gurney2001 OReilly2006 Schroll2012">(<a href="References.html#ref-Gurney2001" role="doc-biblioref">Gurney et al., 2001</a>; <a href="References.html#ref-OReilly2006" role="doc-biblioref">O’Reilly and Frank, 2006</a>; <a href="References.html#ref-Schroll2012" role="doc-biblioref">Schroll et al., 2012</a>)</span>. As depicted in <a href="#fig-intro:bg_structure" class="quarto-xref">Figure&nbsp;<span>1.4</span></a>, there exists many other projections inside the BG which render the understanding of processing within the BG much more complex. The thalamostriatal pathway, formed by projections from the thalamus to the striatum, may for example be involved in attentional processes and help the BG solve the credit-assignment problem <span class="citation" data-cites="Galvan2011">(<a href="References.html#ref-Galvan2011" role="doc-biblioref">Galvan and Smith, 2011</a>)</span>. The reciprocal connections between STN and GPe lead to oscillations under certain circumstances, what could form the basis of an internal pacemaker inside the BG <span class="citation" data-cites="Plenz1999">(<a href="References.html#ref-Plenz1999" role="doc-biblioref">Plenz and Kital, 1999</a>)</span>, but can also become pathological in Parkinson’s disease and explain symptoms such as tremor <span class="citation" data-cites="Levy2002">(<a href="References.html#ref-Levy2002" role="doc-biblioref">Levy et al., 2002</a>)</span>. Much remains to be done to fully understand the role of the STN-GPe loop <span class="citation" data-cites="Kumar2011">(<a href="References.html#ref-Kumar2011" role="doc-biblioref">Kumar et al., 2011</a>)</span>. The role of the pallidostriatal projection between GPe and the striatum is also still mainly unexplored <span class="citation" data-cites="Kita1999 Bahuguna2015">(<a href="References.html#ref-Bahuguna2015" role="doc-biblioref">Bahuguna et al., 2015</a>; <a href="References.html#ref-Kita1999" role="doc-biblioref">Kita et al., 1999</a>)</span>.</p>
</section>
<section id="dopamine-mediated-plasticity" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="dopamine-mediated-plasticity">Dopamine-mediated plasticity</h4>
<p>The striking feature of the BG is their dependency on dopamine, either as a modulator of activity - elevated DA levels increase the excitability of D1-mediated MSNs and decrease the one of D2 cells <span class="citation" data-cites="Nicola2000">(<a href="References.html#ref-Nicola2000" role="doc-biblioref">Nicola et al., 2000</a>)</span> - or of plasticity - different DA levels can induce selectively <em>long-term potentiation</em> (LTP) or <em>long-term depression</em> (LTD) at corticostriatal synapses <span class="citation" data-cites="Calabresi2007">(<a href="References.html#ref-Calabresi2007" role="doc-biblioref">Calabresi et al., 2007</a>)</span>. All nuclei of the dorsal BG receive dopaminergic input from SNc, while the ventral part receives mainly inputs from VTA. Reciprocally, the striatum is a major source of inhibition to the dopaminergic areas, allowing the BG to control their own dopaminergic input <span class="citation" data-cites="Haber2000">(<a href="References.html#ref-Haber2000" role="doc-biblioref">Haber et al., 2000</a>)</span>.</p>
<p>Dopamine-mediated plasticity is particularly studied in the striatum. MSNs exhibit particular dynamics: their membrane potential can be either in a hyperpolarized <em>down-state</em> or in a depolarized <em>up-state</em>. In the down-state, the excitability of the cell is very low and striatal neurons do not emit spikes. In the up-state, the cell is very excitable and responds to its cortical inputs. The transition between these two states can be spontaneous <span class="citation" data-cites="Leung1993">(it occurs at a rate of 0.5 to 2 Hz, <a href="References.html#ref-Leung1993" role="doc-biblioref">Leung and Yim, 1993</a>)</span>, induced by a phasic DA burst in VTA/SNc <span class="citation" data-cites="Gruber2003">(<a href="References.html#ref-Gruber2003" role="doc-biblioref">Gruber et al., 2003</a>)</span> or by a massive cortical input <span class="citation" data-cites="McGinty2009">(<a href="References.html#ref-McGinty2009" role="doc-biblioref">McGinty and Grace, 2009</a>)</span>. For D1-mediated MSNs, LTP is known to occur at corticostriatal synapses in the presence of a strong cortical input and under elevated DA levels when the cell is in the up-state. LTD happens on the contrary when there are weak cortical inputs, low DA levels and the cell is in the down-state <span class="citation" data-cites="Reynolds2000 Calabresi2007">(<a href="References.html#ref-Calabresi2007" role="doc-biblioref">Calabresi et al., 2007</a>; <a href="References.html#ref-Reynolds2000" role="doc-biblioref">Reynolds and Wickens, 2000</a>)</span>. Put together, plasticity at corticostriatal synapses seems to be driven by a three-term DA-modulated Hebbian learning rule, where the change in synaptic efficiency is ruled by the product of the pre-synaptic activity (<span class="math inline">r^{\text{pre}}</span>, presence of cortical inputs), the post-synaptic activity (<span class="math inline">r^{\text{post}}</span>, up- or down-state) and the deviation of the dopamine level from its baseline <span class="math inline">\delta</span>:</p>
<p><span class="math display">
    \Delta w = \delta \cdot r^{\text{pre}} \cdot r^{\text{post}}
</span></p>
<p>The opposite pattern is found for D2-mediated MSNs: high DA levels induce LTD while low levels induce LTP <span class="citation" data-cites="Shen2008">(<a href="References.html#ref-Shen2008" role="doc-biblioref">Shen et al., 2008</a>)</span>. With this model of corticostriatal plasticity, DA becomes able to selectively reinforce corticostriatal associations. If a motor plan selected by the direct pathway led to reward, DA will strengthen the corticostriatal synapses to D1-mediated MSNs that were previously activated and reduce the ones to D2-mediated MSNs. This increases the probability that the same motor plan will be selected again in the future by favoring the direct pathway in its competition with the indirect one. Oppositely, if the action leads to less reward than expected, the D1-mediated synapses will be reduced and the D2-mediated ones increased, what strengthens the indirect pathway and prevents further selection of that motor plan.</p>
<p>This mechanism of dopamine-based reinforcement in the BG further emphasized the analogy with reinforcement learning, especially the <em>actor-critic</em> architecture <span class="citation" data-cites="Sutton1998">(<a href="References.html#ref-Sutton1998" role="doc-biblioref">Sutton and Barto, 1998</a>)</span>. In this framework, the critic produces the TD error signal which is used both to update the value of a state and to reinforce the state-action association that led to reward. Using this error signal, the actor simply learns to map a state onto the optimal action. In this view, the critic would be composed by the dopaminergic system and the ventral BG, while the actor represents a loop between the cerebral cortex and the dorsal BG. Many neuro-computational models of the BG are based on this architecture <span class="citation" data-cites="Houk1995 berns1998 Gurney2001 Joel2002">(<a href="References.html#ref-berns1998" role="doc-biblioref">Berns and Sejnowski, 1998</a>; <a href="References.html#ref-Gurney2001" role="doc-biblioref">Gurney et al., 2001</a>; <a href="References.html#ref-Houk1995" role="doc-biblioref">Houk et al., 1995</a>; <a href="References.html#ref-Joel2002" role="doc-biblioref">Joel et al., 2002</a>)</span>.</p>
<p>Many criticisms have been formulated to this model. First, DA cells do not only signal RPEs but also respond to aversive, salient and novel stimuli, which does not fit into the reward-prediction error hypothesis <span class="citation" data-cites="Pennartz1995">(<a href="References.html#ref-Pennartz1995" role="doc-biblioref">Pennartz, 1995</a>)</span>. They also respond to reward-predicting stimuli with a very short latency, raising the issue of how their reward-predicting value can be predicted in such a short time <span class="citation" data-cites="redgrave1999">(<a href="References.html#ref-redgrave1999" role="doc-biblioref">Redgrave et al., 1999</a>)</span>. DA is even not required for acquiring the value of a stimulus (“liking”), only for its motivational effect (“wanting”), so the role of the critic might be misunderstood <span class="citation" data-cites="Berridge2007">(<a href="References.html#ref-Berridge2007" role="doc-biblioref">Berridge, 2007</a>)</span>. Another issue with the actor-critic assumption is the temporal credit-assignment problem: rewards are usually delivered well after the causal action is executed. How can this delayed feedback influence motor representations which have long faded away?</p>
<p>More detailed neuro-computational models have been introduced to overcome these issues. The PBWM (prefrontal cortex, basal ganglia working memory) model of <span class="citation" data-cites="OReilly2006">O’Reilly and Frank (<a href="References.html#ref-OReilly2006" role="doc-biblioref">2006</a>)</span> makes a strong use of <em>working memory</em> (WM) processes to bridge the temporal gap between an action and its consequences. It furthermore provides a mechanism by which the content of WM is gated and updated by functional loops between the PFC and the BG. A similar approach was taken in <span class="citation" data-cites="Vitay2010">Vitay and Hamker (<a href="References.html#ref-Vitay2010" role="doc-biblioref">2010</a>)</span>, which will be presented in <a href="3-FICN.html" class="quarto-xref"><span>Chapter 3</span></a>. This model was the first to consider the importance of plasticity within the BG (specifically in the projections from the striatum to the globus pallidus) in addition to corticostriatal plasticity.</p>
<p>Generally, the role of the BG in motor learning and action selection is partially understood, but its contribution to other forms of learning has been less extensively studied. An interesting view considers the BG as a fast learning device quickly acquiring rewarded associations and transferring them to the cerebral cortex where they will be generalized and stored in long-term memory <span class="citation" data-cites="ashby2005">(<a href="References.html#ref-ashby2005" role="doc-biblioref">Ashby et al., 2005</a>)</span>. This hypothesis is backed up by the well-accepted role of the BG in habit formation <span class="citation" data-cites="Seger2011">(<a href="References.html#ref-Seger2011" role="doc-biblioref">Seger and Spiering, 2011</a>)</span>. Even more generally, one can consider the BG as a trainer for the cerebral cortex. Learning in the cerebral cortex can be characterized as unsupervised, in the sense that cortical neurons self-organize to represent internal and external events in the most efficient way. Cortical areas communicate with and adapt to each other, but there is no obvious objective function guiding the learning process (supervised learning minimizes an error function, which is unavailable at the cortical level), while reinforcement learning has to be ruled out because of the slow temporal dynamics of dopamine in the cortex <span class="citation" data-cites="Seamans2004">(the bursts and dips of the DA signal are too smoothed out in the cortex to carry the RPE, <a href="References.html#ref-Seamans2004" role="doc-biblioref">Seamans and Yang, 2004</a>)</span>. The role of BG would be to transfer specific knowledge acquired by reinforcement learning to the more general unsupervised cortical system. In the view of <span class="citation" data-cites="Stocco2010">Stocco et al. (<a href="References.html#ref-Stocco2010" role="doc-biblioref">2010</a>)</span>, the BG may also act as a conditional information-routing system, enabling transmission between remote cortical areas and allowing the learning of new associations.</p>
</section>
</section>
<section id="multiple-loops-and-organization-of-behavior" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="multiple-loops-and-organization-of-behavior"><span class="header-section-number">1.5</span> Multiple loops and organization of behavior</h2>
<p>It was mentioned that the striatum receives projections from the entirety of the cerebral cortex. However, the organization of these projections follows a specific topology on the surface of the striatum. As depicted in <a href="#fig-intro:Rodriguez-Oroz2009" class="quarto-xref">Figure&nbsp;<span>1.5</span></a>, different cortical regions project onto different parts of the striatum: the motor and premotor (PMC) cortices project mainly onto the putamen, the dorsolateral prefrontal cortex (dlPFC) projects mainly on the caudate nucleus, while the orbitofrontal (OFC) and ventromedial prefrontal (vmPFC) cortices project mainly on the nucleus accumbens. As this segregation is preserved throughout the BG, from the projections of the striatum on the GP to the thalamic nuclei relaying the output of the BG back to the cortex, the prefrontal cortex / basal ganglia system is said to be organized in parallel segregated loops <span class="citation" data-cites="Alexander1986">(<a href="References.html#ref-Alexander1986" role="doc-biblioref">Alexander et al., 1986</a>)</span>.</p>
<div id="fig-intro:Rodriguez-Oroz2009" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro:Rodriguez-Oroz2009-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/intro/Rodriguez-Oroz2009.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro:Rodriguez-Oroz2009-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.5: Parallel segregated loops between the cerebral cortex and the BG. The motor loop starts from the motor and premotor cortices and involves mainly the putamen. The associative (or cognitive) loop involves the dorsolateral cortex and the caudate nucleus. The limbic loop involves the orbitofrontal and ventromedial cortices to the ventral striatum (nucleus accumbens). Adapted from <span class="citation" data-cites="Rodriguez-Oroz2009">Rodriguez-Oroz et al. (<a href="References.html#ref-Rodriguez-Oroz2009" role="doc-biblioref">2009</a>)</span>.
</figcaption>
</figure>
</div>
<p>Each loop is therefore specialized in a particular functional domain: the motor loop is involved in motor learning and action selection, the associative loop in cognitive processes such as sequence learning and WM updating, the limbic loop in motivation and goal-directed learning. These subdivisions can be further refined: the motor loop is in fact composed of multiple segregated loops depending on the cortical region of origin (M1, SMA, pre-SMA…). Other loops have been identified, such as the oculomotor loop, devoted to the control of eye movements, or the visual loop, linking the inferotemporal and medial temporal cortices to the tail of the caudate nucleus. The importance of the visual loop will be explained in <a href="2-JOCN.html" class="quarto-xref"><span>Chapter 2</span></a> and <a href="3-FICN.html" class="quarto-xref"><span>Chapter 3</span></a>. A similar topological segregation can further be extended to the projections within a functional loop: the topology of the cortical area (e.g.&nbsp;the somatotopic representation of body parts in the motor cortex) is preserved inside the BG <span class="citation" data-cites="Nambu2011">(<a href="References.html#ref-Nambu2011" role="doc-biblioref">Nambu, 2011</a>)</span>. In this view, the PFC/BG system is composed by thousands of small parallel loops <span class="citation" data-cites="OReilly2006">(<a href="References.html#ref-OReilly2006" role="doc-biblioref">O’Reilly and Frank, 2006</a>)</span>.</p>
<p>The segregation is however not total: a certain degree of overlap is observed in the corticostriatal projections, allowing for example parts of the striatum to integrate both motor and associative information. The funneling structure of the BG - there are 100 times more neurons in the striatum than in GPi/SNr - also increases the probability that the loops communicate with each other inside the BG <span class="citation" data-cites="Bar-Gad2003">(<a href="References.html#ref-Bar-Gad2003" role="doc-biblioref">Bar-Gad et al., 2003</a>)</span>. Finally, the thalamic nuclei relaying the output of the BG back to the cortex do not target only the original cortical area, but reach also adjacent ones. In the PFC / BG system, one distinguishes <em>closed loops</em>, where a single cortical area projects to the striatum and receives the processed information back, from <em>open loops</em>, where a cortical area sends information to the BG and the result is “forwarded” to another cortical area <span class="citation" data-cites="Ebner2015">(<a href="References.html#ref-Ebner2015" role="doc-biblioref">Ebner et al., 2015</a>)</span>. Category learning in the visual loop between the inferotemporal cortex and the BG is for example transferred to the motor cortex through an open loop <span class="citation" data-cites="Seger2008">(<a href="References.html#ref-Seger2008" role="doc-biblioref">Seger, 2008</a>)</span>. The exact organization of the PFC / BG system into closed and open loops is still not precisely known, but this is an important mechanism by which the BG can modulate information transmission in the prefrontal cortex <span class="citation" data-cites="Stocco2010">(<a href="References.html#ref-Stocco2010" role="doc-biblioref">Stocco et al., 2010</a>)</span>.</p>
<p>The question that arises is how these multiple loops could learn useful associations in their respective domains based on a single unitary reward-prediction error signal, as hypothesized by the TD analogy. SNc and VTA actually display a complex topological organization depending on their reciprocal connections with the striatum <span class="citation" data-cites="Haber2000">(striato-nigro-striatal system, <a href="References.html#ref-Haber2000" role="doc-biblioref">Haber et al., 2000</a>)</span>. As depicted in <a href="#fig-intro:daloops" class="quarto-xref">Figure&nbsp;<span>1.6</span></a>, each region of the striatum engaged in a closed loop with the cerebral cortex forms reciprocal connections with a specific region of the SNc/VTA dopaminergic areas: the striatum sends inhibitory connections to SNc/VTA, which returns a dopaminergic signal. However, each striatal region also projects on the adjacent dopaminergic region along a rostro-caudal axis, i.e.&nbsp;from limbic to associative to motor domains. This pattern on connectivity forms a spiraling structure which allows different striatal regions to influence others by modulating their dopaminergic inputs.</p>
<div id="fig-intro:daloops" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro:daloops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/intro/daloops.png" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro:daloops-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.6: Spiraling connectivity pattern in the striato-nigro-striatal system. Different cortical areas (here, vmPFC, OFC, dACC - the dorsal anterior cingulate cortex -, dlPFC and SMC - the supplementary motor area) form closed loops with different parts of the striatum (ventral for vmPFC and OFC, dorsal for the others) following a rostro-caudal axis. Each part of the striatum projects on specific regions of the SNc/VTA system, which reciprocate the connections. However, they also project on adjacent dopaminergic regions in the caudal direction, forming a spiraling structure allowing the different closed loops to communicate through dopaminergic activity. Adapted from <span class="citation" data-cites="Keramati2013">Keramati and Gutkin (<a href="References.html#ref-Keramati2013" role="doc-biblioref">2013</a>)</span>.
</figcaption>
</figure>
</div>
<p>The resulting organization of PFC-BG loops along a limbic-associative-motor gradient has fundamental consequences on goal-directed behavior. Limbic regions, critical for motivational and affective processes, are in a position to influence how cognitive plans are formed and learned by associative regions, which themselves control how individual movements and actions are executed in motor regions. This highlights the tight integration between cognitive and emotional processes: goals are mainly represented in OFC, which is strongly connected with the limbic system (amygdala, ventral BG) and influences cognitive processes in dlPFC. Based on neuro-anatomical evidence, the classical view opposing cognition and emotion as competitors to produce behavior has to be replaced by an emphasis on the cooperation between the two systems.</p>
<p>This gradient also has consequences on learning: striatal regions associated to goal-directed learning influence plasticity in striatal regions associated to habit formation <span class="citation" data-cites="Yin2004 Khamassi2012">(<a href="References.html#ref-Khamassi2012" role="doc-biblioref">Khamassi and Humphries, 2012</a>; <a href="References.html#ref-Yin2004" role="doc-biblioref">Yin et al., 2004</a>)</span>. This provides a mechanism by which flexible behaviors acquired through goal-directed learning can be transferred into procedural memory to become habits. Similarly, Pavlovian-to-Instrumental transfer (PIT) is the ability to transfer stimulus values acquired through Pavlovian conditioning to instrumental behavior: after a first phase of operant conditioning where a rat learns to press levers to obtain different outcomes (say, food and water), a classical conditioning phase is introduced, pairing initially neutral stimuli (tone or light) to the same outcomes. The effect of PIT is that, when back in the operant conditioning room, the conditioned stimuli will now trigger the lever press leading to the same outcome <span class="citation" data-cites="Corbit2011">(<a href="References.html#ref-Corbit2011" role="doc-biblioref">Corbit and Balleine, 2011</a>)</span>. The mechanisms allowing a transfer of learning between classical and instrumental conditioning happen in the cooperation between two loops within the ventral BG, involving two parts of the nucleus accumbens, the core and the shell <span class="citation" data-cites="Gruber2012">(<a href="References.html#ref-Gruber2012" role="doc-biblioref">Gruber and McDonald, 2012</a>)</span>.</p>
<p>Although the concept of multiple parallel PFC/BG loops has been often used in neuro-computational models <span class="citation" data-cites="nakahara2001 OReilly2006 Nguyen2014">(<a href="References.html#ref-Nguyen2014" role="doc-biblioref">N’guyen et al., 2014</a>; e.g. <a href="References.html#ref-nakahara2001" role="doc-biblioref">Nakahara et al., 2001</a>; <a href="References.html#ref-OReilly2006" role="doc-biblioref">O’Reilly and Frank, 2006</a>)</span>, only a few have used the underlying limbic-associative-motor gradient in dopaminergic connectivity to investigate the organization of behavior. <span class="citation" data-cites="Keramati2013">Keramati and Gutkin (<a href="References.html#ref-Keramati2013" role="doc-biblioref">2013</a>)</span> for example studied this system to explain the mechanisms of addiction. In <span class="citation" data-cites="Schroll2012">Schroll et al. (<a href="References.html#ref-Schroll2012" role="doc-biblioref">2012</a>)</span> (<a href="4-NN.html" class="quarto-xref"><span>Chapter 4</span></a>), we proposed a neuro-computational model of working memory formation and maintenance involving three PFC/BG loops, two associative and one motor, which coordinate their learning through the spiraling striato-nigro-striatal system. The dopaminergic system has a central role in organizing behavior and learning; very simplified models such as TD actually limit our ability to understand the underlying processes.</p>
</section>
<section id="structure-of-the-thesis-and-contribution" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="structure-of-the-thesis-and-contribution"><span class="header-section-number">1.6</span> Structure of the thesis and contribution</h2>
<p>This thesis is composed of five articles published in international peer-reviewed journals. They were selected to be representative of the different aspects of my research on the role of dopamine in motivated behavior. In <span class="citation" data-cites="Vitay2008">Vitay and Hamker (<a href="References.html#ref-Vitay2008" role="doc-biblioref">2008</a>)</span> (<a href="2-JOCN.html" class="quarto-xref"><span>Chapter 2</span></a>), we studied the influence of dopamine on memory retrieval in the perirhinal cortex, a part of the temporal lobe involved in object recognition and visual memory. In <span class="citation" data-cites="Vitay2010">Vitay and Hamker (<a href="References.html#ref-Vitay2010" role="doc-biblioref">2010</a>)</span> (<a href="3-FICN.html" class="quarto-xref"><span>Chapter 3</span></a>), we designed a neuro-computational mode of the BG which is able to solve delayed rewarded visual memory tasks. This fundamental model was the first to introduce plasticity within the BG and was further extended in collaboration with Dr.&nbsp;Henning Schroll to account for working memory formation <a href="3-FICN.html" class="quarto-xref"><span>Chapter 3</span></a>. In <span class="citation" data-cites="Vitay2014">Vitay and Hamker (<a href="References.html#ref-Vitay2014" role="doc-biblioref">2014</a>)</span> (<a href="5-FINR.html" class="quarto-xref"><span>Chapter 5</span></a>), we designed a detailed model of the dopaminergic system during conditioning, with a strong emphasis on its dependency on timing processes. Additionally, in <span class="citation" data-cites="Vitay2015">Vitay et al. (<a href="References.html#ref-Vitay2015" role="doc-biblioref">2015</a>)</span> (<a href="6-FINI.html" class="quarto-xref"><span>Chapter 6</span></a>), we present a neural simulator that was developed in parallel and which allows to define these neuro-computational models easily and simulate them efficiently on parallel hardware. A detailed description of the content of these articles is provided in the following sections.</p>
<section id="list-of-publications-included-in-the-thesis" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="list-of-publications-included-in-the-thesis">List of publications included in the thesis</h3>
<ol type="1">
<li><p>Vitay, J. and Hamker, F. H. (2008). Sustained activities and retrieval in a computational model of the perirhinal cortex. <em>Journal of Cognitive Neuroscience</em>, 20, 11, 1993-2005, doi: <a href="http://www.mitpressjournals.org/doi/abs/10.1162/jocn.2008.20147">10.1162/jocn.2008.20147</a></p></li>
<li><p>Vitay, J. and Hamker, F. H. (2010). A computational model of basal ganglia and its role in memory retrieval in rewarded visual memory tasks. <em>Frontiers in Computational Neuroscience</em>, 4, doi: <a href="http://dx.doi.org/10.3389/fncom.2010.00013">10.3389/fncom.2010.00013</a></p></li>
<li><p>Schroll, H., Vitay, J., and Hamker, F. H. (2012). Working memory and response selection: a computational account of interactions among cortico-basalganglio-thalamic loops. <em>Neural Networks</em>, 26, 59–74, doi: <a href="http://dx.doi.org/10.1016/j.neunet.2011.10.008">10.1016/j.neunet.2011.10.008</a></p></li>
<li><p>Vitay, J. and Hamker, F. H. (2014). Timing and expectation of reward: a neuro-computational model of the afferents to the ventral tegmental area. <em>Frontiers in Neurorobotics</em>, 8, 4, doi: <a href="http://dx.doi.org/10.3389/fnbot.2014.00004">10.3389/fnbot.2014.00004</a></p></li>
<li><p>Vitay, J., Dinkelbach, H. Ü., and Hamker, F. H. (2015). ANNarchy: a code generation approach to neural simulations on parallel hardware. <em>Frontiers in Neuroinformatics</em>, 9, 19, doi:<a href="http://dx.doi.org/10.3389/fninf.2015.00019">10.3389/fninf.2015.00019</a></p></li>
</ol>
</section>
<section id="contribution-to-each-article" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="contribution-to-each-article">Contribution to each article</h3>
<p>I am the primary author of articles 1, 2 and 4, having conducted the research, implemented the models, performed the experiments, analyzed the results and primarily written the manuscripts. Prof.&nbsp;Hamker supervised the research, guided the whole process and participated in the writing. For article 3, Dr.&nbsp;Henning Schroll is the primary author. He implemented the model, ran the experiments, analyzed the results and primarily wrote the article. I co-supervised the development of the model together with Prof.&nbsp;Hamker and participated in the writing. For article 5, Helge Ülo Dinkelbach was involved in developing the neural simulator and running the experiments, co-supervised by Prof.&nbsp;Hamker and me. I developed equally the neural simulator and wrote primarily the manuscript.</p>
</section>
<section id="chapter-2-perirhinal-cortex-and-dopamine" class="level3" data-number="1.6.1">
<h3 data-number="1.6.1" class="anchored" data-anchor-id="chapter-2-perirhinal-cortex-and-dopamine"><span class="header-section-number">1.6.1</span> Chapter 2 : Perirhinal cortex and dopamine</h3>
<p>Working memory is the ability to temporarily store and manage information in order to use it for cognitive processes <span class="citation" data-cites="Baddeley1986">(<a href="References.html#ref-Baddeley1986" role="doc-biblioref">Baddeley, 1986</a>)</span>. A typical example is remembering a phone number before typing it: the number is stored in short-term memory as long as it is needed for the action, but the memory fades away when it is not required anymore. The neural correlate of WM processes is <em>sustained activation</em>: neurons which are activated by the presence of the information stay active during the whole period between its disappearance and its later use by cognitive processes. Sustained activation has been found in many brain areas, including the prefrontal cortex <span class="citation" data-cites="funahashi1989">(<a href="References.html#ref-funahashi1989" role="doc-biblioref">Funahashi et al., 1989</a>)</span>, the parietal cortex <span class="citation" data-cites="Koch1989">(<a href="References.html#ref-Koch1989" role="doc-biblioref">Koch and Fuster, 1989</a>)</span>, the inferotemporal cortex <span class="citation" data-cites="ranganath2004">(<a href="References.html#ref-ranganath2004" role="doc-biblioref">Ranganath et al., 2004</a>)</span> and the medial temporal lobe <span class="citation" data-cites="naya2003">(<a href="References.html#ref-naya2003" role="doc-biblioref">Naya et al., 2003</a>)</span>. The medial temporal lobe (MTL) has an important role in interfacing high-level visual information represented in the inferotemporal cortex (IT) with long-term mnemonic information encoded in the hippocampal formation. It is composed of the perirhinal (PRh), entorhinal (ERh) and parahippocampal (PHC) cortices.</p>
<p>PRh is in particular involved in visual object categorization <span class="citation" data-cites="murray2001">(<a href="References.html#ref-murray2001" role="doc-biblioref">Murray and Richmond, 2001</a>)</span>, multimodal integration <span class="citation" data-cites="taylor2006">(<a href="References.html#ref-taylor2006" role="doc-biblioref">Taylor et al., 2006</a>)</span>, long-term memory encoding <span class="citation" data-cites="buffalo2000">(<a href="References.html#ref-buffalo2000" role="doc-biblioref">Buffalo et al., 2000</a>)</span> and retrieval <span class="citation" data-cites="brown1998">(<a href="References.html#ref-brown1998" role="doc-biblioref">Brown and Xiang, 1998</a>)</span>. In visual object categorization, PRh develops view-independent representation of objects: objects are in general seen from particular angles or are only partially visible. PRh learns to integrate over time these different views and bind them together in a unitary representation. In the model of PRh we developed <span class="citation" data-cites="Vitay2008">(<a href="References.html#ref-Vitay2008" role="doc-biblioref">Vitay and Hamker, 2008</a>)</span>, PRh is represented by two populations of excitatory and inhibitory neurons, respectively, with biologically plausible proportions and connectivity. Different objects are presented to the model through connections from a model of IT to the excitatory neurons. Each object is composed of different parts, which are randomly selected at each presentation: for example the first presentation of a chair would contain its right side and three feet, the second would be its back and only two feet, and so on. Through plasticity in the lateral connections between the excitatory neurons, we observe the formation of connected clusters of neurons which represent the object as whole: individual neurons of the cluster receive visual input from only one part of the object, but they have become connected to neurons representing all the other parts of the object.</p>
<p>Sustained activation has been observed in PRh during delayed matching-to-sample (DMS) tasks, where a visual object (the sample) is shortly presented and removed for a variable duration called the delay period. The same or a different object (the match) is then presented and the subject has to respond if the new object matches the sample. PRh neurons representing the sample object stay active during the delay period <span class="citation" data-cites="Nakamura1995">(<a href="References.html#ref-Nakamura1995" role="doc-biblioref">Nakamura and Kubota, 1995</a>)</span>. The model reproduces this effect by incorporating the effect of DA on synaptic transmission in the cortex, extrapolated from its known influence in the prefrontal cortex <span class="citation" data-cites="Seamans2004 durstewitz2000">(<a href="References.html#ref-durstewitz2000" role="doc-biblioref">Durstewitz et al., 2000</a>; <a href="References.html#ref-Seamans2004" role="doc-biblioref">Seamans and Yang, 2004</a>)</span>. We observed that PRh neurons show sustained activation under intermediate levels of DA, but not low or high doses, a phenomenon known as <em>inverted-U curve</em> in the prefrontal cortex <span class="citation" data-cites="Vijayraghavan2007">(<a href="References.html#ref-Vijayraghavan2007" role="doc-biblioref">Vijayraghavan et al., 2007</a>)</span>. Moreover, intermediate levels of DA favor the propagation of activity within a cluster: while at low DA levels only the neurons receiving visual information get activated, the whole cluster gets activated at intermediate levels because of the enhanced lateral connections within the cluster. Instead of representing a partial view of the object, PRh represents all possible views at the same time, leading to a complete representation of the object. This provides a mechanism by which DA modulates processing in PRh and allows memory retrieval.</p>
<p>The mechanisms used in this model are a very important step for visual processing as they allow view-invariant representations of an object to be formed and retrieved by cognitive processes. Under optimal DA levels, object representations can be completed and help categorization. Furthermore, the visual template representing an object in PRh can be activated by cognitive processes (either through direct projections from the PFC or through the thalamus) and used to guide visual search. The visual system is principally organized in two separate pathways: the ventral pathway, originating in the primary visual cortex (V1) and ending in the inferotemporal lobe, is specialized in object recognition; the dorsal pathway, originating in V1 and ending in the parietal cortex, focuses on the localization of visual objects and their manipulation <span class="citation" data-cites="ungerleider1982">(<a href="References.html#ref-ungerleider1982" role="doc-biblioref">Ungerleider and Mishkin, 1982</a>)</span>. Activating a template in PRh biases IT toward the characteristic features of this object, which itself biases representations in the ventral pathway through feedback projections. Once the corresponding features are enhanced in V1, the dorsal pathway can then locate the object and direct an action toward it <span class="citation" data-cites="Hamker2004a Hamker2005">(<a href="References.html#ref-Hamker2004a" role="doc-biblioref">Hamker, 2004</a>; <a href="References.html#ref-Hamker2005" role="doc-biblioref">Hamker, 2005</a>)</span>. Understanding how visual templates are formed and retrieved is a first step toward understanding the cognitive control of vision.</p>
<section id="insights-on-the-role-of-da" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="insights-on-the-role-of-da">Insights on the role of DA</h4>
<p>Tonic levels of DA control the processing properties of PRh by switching from a representational mode - only the perceived information is represented - to a mnemonic one - visual templates are completed or retrieved.</p>
</section>
</section>
<section id="chapter-3-basal-ganglia-and-memory-retrieval" class="level3" data-number="1.6.2">
<h3 data-number="1.6.2" class="anchored" data-anchor-id="chapter-3-basal-ganglia-and-memory-retrieval"><span class="header-section-number">1.6.2</span> Chapter 3 : Basal ganglia and memory retrieval</h3>
<p>Maintaining visual templates in PRh is a critical component of delayed rewarded tasks such as delayed match-to-sample (DMS, reward is delivered if a response is made when the target matches the sample), delayed non-match-to-sample (DNMS, the response is rewarded only if the target does not match the sample) or delayed pair-association (DPA, similar to DMS but there is a an arbitrary association between the sample and the rewarded target - e.g.&nbsp;respond for an apple when the sample is a car). The visual loop of the BG, linking high-level visual cortical areas such as IT and PRh with the body and tail of the caudate nucleus, is involved in selectively activating visual templates during the delay period of such tasks in order to prepare the correct response <span class="citation" data-cites="Levy1997">(<a href="References.html#ref-Levy1997" role="doc-biblioref">Levy et al., 1997</a>)</span>. The major difficulty of these three tasks is that the visual template to be activated can be different from the presented sample, so the target has to be retrieved from memory.</p>
<p>In <span class="citation" data-cites="Vitay2010">Vitay and Hamker (<a href="References.html#ref-Vitay2010" role="doc-biblioref">2010</a>)</span>, we developed a neuro-computational model of the visual loop of the BG. It is composed of a closed loop between PRh, the caudate nucleus, SNr and the ventro-anterior thalamus, and an open loop with a projection from the dlPFC to the caudate nucleus. Contrary to the generic scheme described on <a href="#fig-intro:bg_structure" class="quarto-xref">Figure&nbsp;<span>1.4</span></a>, we only modeled the direct pathway of this loop. In the experimental setup, a sample is first presented and stored in dlPFC. After a delay of 150 ms, a cue indicated which task to perform (DMS, DNMS or DPA) is presented and stored in dlPFC. Finally, after another delay, two stimuli are presented: the target (which matches the sample depending on the task) and a distractor. After a delay, we measure the maximal activity in PRh and deliver reward to SNc if the target has a higher activity. The dopaminergic signal in SNc in response to the reward modulates learning at corticostriatal synapses (both from PRh and dlPFC) according to the three-term DA-modulated Hebbian learning rule presented in this chapter. This is in line with many models of the BG <span class="citation" data-cites="Brown1999 OReilly2006">(e.g. <a href="References.html#ref-Brown1999" role="doc-biblioref">Brown et al., 1999</a>; <a href="References.html#ref-OReilly2006" role="doc-biblioref">O’Reilly and Frank, 2006</a>)</span>. The novelty of this model is that DA also modulates plasticity within the BG, in the connections from the striatum to SNr as well as in the lateral connections of SNr.</p>
<p>This internal plasticity, confirmed by experimental evidence <span class="citation" data-cites="Rueda-Orozco2009">(<a href="References.html#ref-Rueda-Orozco2009" role="doc-biblioref">Rueda-Orozco et al., 2009</a>)</span>, releases the constraints on the striatum. In other models, each striatal region converges on a small number of GPi/SNr cells, allowing to disinhibit a single action. The corticostriatal projections must therefore solve two different problems: integrating different cortical representations (here, the sample and the task cue) and map them on the correct action. If plasticity in the projection between the striatum and GPi/SNr is added, corticostriatal projections only need to map cortical associations on the striatum (a form of self-organization), while the striatopallidal ones learn to map these representations onto the correct action. Additionally, plasticity within SNr ensures selectiveness in the output of the BG.</p>
<p>The resulting model is able to learn through reinforcement learning the three tasks using a limited number of objects. It provides a novel mechanism by which cognitive processes in the PFC can learn to influence visual processing by retrieving visual templates. Two limitations of this model should be outlined: first, it only considers the direct pathway of the BG, neglecting the indirect and hyperdirect ones; second, the mechanisms to encode the sample and the task cue in working memory in dlPFC are hard-coded and not learned. The first limitation was since overcome by an extension of this model including the indirect and hyperdirect pathways, with a strong emphasis on the dopamine-modulated plasticity in these pathways. This extended model was successfully used to explain cognitive deficits in various BG-related diseases, such as Parkinson’s disease <span class="citation" data-cites="Schroll2014">(<a href="References.html#ref-Schroll2014" role="doc-biblioref">Schroll et al., 2014</a>)</span> and Huntington’s disease <span class="citation" data-cites="Schroll2015a">(<a href="References.html#ref-Schroll2015a" role="doc-biblioref">Schroll et al., 2015</a>)</span>. Flexible WM mechanisms to learn to maintain relevant information in dlPFC are presented in the next section <span class="citation" data-cites="Schroll2012">(<a href="References.html#ref-Schroll2012" role="doc-biblioref">Schroll et al., 2012</a>)</span>.</p>
<section id="insights-on-the-role-of-da-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="insights-on-the-role-of-da-1">Insights on the role of DA</h4>
<p>Dopamine regulates plasticity in the projections to the BG, but also between the different nuclei of the BG. Its phasic component carries a reward-prediction error that reinforces successful stimulus-response associations. DA-mediated plasticity occurs only in the acquisition phase, when the success of a response is not predicted yet. When a striatal representation is associated with reward delivery, it cancels dopaminergic activation and suppresses learning.</p>
</section>
</section>
<section id="chapter-4-wm-and-multiple-basal-ganglia-loops" class="level3" data-number="1.6.3">
<h3 data-number="1.6.3" class="anchored" data-anchor-id="chapter-4-wm-and-multiple-basal-ganglia-loops"><span class="header-section-number">1.6.3</span> Chapter 4 : WM and multiple basal ganglia loops</h3>
<p>Updating and maintaining information in WM is a complex cognitive process involving mainly the dlPFC and the BG <span class="citation" data-cites="frank2001">(<a href="References.html#ref-frank2001" role="doc-biblioref">Frank et al., 2001</a>)</span>, although many other cortical areas play a significant role <span class="citation" data-cites="Jonides1998 ashby2005">(<a href="References.html#ref-ashby2005" role="doc-biblioref">Ashby et al., 2005</a>; <a href="References.html#ref-Jonides1998" role="doc-biblioref">Jonides et al., 1998</a>)</span>. Many neuro-computational models consider that the BG is involved only in WM updating, i.e.&nbsp;the conditional entry of stimuli into it <span class="citation" data-cites="Helie2013 Uttal2015">(<a href="References.html#ref-Helie2013" role="doc-biblioref">Helie et al., 2013</a>; <a href="References.html#ref-Uttal2015" role="doc-biblioref">Uttal, 2015</a>)</span>. One of the most prominent models of WM <span class="citation" data-cites="OReilly2006">(<a href="References.html#ref-OReilly2006" role="doc-biblioref">O’Reilly and Frank, 2006</a>)</span> for example considers the BG as a gating mechanism allowing, based on reinforcement learning, sensory information to enter recurrent loops within the PFC. It has among others been applied to the complex 1-2-AX task, which can be described as followed: a sequence of letters (A, B, X, Y) and digits (1, 2) is displayed on a screen. The subjects have to respond with the left button if they see an A followed by an X, but only if the last digit they saw was a 1. If that last digit was a 2, they have to press left when they see a B followed by a Y. In all other cases, they have to press right.</p>
<p>The 1-2-AX task is very complex, even for humans. It involves maintaining two levels of information in WM: what was the last digit I saw (outer loop) and have I just seen an A or a B (inner loop)? If these two pieces of information are kept in WM, deciding whether to press left or right when an X or Y appears becomes as trivial as a stimulus-response association. The difficulty is to know how a system can learn to maintain the outer and inner loops based solely on reinforcement learning, i.e.&nbsp;without explicit knowledge of the task. <span class="citation" data-cites="OReilly2006">O’Reilly and Frank (<a href="References.html#ref-OReilly2006" role="doc-biblioref">2006</a>)</span> solve the problem by implementing three parallel PFC/BG loops, one learning to maintain 1 and 2, another A and B and the last one X or Y. The structural credit assignment problem - if the response is incorrect, which of these three loops has failed? - is solved by allowing each loop to modulate its own dopaminergic reward signal, but these loops are mostly independent of each other. Moreover the BG are only used to update WM content, not actually maintain it, contrary to experimental evidence <span class="citation" data-cites="Landau2009">(<a href="References.html#ref-Landau2009" role="doc-biblioref">Landau et al., 2009</a>)</span>.</p>
<p>In <span class="citation" data-cites="Schroll2012">Schroll et al. (<a href="References.html#ref-Schroll2012" role="doc-biblioref">2012</a>)</span>, we proposed a neuro-computational model of WM updating and maintenance involving three PFC-BG loops: two associative loops and a motor one. The role of the motor loop is to decide which motor response (left or right) should be executed based on short-term mnemonic information maintained in the associative loops during a 1-2-AX task. The role of the two associative loops is to learn to maintain the outer (1 and 2) and inner (A and B) loops, respectively. Based on an idea by <span class="citation" data-cites="Krueger2009">Krueger and Dayan (<a href="References.html#ref-Krueger2009" role="doc-biblioref">2009</a>)</span>, we posit that shaping plays an important role in organizing the different loops: animals usually don’t address complex cognitive tasks directly, but incrementally generate more and more complex behavior by reusing abilities that were previously acquired. In the case of the 1-2-AX task, this would correspond to responding first to a 1 or 2, then to 1 followed by A or 2 followed by B, and finally by the 1-2-AX task task itself. Once a subtask is mastered, errors in performance can be interpreted as a change in task complexity, signaling that more cognitive resources should be allocated to solve the problem.</p>
<p>In the first shaping phase (only digits are presented), the motor PFC/BG loop learns to respond appropriately using the same mechanisms as in <span class="citation" data-cites="Vitay2010">Vitay and Hamker (<a href="References.html#ref-Vitay2010" role="doc-biblioref">2010</a>)</span>. When the second phase is introduced (A-X or B-Y), the motor loop can not solve the problem because it has no memory of the last digit seen. One of the two associative loops then starts learning to maintain this information through a closed loop. The sustained activation of a digit then biases the motor loop to respond correctly to a 1-A or 2-B association. Finally, when the full 1-2-AX task is introduced, the associative and motor loops fail again, as only the outer loop is maintained. The associative loop sends a “distress” signal, telling the other associative loop to help solve the task. The new loop then learns to maintain A and B, providing enough information to the motor loop to execute the correct motor response.</p>
<p>Associative PFC/BG loops learn from errors as long as they are not confident in their output. When they become confident but the whole behavior fails, they ask for more cognitive resources to be allocated to the task instead of simply unlearning what they were previously correctly doing. Communication between the loops and the subsequent recruitment of cognitive resources is based on the spiraling striato-nigro-striatal connectivity <span class="citation" data-cites="Haber2000">(<a href="References.html#ref-Haber2000" role="doc-biblioref">Haber et al., 2000</a>)</span>: each loop has its own dopaminergic signal, which can be activated by loops higher in the hierarchy. When the first associative loop fails to solve the task although it was previously performing well, it signals the second loop through its dopaminergic system that it should get engaged in order to improve the organism’s ability to acquire rewards.</p>
<p>Monitoring of performance is a crucial mechanism by which cognitive resources can be allocated to solve a problem. The brain does not relearn everything every time it is confronted with a new problem, it first tries already acquired solutions and only tries to combine or update them when the performance is not satisfying <span class="citation" data-cites="Botvinick2009">(<a href="References.html#ref-Botvinick2009" role="doc-biblioref">Botvinick et al., 2009</a>)</span>. Based on neuro-anatomy and the functional importance of dopamine in goal-directed behavior, the spiraling structure of the striato-nigro-striatal system is a good candidate to coordinate the flexible recruitment of PFC-BG loops. However, the anterior cingulate cortex (ACC) is known to be crucial in self-performance assessment and error monitoring. As ACC is involved in a PFC-BG loop located just in between the limbic regions (OFC, vmPFC) and the associative ones (dlPFC) <span class="citation" data-cites="Haber2010">(<a href="References.html#ref-Haber2010" role="doc-biblioref">Haber and Knutson, 2010</a>)</span>, its dominating position may be the crucial link to determine the involvement of different associative loops to solve cognitive problems. In all cases, understanding how the dopaminergic system processes reward expectations and errors in these different loops is important for the understanding of the organization of PFC-BG loops.</p>
<section id="insights-on-the-role-of-da-2" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="insights-on-the-role-of-da-2">Insights on the role of DA</h4>
<p>The activation of dopaminergic neurons is not uniform but specific to each PFC-BG loop. Different loops can control their learning ability by modulating their influx of dopamine. Moreover, the hierarchical organization of the reciprocal connections between the striatum and the dopaminergic areas allows the flexible recruitment of cognitive resources when needed.</p>
</section>
</section>
<section id="chapter-5-timing-and-expectation-of-reward" class="level3" data-number="1.6.4">
<h3 data-number="1.6.4" class="anchored" data-anchor-id="chapter-5-timing-and-expectation-of-reward"><span class="header-section-number">1.6.4</span> Chapter 5 : Timing and expectation of reward</h3>
<p>The TD error signal depends only on two pieces of information: the prediction of the value of a state (or action) and the reward actually received. As shown on <a href="#fig-intro:vta_afferents" class="quarto-xref">Figure&nbsp;<span>1.7</span></a>, VTA receives information from many other brain regions: a massive inhibitory projection from NAcc (possibly excitatory through a relay on the ventral pallidum - VP), direct cortical excitation from the PFC, excitatory connections from reward-related brainstem regions such as the pedunculopontine tegmental nucleus (PPTN), the lateral habenula (LHb) or the lateral hypothalamus (LH). As discovered recently, it also receives inhibitory connections from the mesopontine rostromedial tegmental nucleus <span class="citation" data-cites="Jhou2009 Bourdy2012">(<a href="References.html#ref-Bourdy2012" role="doc-biblioref">Bourdy and Barrot, 2012</a>; RMTg, <a href="References.html#ref-Jhou2009" role="doc-biblioref">Jhou et al., 2009</a>)</span>. Inhibitory neurons in the VTA furthermore control the activity of VTA cells and project on NAcc and PFC. The complexity of the afferent system to VTA suggests that it computes more than a simple reward-prediction error signal.</p>
<div id="fig-intro:vta_afferents" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intro:vta_afferents-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/intro/vta_afferents.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intro:vta_afferents-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.7: Major afferent areas to VTA. The prefrontal cortex (PFC), the basolateral amygdala (BLA), the ventral subiculum of the hippocampus (vSub/Hipp) project on the nucleus accumbens (NAc), which has a strong inhibitory influence on VTA. VTA also receives direct excitatory connections from the PFC. The pedunculopontine tegmental nucleus (PPTg), laterodorsal tegmental nucleus (LDT), lateral hypothalamic and lateral preoptic areas (LHA/LPOA), lateral habenula (LHb), among others, also provide excitatory inputs to the dopaminergic cells of VTA. The mesopontine rostromedial tegmental nucleus (RMTg) provides inhibitory input. VTA also comprises GABAergic cells, which inhibit the dopaminergic ones as well as the PFC and NAc. Adapted from <span class="citation" data-cites="Sesack2010">Sesack and Grace (<a href="References.html#ref-Sesack2010" role="doc-biblioref">2010</a>)</span>.
</figcaption>
</figure>
</div>
<p>Several neuro-computational models of the dopaminergic system have been proposed to explain this organization <span class="citation" data-cites="Brown1999 OReilly2007 Tan2008">(<a href="References.html#ref-Brown1999" role="doc-biblioref">Brown et al., 1999</a>; <a href="References.html#ref-OReilly2007" role="doc-biblioref">O’Reilly et al., 2007</a>; <a href="References.html#ref-Tan2008" role="doc-biblioref">Tan and Bullock, 2008</a>)</span>. A common point of these <em>dual-pathway</em> models is that they distinguish the excitatory and inhibitory components driving VTA activity for rewards and reward-predicting stimuli, although some debate exists on the exact structures carrying these informations. The DA burst in response to delivery of reward likely originates from the PPTN, while the cancellation of this response when the reward is fully predicted originates from the striatum. Reward-predicting stimuli activate VTA either though the excitatory projection from PFC or from the amygdala. The main difference between those models is how the temporal component of the DA signal is computed: in the experiments of <span class="citation" data-cites="Schultz1997">Schultz et al. (<a href="References.html#ref-Schultz1997" role="doc-biblioref">1997</a>)</span>, VTA shows a dip below baseline at the exact time where a reward was expected but did not occur. As no sensory event happens at this time, this indicates that internal timing mechanisms are involved in generating the DA signal.</p>
<p>The hypothesis taken by <span class="citation" data-cites="Brown1999">Brown et al. (<a href="References.html#ref-Brown1999" role="doc-biblioref">1999</a>)</span> and <span class="citation" data-cites="Tan2008">Tan and Bullock (<a href="References.html#ref-Tan2008" role="doc-biblioref">2008</a>)</span> is that the striatum implements a <em>spectral timing</em> mechanism <span class="citation" data-cites="Grossberg1989">(<a href="References.html#ref-Grossberg1989" role="doc-biblioref">Grossberg and Schmajuk, 1989</a>)</span> where striatal neurons have intracellular calcium levels which peak at different times after stimulus onset: detecting these peaks allows to estimate the time elapsed since onset. Because of the lack of evidence for such a mechanism, we decided in <span class="citation" data-cites="Vitay2014">Vitay and Hamker (<a href="References.html#ref-Vitay2014" role="doc-biblioref">2014</a>)</span> to investigate alternative mechanisms for interval timing. A successful model of interval timing is the <em>Striatal-Beat Frequency</em> model <span class="citation" data-cites="Matell2004">(<a href="References.html#ref-Matell2004" role="doc-biblioref">Matell and Meck, 2004</a>)</span>. The basic principle is that cortical neurons behave as oscillators at different frequencies which are synchronized at stimulus onset. The population code composed by these oscillators provides a unique description of the time elapsed since onset: if enough neurons and a large enough range of frequencies are used, the population will never display twice the same pattern, while being reproducible between different trials. Striatal neurons can then detect the elapsed duration by learning to respond to the cortical pattern present when reward is delivered: the DA burst at reward delivery influences plasticity at corticostriatal synapses so they become selective only for that pattern. This model captures many aspects of the link between dopaminergic activity and timing processes, including the accelerated sense of time when DA is elevated - for example in aroused states or during recreational drug use - or the effect of lesions of SNc/VTA or the striatum on interval timing <span class="citation" data-cites="Coull2011">(<a href="References.html#ref-Coull2011" role="doc-biblioref">Coull et al., 2011</a>)</span>.</p>
<p>Using this hypothesis, we developed a novel neuro-computational model shedding new light on the afferent system to VTA based on neuro-anatomical evidence. Although the response to primary rewards is classically mediated through PPTN, we propose that conditioned stimuli activate VTA through the existing connection between the amygdala - a structure known for its involvement in classical conditioning - and PPTN. Furthermore, we propose that the cancellation of the DA burst when a reward is predicted and the DA dip when a reward is omitted are processed by two different mechanisms: the direct inhibitory projection from NAcc to VTA can inhibit the response to primary rewards, but bringing VTA activity below baseline requires a complex sub-network linking the ventral BG (NAcc and VP) to VTA through LHb and RMTg.</p>
<p>The model is able to reproduce a wealth of experimental findings: the progressive appearance of phasic bursts at CS onset through classical conditioning, the progressive canceling of the amplitude of the phasic bursts elicited by primary rewards, the strong phasic inhibition at the time when reward is expected but not delivered, the dependency on reward magnitude of the activities in BLA and VTA, the response to reward delivered earlier than expected <span class="citation" data-cites="Schultz1997 Fiorillo2003 Pan2005a">(<a href="References.html#ref-Fiorillo2003" role="doc-biblioref">Fiorillo et al., 2003</a>; <a href="References.html#ref-Pan2005a" role="doc-biblioref">Pan and Hyland, 2005</a>; <a href="References.html#ref-Schultz1997" role="doc-biblioref">Schultz et al., 1997</a>)</span>. This model is currently limited to VTA activity during classical conditioning but provides a detailed functional basis to address the mechanisms of dopamine release in the PFC-BG system.</p>
<section id="insights-on-the-role-of-da-3" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="insights-on-the-role-of-da-3">Insights on the role of DA</h4>
<p>The dopaminergic system integrates information from diverse structures, signaling reward delivery, prediction and omission through different projections. Cognitive, motor and emotional information converge on the dopaminergic system, which then redistributes back the most relevant aspects. It is critically involved in timing processes and therefore the organization of behavior through time.</p>
</section>
</section>
<section id="chapter-6-neural-simulator-annarchy" class="level3" data-number="1.6.5">
<h3 data-number="1.6.5" class="anchored" data-anchor-id="chapter-6-neural-simulator-annarchy"><span class="header-section-number">1.6.5</span> Chapter 6 : Neural simulator ANNarchy</h3>
<p>Neuro-computational models are described by a limited set of information:</p>
<ol type="1">
<li>The number of populations of neurons (or areas), the number of neurons in each population and possibly a topology;</li>
<li>A set of ordinary differential equations (ODE) describing the dynamics of each neuron model in the model;</li>
<li>Connectivity patterns for the projections between the populations: all-to-all, probabilistic, distance-based, etc;</li>
<li>A set of ODEs describing the dynamics of synaptic plasticity for the projections;</li>
<li>Methods to provide inputs and read out outputs of the network.</li>
</ol>
<p>Some of these informations can be inferred from anatomical and physiological data. Neural and synaptic dynamics are well studied, so only small modifications usually need to be applied to standard models. The main difficulty is actually to find sensible values for the free parameters of the model: time constants, learning rates, etc. Although experimental data constrain the range of possible values, this is the most time-consuming part of the design of a neuro-computational model.</p>
<p>Another difficulty is that neural networks can very quickly become expensive to simulate: the number of connections grow quadratically with the number of neurons and the computations can become very slow if no special care is taken about the optimality of the implementation. Parallel computing offers many advantages for the simulation of neural networks as each neuron only processes local information, but writing optimized parallel code on different hardware (shared-memory systems, distributed systems or recently general-purpose graphical cards - GPU) can be quite difficult and time-consuming.</p>
<p>Consequently, researchers in computational neuroscience use neural simulators instead of writing their own simulation code. These are libraries allowing the definition of a model, usually in a high-level scripting language such as Python or Matlab, and hiding from the user all the low-level implementation details necessary to run efficiently simulations in parallel. Another positive side effect is that neural simulators facilitate the exchange of models between researchers for validation and the integration of different models to obtain more functionalities.</p>
<p>Many different neural simulators are available to the community: NEURON, NEST, GENESIS, Brian, GeNN, Auryn <span class="citation" data-cites="Brette2007">Vitay et al. (<a href="References.html#ref-Vitay2015" role="doc-biblioref">2015</a>)</span>. They all have different strengths and drawbacks: the exhaustiveness of the set of neural and synaptic models which can be included in a model, the simplicity of the interface, their optimization for a particular parallel hardware, etc. These simulators focus on the simulation of spiking networks, where neurons exchange information through discrete events (spikes), while rate-coded models, where neurons exchange directly a firing rate, are usually impossible or very difficult to define. At the exception of Brian, these simulators provide a fixed set of neural and synaptic models which can only be extended with great difficulty: as long as one only needs standard models, these simulators are very practical, but if one wants to investigate new mechanisms, the programming effort becomes important. Brian proposes a very flexible code generation approach, where neural and synaptic dynamics are described using a text-based equation-oriented mathematical description which is used to generate Python code at run-time <span class="citation" data-cites="Stimberg2014">(<a href="References.html#ref-Stimberg2014" role="doc-biblioref">Stimberg et al., 2014</a>)</span>. Using code generation allows the user to define virtually any neural or synaptic model.</p>
<p>In parallel to the design of the neuro-computational models presented above, I developed over several years a neural simulator named ANNarchy (Artificial Neural Networks architect), later in collaboration with Helge Ülo Dinkelbach. Two main principles guided the development: first, it should allow the rapid definition of neural networks, for both rate-coded and spiking models. Second, the simulation should be able to run transparently and efficiently on different parallel hardware (using OpenMP for shared-memory systems, MPI for distributed ones and CUDA for GPUs). Code generation is the core principle of the simulator: the definition of the network in a Python script is analyzed and used to generate entirely the simulation code (including a translation from the text-based description of ODEs to executable code statements), using templates adapted to the parallel framework.</p>
<p>In <span class="citation" data-cites="Vitay2015">Vitay et al. (<a href="References.html#ref-Vitay2015" role="doc-biblioref">2015</a>)</span>, we presented the neural simulator to the community and showed that its parallel performance is at least comparable to the alternatives. It is freely available and released under an open-source license. In addition to being used inside the professorship of Artificial Intelligence of the TU Chemnitz, several research groups have shown interest in this simulator and have started using it for their own research. More than just a tool, ANNarchy is also a very promising platform to study the issues raised by neuro-computational models to the parallel computing community: relying on code generation, it allows to explore systematically the different optimizations and algorithms that allow specific networks to be simulated efficiently on different hardware.</p>
</section>
</section>
<section id="conclusion" class="level2" data-number="1.7">
<h2 data-number="1.7" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">1.7</span> Conclusion</h2>
<p>The common theme of this thesis is the role of dopamine in the cognitive, motor and emotional processes involved in goal-directed behavior. Using biologically-realistic neuro-computational models, I investigated its role in visual object categorization and memory retrieval <span class="citation" data-cites="Vitay2008">(<a href="References.html#ref-Vitay2008" role="doc-biblioref">Vitay and Hamker, 2008</a>)</span>, reinforcement learning and action selection <span class="citation" data-cites="Vitay2010">(<a href="References.html#ref-Vitay2010" role="doc-biblioref">Vitay and Hamker, 2010</a>)</span>, the updating, learning and maintenance of working memory <span class="citation" data-cites="Schroll2012">(<a href="References.html#ref-Schroll2012" role="doc-biblioref">Schroll et al., 2012</a>)</span> and timing processes <span class="citation" data-cites="Vitay2014">(<a href="References.html#ref-Vitay2014" role="doc-biblioref">Vitay and Hamker, 2014</a>)</span>. The involvement of dopamine in such a wide variety of processes highlights the importance of understanding the mechanisms leading to dopamine release as well as its effect on the activity and plasticity of cortical and subcortical structures.</p>
<p>The different models outline different facets of the effect of DA release in the brain. In the cerebral cortex, the most important effect of DA is the modulation of synaptic transmission in localized networks of excitatory and inhibitory neurons. DA release in the prefrontal cortex influences short-term memory processes by inducing two modes of computation: an “open gate” mode, where multiple sensory information can enter the neural substrate and be represented in parallel; and a “closed gate” mode, where only the strongest and most important representation is maintained, allowing sustained activation <span class="citation" data-cites="Seamans2004">(<a href="References.html#ref-Seamans2004" role="doc-biblioref">Seamans and Yang, 2004</a>)</span>. The transition between these two modes follows an inverted U-curve, where low and high DA levels lead to open gates and intermediate levels to closed gates. The proposed model of PRh <span class="citation" data-cites="Vitay2008">(<a href="References.html#ref-Vitay2008" role="doc-biblioref">Vitay and Hamker, 2008</a>)</span> exhibits a similar mechanism: PRh can switch between a representational state (driven by inputs) and a mnemonic state (where visual memory is retrieved) depending on the modulatory influence of DA on synaptic transmission. It is likely that DA is able to induce such different modes of computation in all cortical areas receiving dopaminergic input (the whole frontal lobe, the inferotemporal and parietal cortices). The functional consequences of this property still need to be explored, especially with respect to the spatial scale: do all these cortical areas receive the same dopaminergic input from VTA, or is there a functional topology allowing to selectively switch single areas?</p>
<p>In the basal ganglia, the main mode of action considered in the models is the inducement of plasticity by phasic DA bursts or dips. These short-term deviations around the baseline shape synapses coming from the cortex, but also inside the BG. Although more complex models of plasticity have been used, their influence basically follows a three-term DA-modulated Hebbian learning rule. DA bursts reinforce PFC/BG representations which lead to reward, while DA dips “punish” the ones which led to omission of reward or punishment <span class="citation" data-cites="Vitay2010 Schroll2012">(<a href="References.html#ref-Schroll2012" role="doc-biblioref">Schroll et al., 2012</a>; <a href="References.html#ref-Vitay2010" role="doc-biblioref">Vitay and Hamker, 2010</a>)</span>. This mechanism is fundamentally in line with actor/critic analogies. The short-term duration and the short latency of these phasic responses furthermore allow DA to signal precisely the occurrence of meaningful events, what can be used to learn time intervals and provide an internal sense of elapsed time <span class="citation" data-cites="Vitay2015">(<a href="References.html#ref-Vitay2015" role="doc-biblioref">Vitay et al., 2015</a>)</span>. One aspect of dopamine that will be addressed by future work is the influence of its tonic activity on the BG, which are known to influence the strength and vigor of motor responses as well as the exploration/exploitation trade-off <span class="citation" data-cites="Niv2007 Beeler2010">(<a href="References.html#ref-Beeler2010" role="doc-biblioref">Beeler et al., 2010</a>; <a href="References.html#ref-Niv2007" role="doc-biblioref">Niv et al., 2007</a>)</span>.</p>
<p>An important mechanism proposed in this work is how multiple PFC/BG loops can communicate by influencing each other’s dopaminergic signal. The striato-nigro-striatal connectivity is a remarkable anatomical property whose functional consequences remain largely unexplored. We proposed in <span class="citation" data-cites="Schroll2012">Schroll et al. (<a href="References.html#ref-Schroll2012" role="doc-biblioref">2012</a>)</span> that it provides a mechanism allowing PFC/BG loops to recruit other loops when the task becomes too complex. The ability of each loop to control its dopaminergic input is here fundamental: by knowing how well it performs on a task, it can know if a mistake is its own responsibility, in which case it should continue learning, or if it should rather ask for more cognitive resources to solve the task. This mechanism is fundamental for life-long learning: complex behaviors emerge by composing already acquired simple behaviors, not by learning them from scratch. Future work will broaden this idea to other systems, especially the coordination between the limbic and associative loops which form the basis of goal-directed behavior.</p>
<p>Without a deep comprehension of the neural mechanisms underlying dopamine activity, it would be difficult to design artificial systems showing an intelligent and flexible organization of behavior. Research in computational neuroscience has therefore the opportunity to advance considerably artificial intelligence by transposing biological principles into flexible algorithms. In the proposed work, goal-directed learning focuses on extrinsic rewards. Intrinsic rewards are able to generate more interesting behaviors, such as the discovery of relevant information driven by curiosity or playfulness. Fortunately, dopamine influences similarly the structures responsible for these behaviors and the ones involved with extrinsic rewards, so the principles presented in this thesis will be useful to design such systems. However, intrinsic rewards require an internal state to be acted upon: a core idea of intrinsic motivation is that some actions are directed toward maintaining the system in its “comfort zone” - the homeostasis, for example maintaining the body’s temperature, satiety or safeness - while others on the contrary are the consequence of drives that can never be satiated - curiosity can for example never be completely satisfied, so it keeps the organism exploring its environment <span class="citation" data-cites="Oudeyer2007">(see <a href="References.html#ref-Oudeyer2007" role="doc-biblioref">Oudeyer and Kaplan, 2007</a> for a typology of intrinsic motivation)</span>. This internal state obviously requires a body, so that actions acquire a better meaning than simply collecting external rewards. This outlines the importance of embodiment and future work will address the implementation of the proposed models on robotic platforms.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-Albin2006" class="csl-entry" role="listitem">
Albin, R. L., and Mink, J. W. (2006). Recent advances in tourette syndrome research. <em>Trends Neurosci</em> 29, 175–182. doi:<a href="https://doi.org/10.1016/j.tins.2006.01.001">10.1016/j.tins.2006.01.001</a>.
</div>
<div id="ref-Albin1989" class="csl-entry" role="listitem">
Albin, R. L., Young, A. B., and Penney, J. B. (1989). <a href="https://www.ncbi.nlm.nih.gov/pubmed/2479133">The functional anatomy of basal ganglia disorders.</a> <em>Trends Neurosci</em> 12, 366–375.
</div>
<div id="ref-albus1971" class="csl-entry" role="listitem">
Albus, J. S. (1971). A theory of cerebellar function. <em>Math Biosci</em>, 25–61.
</div>
<div id="ref-Alexander1986" class="csl-entry" role="listitem">
Alexander, G. E., DeLong, M. R., and Strick, P. L. (1986). Parallel organization of functionally segregated circuits linking the basal ganglia and cortex. <em>Ann Rev Neurosci</em> 9, 357–381.
</div>
<div id="ref-ashby2005" class="csl-entry" role="listitem">
Ashby, F. G., Ell, S. W., Valentin, V. V., and Casale, M. B. (2005). FROST: A distributed neurocomputational model of working memory maintenance. <em>J Cogn Neurosci</em> 17, 1728–1743.
</div>
<div id="ref-Baddeley1986" class="csl-entry" role="listitem">
Baddeley, A. D. (1986). <em>Working memory</em>. Oxford: Oxford University Press.
</div>
<div id="ref-Bahuguna2015" class="csl-entry" role="listitem">
Bahuguna, J., Aertsen, A., and Kumar, A. (2015). Existence and control of go/no-go decision transition threshold in the striatum. <em>PLoS Comput Biol</em> 11, e1004233. doi:<a href="https://doi.org/10.1371/journal.pcbi.1004233">10.1371/journal.pcbi.1004233</a>.
</div>
<div id="ref-Baldassarre2013a" class="csl-entry" role="listitem">
Baldassarre, G., Mannella, F., Fiore, V. G., Redgrave, P., Gurney, K., and Mirolli, M. (2013). Intrinsically motivated action-outcome learning and goal-based action recall: A system-level bio-constrained computational model. <em>Neural Netw</em> 41, 168–187. doi:<a href="https://doi.org/10.1016/j.neunet.2012.09.015">10.1016/j.neunet.2012.09.015</a>.
</div>
<div id="ref-Balleine1998" class="csl-entry" role="listitem">
Balleine, B. W., and Dickinson, A. (1998). <a href="https://www.ncbi.nlm.nih.gov/pubmed/9704982">Goal-directed instrumental action: Contingency and incentive learning and their cortical substrates.</a> <em>Neuropharmacology</em> 37, 407–419.
</div>
<div id="ref-Bar-Gad2003" class="csl-entry" role="listitem">
Bar-Gad, I., Morris, G., and Bergman, H. (2003). Information processing, dimensionality reduction and reinforcement learning in the basal ganglia. <em>Prog Neurobiol</em> 71, 439–473. doi:<a href="https://doi.org/10.1016/j.pneurobio.2003.12.001">10.1016/j.pneurobio.2003.12.001</a>.
</div>
<div id="ref-Barto2013" class="csl-entry" role="listitem">
Barto, A., Mirolli, M., and Baldassarre, G. (2013). Novelty or surprise? <em>Front Psychol</em> 4, 907. doi:<a href="https://doi.org/10.3389/fpsyg.2013.00907">10.3389/fpsyg.2013.00907</a>.
</div>
<div id="ref-Beeler2010" class="csl-entry" role="listitem">
Beeler, J. A., Daw, N., Frazier, C. R. M., and Zhuang, X. (2010). Tonic dopamine modulates exploitation of reward learning. <em>Front Behav Neurosci</em> 4, 170. doi:<a href="https://doi.org/10.3389/fnbeh.2010.00170">10.3389/fnbeh.2010.00170</a>.
</div>
<div id="ref-berns1998" class="csl-entry" role="listitem">
Berns, G., and Sejnowski, T. (1998). A computational model of how the basal ganglia produce sequences. <em>J Cogn Neurosci</em> 10, 108–121.
</div>
<div id="ref-Berridge2007" class="csl-entry" role="listitem">
Berridge, K. C. (2007). The debate over dopamine’s role in reward: The case for incentive salience. <em>Psychopharmacology (Berl)</em> 191, 391–431. doi:<a href="https://doi.org/10.1007/s00213-006-0578-x">10.1007/s00213-006-0578-x</a>.
</div>
<div id="ref-Bienenstock1982" class="csl-entry" role="listitem">
Bienenstock, E. L., Cooper, L. N., and Munro, P. W. (1982). <a href="https://www.ncbi.nlm.nih.gov/pubmed/7054394">Theory for the development of neuron selectivity: Orientation specificity and binocular interaction in visual cortex.</a> <em>J Neurosci</em> 2, 32–48.
</div>
<div id="ref-Botvinick2009" class="csl-entry" role="listitem">
Botvinick, M. M., Niv, Y., and Barto, A. C. (2009). Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective. <em>Cognition</em> 113, 262–280. doi:<a href="https://doi.org/10.1016/j.cognition.2008.08.011">10.1016/j.cognition.2008.08.011</a>.
</div>
<div id="ref-Bourdy2012" class="csl-entry" role="listitem">
Bourdy, R., and Barrot, M. (2012). <span class="nocase">A new control center for dopaminergic systems: pulling the VTA by the tail.</span> <em>Trends Neurosci.</em> 35, 681–90. doi:<a href="https://doi.org/10.1016/j.tins.2012.06.007">10.1016/j.tins.2012.06.007</a>.
</div>
<div id="ref-Brette2007" class="csl-entry" role="listitem">
Brette, R., Rudolph, M., Carnevale, T., Hines, M., Beeman, D., Bower, J. M., et al. (2007). <span class="nocase">Simulation of networks of spiking neurons: a review of tools and strategies.</span> <em>J. Comput. Neurosci.</em> 23, 349–98. doi:<a href="https://doi.org/10.1007/s10827-007-0038-6">10.1007/s10827-007-0038-6</a>.
</div>
<div id="ref-Brown1999" class="csl-entry" role="listitem">
Brown, J., Bullock, D., and Grossberg, S. (1999). <span class="nocase">How the basal ganglia use parallel excitatory and inhibitory learning pathways to selectively respond to unexpected rewarding cues.</span> <em>J. Neurosci.</em> 19, 10502–11. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/10575046">http://www.ncbi.nlm.nih.gov/pubmed/10575046</a>.
</div>
<div id="ref-Brown2012" class="csl-entry" role="listitem">
Brown, M. T. C., Tan, K. R., O’Connor, E. C., Nikonenko, I., Muller, D., and Lüscher, C. (2012). Ventral tegmental area GABA projections pause accumbal cholinergic interneurons to enhance associative learning. <em>Nature</em> 492, 452–456. doi:<a href="https://doi.org/10.1038/nature11657">10.1038/nature11657</a>.
</div>
<div id="ref-brown1998" class="csl-entry" role="listitem">
Brown, M. W., and Xiang, J. Z. (1998). Recognition memory: Neuronal substrates of the judgement of prior occurrence. <em>Prog Neurobiol</em> 55, 149–189.
</div>
<div id="ref-buffalo2000" class="csl-entry" role="listitem">
Buffalo, E. A., Ramus, S. J., Squire, L. R., and Zola, S. M. (2000). Perception and recognition memory in monkeys following lesions of area TE and perirhinal cortex. <em>Learn Mem</em> 7, 375–382.
</div>
<div id="ref-Burgess2007" class="csl-entry" role="listitem">
Burgess, N., Barry, C., and O’Keefe, J. (2007). An oscillatory interference model of grid cell firing. <em>Hippocampus</em> 17, 801–812.
</div>
<div id="ref-Cabanac1971" class="csl-entry" role="listitem">
Cabanac, M. (1971). <a href="https://www.ncbi.nlm.nih.gov/pubmed/5098954">Physiological role of pleasure.</a> <em>Science</em> 173, 1103–1107.
</div>
<div id="ref-Calabresi2007" class="csl-entry" role="listitem">
Calabresi, P., Picconi, B., Tozzi, A., and Di Filippo, M. (2007). <span class="nocase">Dopamine-mediated regulation of corticostriatal synaptic plasticity.</span> <em>Trends Neurosci.</em> 30, 211–9. doi:<a href="https://doi.org/10.1016/j.tins.2007.03.001">10.1016/j.tins.2007.03.001</a>.
</div>
<div id="ref-Chen2013" class="csl-entry" role="listitem">
Chen, J. Y., Wang, E. A., Cepeda, C., and Levine, M. S. (2013). Dopamine imbalance in huntington’s disease: A mechanism for the lack of behavioral flexibility. <em>Front Neurosci</em> 7, 114. doi:<a href="https://doi.org/10.3389/fnins.2013.00114">10.3389/fnins.2013.00114</a>.
</div>
<div id="ref-Chevalier1990" class="csl-entry" role="listitem">
Chevalier, G., and Deniau, J. M. (1990). <a href="https://www.ncbi.nlm.nih.gov/pubmed/1695403">Disinhibition as a basic process in the expression of striatal functions.</a> <em>Trends Neurosci</em> 13, 277–280.
</div>
<div id="ref-Corbit2011" class="csl-entry" role="listitem">
Corbit, L. H., and Balleine, B. W. (2011). <span class="nocase">The general and outcome-specific forms of Pavlovian-instrumental transfer are differentially mediated by the nucleus accumbens core and shell.</span> <em>J. Neurosci.</em> 31, 11786–94. doi:<a href="https://doi.org/10.1523/JNEUROSCI.2711-11.2011">10.1523/JNEUROSCI.2711-11.2011</a>.
</div>
<div id="ref-Coull2011" class="csl-entry" role="listitem">
Coull, J. T., Cheng, R.-K., and Meck, W. H. (2011). <span class="nocase">Neuroanatomical and neurochemical substrates of timing.</span> <em>Neuropsychopharmacology</em> 36, 3–25. doi:<a href="https://doi.org/10.1038/npp.2010.113">10.1038/npp.2010.113</a>.
</div>
<div id="ref-Creed2014" class="csl-entry" role="listitem">
Creed, M. C., Ntamati, N. R., and Tan, K. R. (2014). VTA GABA neurons modulate specific learning behaviors through the control of dopamine and cholinergic systems. <em>Front. Behav. Neurosci.</em> 8, 8. doi:<a href="https://doi.org/10.3389/fnbeh.2014.00008">10.3389/fnbeh.2014.00008</a>.
</div>
<div id="ref-damasio1994" class="csl-entry" role="listitem">
Damasio, A. R. (1994). <em>Descartes’ error: Emotion, reason and the human brain</em>. New York: Grosset/Putnam.
</div>
<div id="ref-daw2002" class="csl-entry" role="listitem">
Daw, N. D., and Touretzky, D. S. (2002). Long-term reward prediction in TD models of the dopamine system. <em>Neural Comput</em> 14, 2567–2583.
</div>
<div id="ref-DeLong1990" class="csl-entry" role="listitem">
DeLong, M. R. (1990). <a href="https://www.ncbi.nlm.nih.gov/pubmed/1695404">Primate models of movement disorders of basal ganglia origin.</a> <em>Trends Neurosci</em> 13, 281–285.
</div>
<div id="ref-Doya2006" class="csl-entry" role="listitem">
Doya, K., Ishii, S., Pouget, A., and Rao, R. P. N. eds. (2006). <em>Bayesian brain: Probabilistic approaches to neural coding</em>. The MIT Press.
</div>
<div id="ref-Dranias2008" class="csl-entry" role="listitem">
Dranias, M. R., Grossberg, S., and Bullock, D. (2008). <span class="nocase">Dopaminergic and non-dopaminergic value systems in conditioning and outcome-specific revaluation.</span> <em>Brain Res.</em> 1238, 239–87. doi:<a href="https://doi.org/10.1016/j.brainres.2008.07.013">10.1016/j.brainres.2008.07.013</a>.
</div>
<div id="ref-durstewitz2000" class="csl-entry" role="listitem">
Durstewitz, D., Seamans, J. K., and Sejnowski, T. J. (2000). Neurocomputational models of working memory. <em>Nat Neurosci Supp</em> 3, 1184–1191.
</div>
<div id="ref-Ebner2015" class="csl-entry" role="listitem">
Ebner, C., Schroll, H., Winther, G., Niedeggen, M., and Hamker, F. H. (2015). Open and closed cortico-subcortical loops: A neuro-computational account of access to consciousness in the distractor-induced blindness paradigm. <em>Conscious Cogn</em> 35, 295–307. doi:<a href="https://doi.org/10.1016/j.concog.2015.02.007">10.1016/j.concog.2015.02.007</a>.
</div>
<div id="ref-Everitt2001" class="csl-entry" role="listitem">
Everitt, B. J., Dickinson, A., and Robbins, T. W. (2001). <a href="https://www.ncbi.nlm.nih.gov/pubmed/11690609">The neuropsychological basis of addictive behaviour.</a> <em>Brain Res Brain Res Rev</em> 36, 129–138.
</div>
<div id="ref-Fieres2008" class="csl-entry" role="listitem">
Fieres, J., Schemmel, J., and Meier, K. (2008). Realizing biological spiking network models in a configurable wafer-scale hardware system. in <em>Neural networks, 2008. IJCNN 2008. (IEEE world congress on computational intelligence)</em>, 969–976. doi:<a href="https://doi.org/10.1109/IJCNN.2008.4633916">10.1109/IJCNN.2008.4633916</a>.
</div>
<div id="ref-Fiorillo2003" class="csl-entry" role="listitem">
Fiorillo, C. D., Tobler, P. N., and Schultz, W. (2003). <span class="nocase">Discrete coding of reward probability and uncertainty by dopamine neurons.</span> <em>Science</em> 299, 1898–902. doi:<a href="https://doi.org/10.1126/science.1077349">10.1126/science.1077349</a>.
</div>
<div id="ref-frank2001" class="csl-entry" role="listitem">
Frank, M. J., Loughry, B., and O’Reilly, R. C. (2001). Interactions between frontal cortex and basal ganglia in working memory: A computational model. <em>Cogn Affect Behav Neurosci</em> 1, 137–160.
</div>
<div id="ref-Friston2010" class="csl-entry" role="listitem">
Friston, K. (2010). The free-energy principle: A unified brain theory? <em>Nat Rev Neurosci</em> 11, 127–138. doi:<a href="https://doi.org/10.1038/nrn2787">10.1038/nrn2787</a>.
</div>
<div id="ref-funahashi1989" class="csl-entry" role="listitem">
Funahashi, S., Bruce, C. J., and Goldman-Rakic, P. S. (1989). Mnemonic coding of visual space in the monkey’s dorsolateral prefrontal cortex. <em>J Neurophysiol</em> 61, 331–349.
</div>
<div id="ref-Galvan2011" class="csl-entry" role="listitem">
Galvan, A., and Smith, Y. (2011). The primate thalamostriatal systems: Anatomical organization, functional roles and possible involvement in parkinson’s disease. <em>Basal Ganglia</em> 1, 179–189. doi:<a href="https://doi.org/10.1016/j.baga.2011.09.001">10.1016/j.baga.2011.09.001</a>.
</div>
<div id="ref-Gerfen1990" class="csl-entry" role="listitem">
Gerfen, C. R., Engber, T. M., Mahan, L. C., Susel, Z., Chase, T. N., Monsma, F., Jr, et al. (1990). <a href="https://www.ncbi.nlm.nih.gov/pubmed/2147780">D1 and D2 dopamine receptor-regulated gene expression of striatonigral and striatopallidal neurons.</a> <em>Science</em> 250, 1429–1432.
</div>
<div id="ref-Grossberg1989" class="csl-entry" role="listitem">
Grossberg, S., and Schmajuk, N. A. (1989). <span class="nocase">Neural dynamics of adaptive timing and temporal discrimination during associative learning</span>. <em>Neural Networks</em> 2, 79–102. Available at: <a href="http://www.sciencedirect.com/science/article/pii/0893608089900269">http://www.sciencedirect.com/science/article/pii/0893608089900269</a>.
</div>
<div id="ref-Gruber2012" class="csl-entry" role="listitem">
Gruber, A. J., and McDonald, R. J. (2012). Context, emotion, and the strategic pursuit of goals: Interactions among multiple brain systems controlling motivated behavior. <em>Front Behav Neurosci</em> 6, 50. doi:<a href="https://doi.org/10.3389/fnbeh.2012.00050">10.3389/fnbeh.2012.00050</a>.
</div>
<div id="ref-Gruber2003" class="csl-entry" role="listitem">
Gruber, A. J., Solla, S. A., Surmeier, D. J., and Houk, J. C. (2003). <span class="nocase">Modulation of striatal single units by expected reward: a spiny neuron model displaying dopamine-induced bistability.</span> <em>J. Neurophysiol.</em> 90, 1095–114. doi:<a href="https://doi.org/10.1152/jn.00618.2002">10.1152/jn.00618.2002</a>.
</div>
<div id="ref-Gurney2001" class="csl-entry" role="listitem">
Gurney, K., Prescott, T. J., and Redgrave, P. (2001). <a href="https://www.ncbi.nlm.nih.gov/pubmed/11417052">A computational model of action selection in the basal ganglia.<span> I. A </span>new functional anatomy.</a> <em>Biol Cybern</em> 84, 401–410.
</div>
<div id="ref-Haber2000" class="csl-entry" role="listitem">
Haber, S. N., Fudge, J. L., and McFarland, N. R. (2000). <span class="nocase">Striatonigrostriatal pathways in primates form an ascending spiral from the shell to the dorsolateral striatum.</span> <em>J. Neurosci.</em> 20, 2369–82. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/10704511">http://www.ncbi.nlm.nih.gov/pubmed/10704511</a>.
</div>
<div id="ref-Haber2010" class="csl-entry" role="listitem">
Haber, S. N., and Knutson, B. (2010). <span class="nocase">The reward circuit: linking primate anatomy and human imaging.</span> <em>Neuropsychopharmacology</em> 35, 4–26. doi:<a href="https://doi.org/10.1038/npp.2009.129">10.1038/npp.2009.129</a>.
</div>
<div id="ref-Hamker2004a" class="csl-entry" role="listitem">
Hamker, F. H. (2004). <a href="https://www.ncbi.nlm.nih.gov/pubmed/14680776">A dynamic model of how feature cues guide spatial attention.</a> <em>Vision Res</em> 44, 501–521.
</div>
<div id="ref-Hamker2005" class="csl-entry" role="listitem">
Hamker, F. H. (2005). The reentry hypothesis: The putative interaction of the frontal eye field, ventrolateral prefrontal cortex, and areas <span>V4</span>, <span>IT</span> for attention and eye movement. <em>Cereb Cortex</em> 15, 431–447.
</div>
<div id="ref-Hebb1949" class="csl-entry" role="listitem">
Hebb, D. O. (1949). <em><span class="nocase">The organization of behavior: A neuropsychological theory</span></em>. New York: Wiley.
</div>
<div id="ref-Helie2013" class="csl-entry" role="listitem">
Helie, S., Chakravarthy, S., and Moustafa, A. A. (2013). Exploring the cognitive and motor functions of the basal ganglia: An integrative review of computational cognitive neuroscience models. <em>Front Comput Neurosci</em> 7, 174. doi:<a href="https://doi.org/10.3389/fncom.2013.00174">10.3389/fncom.2013.00174</a>.
</div>
<div id="ref-Hodgkin1952" class="csl-entry" role="listitem">
Hodgkin, A. L., and Huxley, A. F. (1952). A quantitative description of membrane current and its application to conduction and excitation in nerve. <em>The Journal of physiology</em> 117, 500–544. doi:<a href="https://doi.org/10.1113/jphysiol.1952.sp004764">10.1113/jphysiol.1952.sp004764</a>.
</div>
<div id="ref-Hollerman1998" class="csl-entry" role="listitem">
Hollerman, J. R., and Schultz, W. (1998). <span class="nocase">Dopamine neurons report an error in the temporal prediction of reward during learning.</span> <em>Nat. Neurosci.</em> 1, 304–9. doi:<a href="https://doi.org/10.1038/1124">10.1038/1124</a>.
</div>
<div id="ref-Horvitz2000" class="csl-entry" role="listitem">
Horvitz, J. C. (2000). <span class="nocase">Mesolimbocortical and nigrostriatal dopamine responses to salient non-reward events.</span> <em>Neuroscience</em> 96, 651–6. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/10727783">http://www.ncbi.nlm.nih.gov/pubmed/10727783</a>.
</div>
<div id="ref-Houk1995" class="csl-entry" role="listitem">
Houk, J. C., Adams, J. L., and Barto, A. G. (1995). <span>“A model of how the basal ganglia generate and use neural signal that predict reinforcement,”</span> in <em>Models of information processing in the basal ganglia</em>, eds. J. C. Houk, J. L. Davis, and D. G. Beiser (Cambridge, MA: The MIT Press).
</div>
<div id="ref-Humphries2010" class="csl-entry" role="listitem">
Humphries, M. D., and Prescott, T. J. (2010). <span class="nocase">The ventral basal ganglia, a selection mechanism at the crossroads of space, strategy, and reward.</span> <em>Prog. Neurobiol.</em> 90, 385–417. doi:<a href="https://doi.org/10.1016/j.pneurobio.2009.11.003">10.1016/j.pneurobio.2009.11.003</a>.
</div>
<div id="ref-Ikemoto2010" class="csl-entry" role="listitem">
Ikemoto, S. (2010). <span class="nocase">Brain reward circuitry beyond the mesolimbic dopamine system: a neurobiological theory.</span> <em>Neurosci Biobehav Rev</em> 35, 129–150. doi:<a href="https://doi.org/10.1016/j.neubiorev.2010.02.001">10.1016/j.neubiorev.2010.02.001</a>.
</div>
<div id="ref-Jhou2009" class="csl-entry" role="listitem">
Jhou, T. C., Fields, H. L., Baxter, M. G., Saper, C. B., and Holland, P. C. (2009). <span class="nocase">The rostromedial tegmental nucleus (RMTg), a GABAergic afferent to midbrain dopamine neurons, encodes aversive stimuli and inhibits motor responses.</span> <em>Neuron</em> 61, 786–800. doi:<a href="https://doi.org/10.1016/j.neuron.2009.02.001">10.1016/j.neuron.2009.02.001</a>.
</div>
<div id="ref-Joel2002" class="csl-entry" role="listitem">
Joel, D., Niv, Y., and Ruppin, E. (2002). Actor-critic models of the basal ganglia: New anatomical and computational perspectives. <em>Neur Netw</em> 15, 535–547.
</div>
<div id="ref-Jonides1998" class="csl-entry" role="listitem">
Jonides, J., Schumacher, E. H., Smith, E. E., Koeppe, R. A., Awh, E., Reuter-Lorenz, P. A., et al. (1998). <a href="https://www.ncbi.nlm.nih.gov/pubmed/9634568">The role of parietal cortex in verbal working memory.</a> <em>J Neurosci</em> 18, 5026–5034.
</div>
<div id="ref-Joshua2009" class="csl-entry" role="listitem">
Joshua, M., Adler, A., and Bergman, H. (2009). The dynamics of dopamine in control of motor behavior. <em>Current Opinion in Neurobiology</em> 19, 615–620. doi:<a href="https://doi.org/10.1016/j.conb.2009.10.001">10.1016/j.conb.2009.10.001</a>.
</div>
<div id="ref-Kaplan2007" class="csl-entry" role="listitem">
Kaplan, F., and Oudeyer, P.-Y. (2007). In search of the neural circuits of intrinsic motivation. <em>Front Neurosci</em> 1, 225–236. doi:<a href="https://doi.org/10.3389/neuro.01.1.1.017.2007">10.3389/neuro.01.1.1.017.2007</a>.
</div>
<div id="ref-Keramati2013" class="csl-entry" role="listitem">
Keramati, M., and Gutkin, B. (2013). Imbalanced decision hierarchy in addicts emerging from drug-hijacked dopamine spiraling circuit. <em>PLoS ONE</em> 8. doi:<a href="https://doi.org/10.1371/journal.pone.0061489">10.1371/journal.pone.0061489</a>.
</div>
<div id="ref-Khamassi2012" class="csl-entry" role="listitem">
Khamassi, M., and Humphries, M. D. (2012). Integrating cortico-limbic-basal ganglia architectures for learning model-based and model-free navigation strategies. <em>Front Behav Neurosci</em> 6, 79. doi:<a href="https://doi.org/10.3389/fnbeh.2012.00079">10.3389/fnbeh.2012.00079</a>.
</div>
<div id="ref-kirkpatrick2000" class="csl-entry" role="listitem">
Kirkpatrick, K., and Church, R. M. (2000). Stimulus and temporal cues in classical conditioning. <em>J Exp Psychol Anim Behav Process</em> 26, 206–219.
</div>
<div id="ref-Kita1999" class="csl-entry" role="listitem">
Kita, H., Tokuno, H., and Nambu, A. (1999). <a href="https://www.ncbi.nlm.nih.gov/pubmed/10380964">Monkey globus pallidus external segment neurons projecting to the neostriatum.</a> <em>Neuroreport</em> 10, 1467–1472.
</div>
<div id="ref-Koch1989" class="csl-entry" role="listitem">
Koch, K. W., and Fuster, J. M. (1989). <a href="https://www.ncbi.nlm.nih.gov/pubmed/2767186">Unit activity in monkey parietal cortex related to haptic perception and temporary memory.</a> <em>Exp Brain Res</em> 76, 292–306.
</div>
<div id="ref-Krueger2009" class="csl-entry" role="listitem">
Krueger, K. A., and Dayan, P. (2009). Flexible shaping: How learning in small steps helps. <em>Cognition</em> 110, 380–394. doi:<a href="https://doi.org/10.1016/j.cognition.2008.11.014">10.1016/j.cognition.2008.11.014</a>.
</div>
<div id="ref-Kuhn1962" class="csl-entry" role="listitem">
Kuhn, T. S. (1962). <em>The structure of scientific revolutions</em>. 1st ed. University of Chicago Press.
</div>
<div id="ref-Kumar2011" class="csl-entry" role="listitem">
Kumar, A., Cardanobile, S., Rotter, S., and Aertsen, A. (2011). The role of inhibition in generating and controlling parkinson’s disease oscillations in the basal ganglia. <em>Front Syst Neurosci</em> 5, 86. doi:<a href="https://doi.org/10.3389/fnsys.2011.00086">10.3389/fnsys.2011.00086</a>.
</div>
<div id="ref-Kurzweil2005" class="csl-entry" role="listitem">
Kurzweil, R. (2005). <em>The singularity is near</em>. New York: Viking Books.
</div>
<div id="ref-Landau2009" class="csl-entry" role="listitem">
Landau, S. M., Lal, R., O’Neil, J. P., Baker, S., and Jagust, W. J. (2009). Striatal dopamine and working memory. <em>Cereb Cortex</em> 19, 445–454. doi:<a href="https://doi.org/10.1093/cercor/bhn095">10.1093/cercor/bhn095</a>.
</div>
<div id="ref-Langley2009" class="csl-entry" role="listitem">
Langley, P., Laird, J. E., and Rogers, S. (2009). Cognitive architectures: Research issues and challenges. <em>Cognitive Systems Research</em> 10, 141–160.
</div>
<div id="ref-Lapicque1907" class="csl-entry" role="listitem">
Lapicque, L. (1907). Recherches quantitatives sur l’excitation électrique des nerfs traitée comme une polarisation. <em>J. Physiol. Pathol. Gen.</em> 9, 620–635.
</div>
<div id="ref-LeCun2015" class="csl-entry" role="listitem">
LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. <em>Nature</em> 521, 436–444. doi:<a href="https://doi.org/10.1038/nature14539">10.1038/nature14539</a>.
</div>
<div id="ref-Lecun1998" class="csl-entry" role="listitem">
Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). <span class="nocase">Gradient-based learning applied to document recognition</span>. <em>Proc. IEEE</em> 86, 2278–2324. doi:<a href="https://doi.org/10.1109/5.726791">10.1109/5.726791</a>.
</div>
<div id="ref-Leung1993" class="csl-entry" role="listitem">
Leung, L. S., and Yim, C. Y. (1993). <span class="nocase">Rhythmic delta-frequency activities in the nucleus accumbens of anesthetized and freely moving rats.</span> <em>Can. J. Physiol. Pharmacol.</em> 71, 311–20. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/8104675">http://www.ncbi.nlm.nih.gov/pubmed/8104675</a>.
</div>
<div id="ref-Levy1997" class="csl-entry" role="listitem">
Levy, R., Friedman, H. R., Davachi, L., and Goldman-Rakic, P. S. (1997). <a href="https://www.ncbi.nlm.nih.gov/pubmed/9133405">Differential activation of the caudate nucleus in primates performing spatial and nonspatial working memory tasks.</a> <em>J Neurosci</em> 17, 3870–3882.
</div>
<div id="ref-Levy2002" class="csl-entry" role="listitem">
Levy, R., Hutchison, W. D., Lozano, A. M., and Dostrovsky, J. O. (2002). Synchronized neuronal discharge in the basal ganglia of parkinsonian patients is limited to oscillatory activity. <em>J Neurosci</em> 22, 2855–2861. doi:<a href="https://doi.org/20026193">20026193</a>.
</div>
<div id="ref-Ljungberg1992" class="csl-entry" role="listitem">
Ljungberg, T., Apicella, P., and Schultz, W. (1992). <span class="nocase">Responses of monkey dopamine neurons during learning of behavioral reactions.</span> <em>J. Neurophysiol.</em> 67, 145–63. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/1552316">http://www.ncbi.nlm.nih.gov/pubmed/1552316</a>.
</div>
<div id="ref-Maass1999" class="csl-entry" role="listitem">
Maass, W., and Zador, A. M. (1999). <a href="https://www.ncbi.nlm.nih.gov/pubmed/10226188">Dynamic stochastic synapses as computational units.</a> <em>Neural Comput</em> 11, 903–917.
</div>
<div id="ref-Mancall2011" class="csl-entry" role="listitem">
Mancall, E. L., and Brock, D. G. (2011). <em>Gray’s clinical neuroanatomy: The anatomic basis for clinical neuroscience</em>. Elsevier Health Sciences.
</div>
<div id="ref-Markram2006" class="csl-entry" role="listitem">
Markram, H. (2006). The blue brain project. <em>Nat Rev Neurosci</em> 7, 153–60.
</div>
<div id="ref-Matell2004" class="csl-entry" role="listitem">
Matell, M. S., and Meck, W. H. (2004). <span class="nocase">Cortico-striatal circuits and interval timing: coincidence detection of oscillatory processes</span>. <em>Cogn. Brain Res.</em> 21, 139–170. Available at: <a href="http://www.sciencedirect.com/science/article/pii/S0926641004001697">http://www.sciencedirect.com/science/article/pii/S0926641004001697</a>.
</div>
<div id="ref-Matsumoto2009" class="csl-entry" role="listitem">
Matsumoto, M., and Hikosaka, O. (2009). <span class="nocase">Two types of dopamine neuron distinctly convey positive and negative motivational signals.</span> <em>Nature</em> 459, 837–41. doi:<a href="https://doi.org/10.1038/nature08028">10.1038/nature08028</a>.
</div>
<div id="ref-McGinty2009" class="csl-entry" role="listitem">
McGinty, V. B., and Grace, A. A. (2009). <span class="nocase">Activity-dependent depression of medial prefrontal cortex inputs to accumbens neurons by the basolateral amygdala.</span> <em>Neuroscience</em> 162, 1429–36. doi:<a href="https://doi.org/10.1016/j.neuroscience.2009.05.028">10.1016/j.neuroscience.2009.05.028</a>.
</div>
<div id="ref-Minsky1968" class="csl-entry" role="listitem">
Minsky, M. (1968). <em>Semantic information processing</em>. Cambridge, MA: MIT Press.
</div>
<div id="ref-Mirolli2013" class="csl-entry" role="listitem">
Mirolli, M., Santucci, V. G., and Baldassarre, G. (2013). Phasic dopamine as a prediction error of intrinsic and extrinsic reinforcements driving both action acquisition and reward maximization: A simulated robotic study. <em>Neural Netw</em> 39, 40–51. doi:<a href="https://doi.org/10.1016/j.neunet.2012.12.012">10.1016/j.neunet.2012.12.012</a>.
</div>
<div id="ref-murray2001" class="csl-entry" role="listitem">
Murray, E. A., and Richmond, B. J. (2001). Role of perirhinal cortex in object perception, memory, and associations. <em>Curr Opin Neurobiol</em> 11, 188–193.
</div>
<div id="ref-Nguyen2014" class="csl-entry" role="listitem">
N’guyen, S., Thurat, C., and Girard, B. (2014). Saccade learning with concurrent cortical and subcortical basal ganglia loops. <em>Front Comput Neurosci</em> 8, 48. doi:<a href="https://doi.org/10.3389/fncom.2014.00048">10.3389/fncom.2014.00048</a>.
</div>
<div id="ref-nakahara2001" class="csl-entry" role="listitem">
Nakahara, H., Doya, K., and Hikosaka, O. (2001). Parallel cortico-basal ganglia mechanisms for acquisition and execution of visuomotor sequences - a computational approach. <em>J Cogn Neurosci</em> 13, 626–647.
</div>
<div id="ref-Nakamura1995" class="csl-entry" role="listitem">
Nakamura, K., and Kubota, K. (1995). Mnemonic firing of neurons in the monkey temporal pole during a visual recognition memory task. <em>J Neurophysiol</em> 74, 162–178.
</div>
<div id="ref-Nambu2011" class="csl-entry" role="listitem">
Nambu, A. (2011). Somatotopic organization of the primate basal ganglia. <em>Front Neuroanat</em> 5, 26. doi:<a href="https://doi.org/10.3389/fnana.2011.00026">10.3389/fnana.2011.00026</a>.
</div>
<div id="ref-Nambu2002" class="csl-entry" role="listitem">
Nambu, A., Kaneda, K., Tokuno, H., and Takada, M. (2002). <a href="https://www.ncbi.nlm.nih.gov/pubmed/12364509">Organization of corticostriatal motor inputs in monkey putamen.</a> <em>J Neurophysiol</em> 88, 1830–1842.
</div>
<div id="ref-naya2003" class="csl-entry" role="listitem">
Naya, Y., Yoshida, M., Takeda, M., Fujimichi, R., and Miyashita, Y. (2003). Delay-period activities in two subdivisions of monkey inferotemporal cortex during pair association memory task. <em>Eur J Neurosci</em> 18, 2915–2918.
</div>
<div id="ref-Nicola2000" class="csl-entry" role="listitem">
Nicola, S. M., Surmeier, J., and Malenka, R. C. (2000). Dopaminergic modulation of neuronal excitability in the striatum and nucleus accumbens. <em>Annu Rev Neurosci</em> 23, 185–215.
</div>
<div id="ref-Niv2007" class="csl-entry" role="listitem">
Niv, Y., Daw, N. D., Joel, D., and Dayan, P. (2007). <span class="nocase">Tonic dopamine: opportunity costs and the control of response vigor.</span> <em>Psychopharmacology (Berl).</em> 191, 507–20. doi:<a href="https://doi.org/10.1007/s00213-006-0502-4">10.1007/s00213-006-0502-4</a>.
</div>
<div id="ref-OReilly2006" class="csl-entry" role="listitem">
O’Reilly, R. C., and Frank, M. J. (2006). <span class="nocase">Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia.</span> <em>Neural Comput.</em> 18, 283–328. doi:<a href="https://doi.org/10.1162/089976606775093909">10.1162/089976606775093909</a>.
</div>
<div id="ref-OReilly2007" class="csl-entry" role="listitem">
O’Reilly, R. C., Frank, M. J., Hazy, T. E., and Watz, B. (2007). <span>PVLV:</span> The primary value and learned value pavlovian learning algorithm. <em>Behav Neurosci</em> 121, 31–49.
</div>
<div id="ref-Oja1982" class="csl-entry" role="listitem">
Oja, E. (1982). <span class="nocase">A simplified neuron model as a principal component analyzer.</span> <em>J. Math. Biol.</em> 15, 267–73. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/7153672">http://www.ncbi.nlm.nih.gov/pubmed/7153672</a>.
</div>
<div id="ref-Olshausen1997" class="csl-entry" role="listitem">
Olshausen, B. A., and Field, D. J. (1997). <a href="https://www.ncbi.nlm.nih.gov/pubmed/9425546">Sparse coding with an overcomplete basis set: A strategy employed by V1?</a> <em>Vision Res</em> 37, 3311–3325.
</div>
<div id="ref-Oudeyer2007" class="csl-entry" role="listitem">
Oudeyer, P.-Y., and Kaplan, F. (2007). What is intrinsic motivation? A typology of computational approaches. <em>Front Neurorobot</em> 1, 6. doi:<a href="https://doi.org/10.3389/neuro.12.006.2007">10.3389/neuro.12.006.2007</a>.
</div>
<div id="ref-Pan2005a" class="csl-entry" role="listitem">
Pan, W.-X., and Hyland, B. I. (2005). <span class="nocase">Pedunculopontine tegmental nucleus controls conditioned responses of midbrain dopamine neurons in behaving rats.</span> <em>J. Neurosci.</em> 25, 4725–32. doi:<a href="https://doi.org/10.1523/JNEUROSCI.0277-05.2005">10.1523/JNEUROSCI.0277-05.2005</a>.
</div>
<div id="ref-Pennartz1995" class="csl-entry" role="listitem">
Pennartz, C. M. (1995). <a href="https://www.ncbi.nlm.nih.gov/pubmed/8806015">The ascending neuromodulatory systems in learning by reinforcement: Comparing computational conjectures with experimental findings.</a> <em>Brain Res Brain Res Rev</em> 21, 219–245.
</div>
<div id="ref-Plenz1999" class="csl-entry" role="listitem">
Plenz, D., and Kital, S. T. (1999). A basal ganglia pacemaker formed by the subthalamic nucleus and external globus pallidus. <em>Nature</em> 400, 677–682. doi:<a href="https://doi.org/10.1038/23281">10.1038/23281</a>.
</div>
<div id="ref-Price2012" class="csl-entry" role="listitem">
Price, T. F., Peterson, C. K., and Harmon-Jones, E. (2012). The emotive neuroscience of embodiment. <em>Motivation and Emotion</em> 36, 27–37. doi:<a href="https://doi.org/10.1007/s11031-011-9258-1">10.1007/s11031-011-9258-1</a>.
</div>
<div id="ref-ranganath2004" class="csl-entry" role="listitem">
Ranganath, C., Cohen, M. X., Dam, C., and D’Esposito, M. (2004). Inferior temporal, prefrontal, and hippocampal contributions to visual working memory maintenance and associative memory retrieval. <em>J Neurosci</em> 24, 3917–3925.
</div>
<div id="ref-Rao2010" class="csl-entry" role="listitem">
Rao, R. P. N. (2010). <span class="nocase">Decision making under uncertainty: a neural model based on partially observable markov decision processes.</span> <em>Front. Comput. Neurosci.</em> 4, 146. doi:<a href="https://doi.org/10.3389/fncom.2010.00146">10.3389/fncom.2010.00146</a>.
</div>
<div id="ref-Rast2011" class="csl-entry" role="listitem">
Rast, A., Galluppi, F., Davies, S., Plana, L., Patterson, C., Sharp, T., et al. (2011). Concurrent heterogeneous neural model simulation on real-time neuromimetic hardware. <em>Neural Networks</em> 24, 961–978. doi:<a href="http://dx.doi.org/10.1016/j.neunet.2011.06.014">http://dx.doi.org/10.1016/j.neunet.2011.06.014</a>.
</div>
<div id="ref-redgrave2006" class="csl-entry" role="listitem">
Redgrave, P., and Gurney, K. (2006). The short-latency dopamine signal: A role in discovering novel actions? <em>Nat Rev Neurosci</em> 7, 967–975.
</div>
<div id="ref-redgrave1999" class="csl-entry" role="listitem">
Redgrave, P., Prescott, T. J., and Gurney, K. (1999). Is the short-latency dopamine response too short to signal reward error? <em>Trends Neurosci</em> 22, 146–151.
</div>
<div id="ref-Reynolds2000" class="csl-entry" role="listitem">
Reynolds, J. N., and Wickens, J. R. (2000). <a href="https://www.ncbi.nlm.nih.gov/pubmed/10938425">Substantia nigra dopamine regulates synaptic plasticity and membrane potential fluctuations in the rat neostriatum, in vivo.</a> <em>Neuroscience</em> 99, 199–203.
</div>
<div id="ref-Rivest2010" class="csl-entry" role="listitem">
Rivest, F., Kalaska, J. F., and Bengio, Y. (2010). <span class="nocase">Alternative time representation in dopamine models.</span> <em>J. Comput. Neurosci.</em> 28, 107–30. doi:<a href="https://doi.org/10.1007/s10827-009-0191-1">10.1007/s10827-009-0191-1</a>.
</div>
<div id="ref-Rodriguez-Oroz2009" class="csl-entry" role="listitem">
Rodriguez-Oroz, M. C., Jahanshahi, M., Krack, P., Litvan, I., Macias, R., Bezard, E., et al. (2009). Initial clinical manifestations of parkinson’s disease: Features and pathophysiological mechanisms. <em>Lancet Neurol</em> 8, 1128–1139. doi:<a href="https://doi.org/10.1016/S1474-4422(09)70293-5">10.1016/S1474-4422(09)70293-5</a>.
</div>
<div id="ref-Rolls2001" class="csl-entry" role="listitem">
Rolls, E., and Deco, G. (2001). <em>Computational neuroscience of vision</em>. Oxford Univ. Press.
</div>
<div id="ref-Rueda-Orozco2009" class="csl-entry" role="listitem">
Rueda-Orozco, P. E., Mendoza, E., Hernandez, R., Aceves, J. J., Ibanez-Sandoval, O., Galarraga, E., et al. (2009). Diversity in long-term synaptic plasticity at inhibitory synapses of striatal spiny neurons. <em>Learn Mem</em> 16, 474–478. doi:<a href="https://doi.org/10.1101/lm.1439909">10.1101/lm.1439909</a>.
</div>
<div id="ref-rumelhart1986" class="csl-entry" role="listitem">
Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). <span>“Learning internal representations by error propagation,”</span> in <em>Parallel distributed processing: Explorations in the microstructure of cognition</em>, eds. D. E. Rumelhart and J. L. McClelland (Cambridge, MA: MIT Press), 318–362.
</div>
<div id="ref-Samejima2007" class="csl-entry" role="listitem">
Samejima, K., and Doya, K. (2007). <span class="nocase">Multiple representations of belief states and action values in corticobasal ganglia loops.</span> <em>Ann. N. Y. Acad. Sci.</em> 1104, 213–28. doi:<a href="https://doi.org/10.1196/annals.1390.024">10.1196/annals.1390.024</a>.
</div>
<div id="ref-Schroll2015a" class="csl-entry" role="listitem">
Schroll, H., Beste, C., and Hamker, F. H. (2015). Combined lesions of direct and indirect basal ganglia pathways but not changes in dopamine levels explain learning deficits in patients with huntington’s disease. <em>Eur J Neurosci</em> 41, 1227–1244. doi:<a href="https://doi.org/10.1111/ejn.12868">10.1111/ejn.12868</a>.
</div>
<div id="ref-Schroll2012" class="csl-entry" role="listitem">
Schroll, H., Vitay, J., and Hamker, F. H. (2012). <span class="nocase">Working memory and response selection: a computational account of interactions among cortico-basalganglio-thalamic loops.</span> <em>Neural Netw.</em> 26, 59–74. doi:<a href="https://doi.org/10.1016/j.neunet.2011.10.008">10.1016/j.neunet.2011.10.008</a>.
</div>
<div id="ref-Schroll2014" class="csl-entry" role="listitem">
Schroll, H., Vitay, J., and Hamker, F. H. (2014). <span class="nocase">Dysfunctional and compensatory synaptic plasticity in Parkinson’s disease.</span> <em>Eur. J. Neurosci.</em> 39, 688–702. doi:<a href="https://doi.org/10.1111/ejn.12434">10.1111/ejn.12434</a>.
</div>
<div id="ref-schultz1998" class="csl-entry" role="listitem">
Schultz, W. (1998). Predictive reward signal of dopamine neurons. <em>J Neurophysiol</em> 80, 1–27.
</div>
<div id="ref-Schultz1997" class="csl-entry" role="listitem">
Schultz, W., Dayan, P., and Montague, P. R. (1997). <span class="nocase">A neural substrate of prediction and reward.</span> <em>Science</em> 275, 1593–9. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/9054347">http://www.ncbi.nlm.nih.gov/pubmed/9054347</a>.
</div>
<div id="ref-Seamans2004" class="csl-entry" role="listitem">
Seamans, J. K., and Yang, C. R. (2004). <span class="nocase">The principal features and mechanisms of dopamine modulation in the prefrontal cortex.</span> <em>Prog. Neurobiol.</em> 74, 1–58. doi:<a href="https://doi.org/10.1016/j.pneurobio.2004.05.006">10.1016/j.pneurobio.2004.05.006</a>.
</div>
<div id="ref-Searle1980" class="csl-entry" role="listitem">
Searle, J. R. (1980). Minds, brains, and programs. <em>Behavioral and Brain Sciences</em> 3, 417–424.
</div>
<div id="ref-Seger2008" class="csl-entry" role="listitem">
Seger, C. A. (2008). How do the basal ganglia contribute to categorization? <span>Their </span>roles in generalization, response selection, and learning via feedback. <em>Neurosci Biobehav Rev</em> 32, 265–278. doi:<a href="https://doi.org/10.1016/j.neubiorev.2007.07.010">10.1016/j.neubiorev.2007.07.010</a>.
</div>
<div id="ref-Seger2011" class="csl-entry" role="listitem">
Seger, C. A., and Spiering, B. J. (2011). A critical review of habit learning and the basal ganglia. <em>Front Syst Neurosci</em> 5, 66. doi:<a href="https://doi.org/10.3389/fnsys.2011.00066">10.3389/fnsys.2011.00066</a>.
</div>
<div id="ref-Sesack2010" class="csl-entry" role="listitem">
Sesack, S. R., and Grace, A. A. (2010). <span class="nocase">Cortico-Basal Ganglia reward network: microcircuitry.</span> <em>Neuropsychopharmacology</em> 35, 27–47. doi:<a href="https://doi.org/10.1038/npp.2009.93">10.1038/npp.2009.93</a>.
</div>
<div id="ref-Shen2008" class="csl-entry" role="listitem">
Shen, W., Flajolet, M., Greengard, P., and Surmeier, D. J. (2008). <span class="nocase">Dichotomous dopaminergic control of striatal synaptic plasticity.</span> <em>Science</em> 321, 848–51. doi:<a href="https://doi.org/10.1126/science.1160575">10.1126/science.1160575</a>.
</div>
<div id="ref-Skinner1938" class="csl-entry" role="listitem">
Skinner, B. F. (1938). <em>The behavior of organisms</em>. New York: Appleton-Century-Crofts.
</div>
<div id="ref-Smith2006" class="csl-entry" role="listitem">
Smith, A., Li, M., Becker, S., and Kapur, S. (2006). Dopamine, prediction error and associative learning: A model-based account. <em>Network</em> 17, 61–84. doi:<a href="https://doi.org/10.1080/09548980500361624">10.1080/09548980500361624</a>.
</div>
<div id="ref-spratling1999" class="csl-entry" role="listitem">
Spratling, M. W. (1999). Pre-synaptic lateral inhibition provides a better architecture for self-organizing neural networks. <em>Network</em> 10, 285–301.
</div>
<div id="ref-Srivastava2014" class="csl-entry" role="listitem">
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from overfitting. <em>J. Mach. Learn. Res.</em> 15, 1929–1958.
</div>
<div id="ref-Stimberg2014" class="csl-entry" role="listitem">
Stimberg, M., Goodman, D. F. M., Benichoux, V., and Brette, R. (2014). <span class="nocase">Equation-oriented specification of neural models for simulations.</span> <em>Front. Neuroinform.</em> 8, 6. doi:<a href="https://doi.org/10.3389/fninf.2014.00006">10.3389/fninf.2014.00006</a>.
</div>
<div id="ref-Stocco2010" class="csl-entry" role="listitem">
Stocco, A., Lebiere, C., and Anderson, J. R. (2010). Conditional routing of information to the cortex: A model of the basal ganglia’s role in cognitive coordination. <em>Psychol Rev</em> 117, 541–574. doi:<a href="https://doi.org/10.1037/a0019077">10.1037/a0019077</a>.
</div>
<div id="ref-Suri2001" class="csl-entry" role="listitem">
Suri, R. E., and Schultz, W. (2001). <span class="nocase">Temporal difference model reproduces anticipatory neural activity.</span> <em>Neural Comput.</em> 13, 841–62. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/11255572">http://www.ncbi.nlm.nih.gov/pubmed/11255572</a>.
</div>
<div id="ref-Surmeier2007" class="csl-entry" role="listitem">
Surmeier, D. J., Ding, J., Day, M., Wang, Z., and Shen, W. (2007). D1 and D2 dopamine-receptor modulation of striatal glutamatergic signaling in striatal medium spiny neurons. <em>Trends Neurosci</em> 30, 228–235. doi:<a href="https://doi.org/10.1016/j.tins.2007.03.008">10.1016/j.tins.2007.03.008</a>.
</div>
<div id="ref-Sutton1988" class="csl-entry" role="listitem">
Sutton, R. (1988). Learning to predict by the methods of temporal differences. <em>Machine Learning</em> 3, 9–44. doi:<a href="https://doi.org/10.1007/BF00115009">10.1007/BF00115009</a>.
</div>
<div id="ref-Sutton1998" class="csl-entry" role="listitem">
Sutton, R. S., and Barto, A. G. (1998). <em><span class="nocase">Reinforcement learning: An introduction</span></em>. MIT press.
</div>
<div id="ref-Tan2008" class="csl-entry" role="listitem">
Tan, C. O., and Bullock, D. (2008). <span class="nocase">A local circuit model of learned striatal and dopamine cell responses under probabilistic schedules of reward.</span> <em>J. Neurosci.</em> 28, 10062–74. doi:<a href="https://doi.org/10.1523/JNEUROSCI.0259-08.2008">10.1523/JNEUROSCI.0259-08.2008</a>.
</div>
<div id="ref-taylor2006" class="csl-entry" role="listitem">
Taylor, K. I., Moss, H. E., Stamatakis, E. A., and Tyler, L. K. (2006). Binding crossmodal object features in perirhinal cortex. <em>Proc Natl Acad Sci U S A</em> 103, 8239–8244.
</div>
<div id="ref-Thorndike1911" class="csl-entry" role="listitem">
Thorndike, E. L. (1911). <em>Animal intelligence: Experimental studies</em>. Macmillan.
</div>
<div id="ref-ungerleider1982" class="csl-entry" role="listitem">
Ungerleider, L. G., and Mishkin, M. (1982). <span>“Two cortical visual systems,”</span> in <em>Analysis of visual behavior</em>, eds. D. J. Ingle, M. A. Goodale, and R. J. W. Mansfield (Cambridge, MA: The MIT Press), 549–586.
</div>
<div id="ref-Uttal2015" class="csl-entry" role="listitem">
Uttal, W. R. (2015). <em>Macroneural theories in cognitive neuroscience</em>. Psychology Press.
</div>
<div id="ref-Velik2012" class="csl-entry" role="listitem">
Velik, R. (2012). AI reloaded: Objectives, potentials, and challenges of the novel field of brain-like artificial intelligence. <em>BRAIN. Broad Research in Artificial Intelligence and Neuroscience</em> 3.
</div>
<div id="ref-Vijayraghavan2007" class="csl-entry" role="listitem">
Vijayraghavan, S., Wang, M., Birnbaum, S. G., Williams, G. V., and Arnsten, A. F. T. (2007). Inverted-u dopamine D1 receptor actions on prefrontal neurons engaged in working memory. <em>Nat Neurosci</em> 10, 376–384. doi:<a href="https://doi.org/10.1038/nn1846">10.1038/nn1846</a>.
</div>
<div id="ref-Vitay2015" class="csl-entry" role="listitem">
Vitay, J., Dinkelbach, H. Ü., and Hamker, F. H. (2015). ANNarchy: A code generation approach to neural simulations on parallel hardware. <em>Front Neuroinform</em> 9, 19. doi:<a href="https://doi.org/10.3389/fninf.2015.00019">10.3389/fninf.2015.00019</a>.
</div>
<div id="ref-Vitay2008" class="csl-entry" role="listitem">
Vitay, J., and Hamker, F. H. (2008). <span class="nocase">Sustained activities and retrieval in a computational model of the perirhinal cortex.</span> <em>J. Cogn. Neurosci.</em> 20, 1993–2005. doi:<a href="https://doi.org/10.1162/jocn.2008.20147">10.1162/jocn.2008.20147</a>.
</div>
<div id="ref-Vitay2010" class="csl-entry" role="listitem">
Vitay, J., and Hamker, F. H. (2010). <span class="nocase">A computational model of Basal Ganglia and its role in memory retrieval in rewarded visual memory tasks.</span> <em>Front. Comput. Neurosci.</em> 4. doi:<a href="https://doi.org/10.3389/fncom.2010.00013">10.3389/fncom.2010.00013</a>.
</div>
<div id="ref-Vitay2014" class="csl-entry" role="listitem">
Vitay, J., and Hamker, F. H. (2014). Timing and expectation of reward: A neuro-computational model of the afferents to the ventral tegmental area. <em>Front Neurorobot</em> 8, 4. doi:<a href="https://doi.org/10.3389/fnbot.2014.00004">10.3389/fnbot.2014.00004</a>.
</div>
<div id="ref-Wiltschut2009" class="csl-entry" role="listitem">
Wiltschut, J., and Hamker, F. H. (2009). Efficient coding correlates with spatial frequency tuning in a model of V1 receptive field organization. <em>Vis Neurosci</em> 26, 21–34. doi:<a href="https://doi.org/10.1017/S0952523808080966">10.1017/S0952523808080966</a>.
</div>
<div id="ref-Yin2004" class="csl-entry" role="listitem">
Yin, H. H., Knowlton, B. J., and Balleine, B. W. (2004). <a href="https://www.ncbi.nlm.nih.gov/pubmed/14750976">Lesions of dorsolateral striatum preserve outcome expectancy but disrupt habit formation in instrumental learning.</a> <em>Eur J Neurosci</em> 19, 181–189.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link" aria-label="Abstract">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Abstract</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./2-JOCN.html" class="pagination-link" aria-label="Sustained activities and retrieval in a computational model of perirhinal cortex">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Sustained activities and retrieval in a computational model of perirhinal cortex</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>