<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>On the role of dopamine in motivated behavior : a neuro-computational approach - 5&nbsp; Timing and expectation of reward: a neuro-computational model of the afferents to the ventral tegmental area</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./6-FINI.html" rel="next">
<link href="./4-NN.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./5-FINR.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Timing and expectation of reward: a neuro-computational model of the afferents to the ventral tegmental area</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      <img src="./img/tuc.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">On the role of dopamine in motivated behavior : a neuro-computational approach</a> 
        <div class="sidebar-tools-main">
    <a href="./thesis.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Abstract</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1-Introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-JOCN.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Sustained activities and retrieval in a computational model of perirhinal cortex</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-FICN.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">A computational model of basal ganglia and its role in memory retrieval in rewarded visual memory tasks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4-NN.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Working memory and response selection: A computational account of interactions among cortico-basal ganglio-thalamic loops</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5-FINR.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Timing and expectation of reward: a neuro-computational model of the afferents to the ventral tegmental area</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6-FINI.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">ANNarchy: a code generation approach to neural simulations on parallel hardware</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./References.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">5.1</span> Introduction</a></li>
  <li><a href="#material-methods" id="toc-material-methods" class="nav-link" data-scroll-target="#material-methods"><span class="header-section-number">5.2</span> Material &amp; methods</a>
  <ul class="collapse">
  <li><a href="#neurobiological-assumptions" id="toc-neurobiological-assumptions" class="nav-link" data-scroll-target="#neurobiological-assumptions"><span class="header-section-number">5.2.1</span> Neurobiological assumptions</a></li>
  <li><a href="#the-proposed-model" id="toc-the-proposed-model" class="nav-link" data-scroll-target="#the-proposed-model"><span class="header-section-number">5.2.2</span> The proposed model</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="header-section-number">5.3</span> Results</a>
  <ul class="collapse">
  <li><a href="#sec-finr:results-amygdala" id="toc-sec-finr:results-amygdala" class="nav-link" data-scroll-target="#sec-finr\:results-amygdala"><span class="header-section-number">5.3.1</span> CS-US associations in the amygdala</a></li>
  <li><a href="#sec-finr:results-vta" id="toc-sec-finr:results-vta" class="nav-link" data-scroll-target="#sec-finr\:results-vta"><span class="header-section-number">5.3.2</span> Timecourse of activity in VTA</a></li>
  <li><a href="#evolution-of-vta-activity-during-conditioning" id="toc-evolution-of-vta-activity-during-conditioning" class="nav-link" data-scroll-target="#evolution-of-vta-activity-during-conditioning"><span class="header-section-number">5.3.3</span> Evolution of VTA activity during conditioning</a></li>
  <li><a href="#sec-finr:rewardmagnitude" id="toc-sec-finr:rewardmagnitude" class="nav-link" data-scroll-target="#sec-finr\:rewardmagnitude"><span class="header-section-number">5.3.4</span> Influence of reward magnitude on conditioning</a></li>
  <li><a href="#timing-mechanism-in-nacc" id="toc-timing-mechanism-in-nacc" class="nav-link" data-scroll-target="#timing-mechanism-in-nacc"><span class="header-section-number">5.3.5</span> Timing mechanism in NAcc</a></li>
  <li><a href="#acquisition-rate-of-temporal-prediction" id="toc-acquisition-rate-of-temporal-prediction" class="nav-link" data-scroll-target="#acquisition-rate-of-temporal-prediction"><span class="header-section-number">5.3.6</span> Acquisition rate of temporal prediction</a></li>
  <li><a href="#time-course-of-forebrain-nuclei" id="toc-time-course-of-forebrain-nuclei" class="nav-link" data-scroll-target="#time-course-of-forebrain-nuclei"><span class="header-section-number">5.3.7</span> Time course of forebrain nuclei</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion"><span class="header-section-number">5.4</span> Discussion</a>
  <ul class="collapse">
  <li><a href="#relation-to-other-work" id="toc-relation-to-other-work" class="nav-link" data-scroll-target="#relation-to-other-work"><span class="header-section-number">5.4.1</span> Relation to other work</a></li>
  <li><a href="#sec-finr:discussion-bio" id="toc-sec-finr:discussion-bio" class="nav-link" data-scroll-target="#sec-finr\:discussion-bio"><span class="header-section-number">5.4.2</span> Biological plausibility</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">5.4.3</span> Conclusion</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-chapter:VTA" class="quarto-section-identifier"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Timing and expectation of reward: a neuro-computational model of the afferents to the ventral tegmental area</span></span></h1>
</div>



<div class="quarto-title-meta column-body">

    
  
    
  </div>
  


</header>


<section id="abstract" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="abstract">Abstract</h4>
<p>Neural activity in dopaminergic areas such as the ventral tegmental area is influenced by timing processes, in particular by the temporal expectation of rewards during Pavlovian conditioning. Receipt of a reward at the expected time allows to compute reward-prediction errors which can drive learning in motor or cognitive structures. Reciprocally, dopamine plays an important role in the timing of external events. Several models of the dopaminergic system exist, but the substrate of temporal learning is rather unclear. In this article, we propose a neuro-computational model of the afferent network to the ventral tegmental area, including the lateral hypothalamus, the pedunculopontine nucleus, the amygdala, the ventromedial prefrontal cortex, the ventral basal ganglia (including the nucleus accumbens and the ventral pallidum), as well as the lateral habenula and the rostromedial tegmental nucleus. Based on a plausible connectivity and realistic learning rules, this neuro-computational model reproduces several experimental observations, such as the progressive cancellation of dopaminergic bursts at reward delivery, the appearance of bursts at the onset of reward-predicting cues or the influence of reward magnitude on activity in the amygdala and ventral tegmental area. While associative learning occurs primarily in the amygdala, learning of the temporal relationship between the cue and the associated reward is implemented as a dopamine-modulated coincidence detection mechanism in the nucleus accumbens.</p>
</section>
<section id="introduction" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">5.1</span> Introduction</h2>
<p>Dopamine (DA) is a key neuromodulator influencing processing and learning in many brain areas, such as the basal ganglia <span class="citation" data-cites="Bolam2000 Haber2000">(<a href="References.html#ref-Bolam2000" role="doc-biblioref">Bolam et al., 2000</a>; <a href="References.html#ref-Haber2000" role="doc-biblioref">Haber et al., 2000</a>)</span>, the prefrontal cortex <span class="citation" data-cites="Goldman-Rakic1992 Seamans2004">(<a href="References.html#ref-Goldman-Rakic1992" role="doc-biblioref">Goldman-Rakic et al., 1992</a>; <a href="References.html#ref-Seamans2004" role="doc-biblioref">Seamans and Yang, 2004</a>)</span> or the amygdala <span class="citation" data-cites="Bissiere2003 Pape2010">(<a href="References.html#ref-Bissiere2003" role="doc-biblioref">Bissière et al., 2003</a>; <a href="References.html#ref-Pape2010" role="doc-biblioref">Pape and Pare, 2010</a>)</span>. Dopaminergic neurons in the ventral tegmental area (VTA) and substantia nigra pars compacta (SNc) are phasically activated by unexpected rewards, aversive, salient or novel stimuli <span class="citation" data-cites="Mirenowicz1994 Schultz1993 Horvitz2000 Redgrave2008">(<a href="References.html#ref-Horvitz2000" role="doc-biblioref">Horvitz, 2000</a>; <a href="References.html#ref-Mirenowicz1994" role="doc-biblioref">Mirenowicz and Schultz, 1994</a>; <a href="References.html#ref-Redgrave2008" role="doc-biblioref">Redgrave et al., 2008</a>; <a href="References.html#ref-Schultz1993" role="doc-biblioref">Schultz et al., 1993</a>)</span>. During classical conditioning with appetitive rewards (unconditioned stimulus US), cells in VTA gradually show the same phasic activation at the onset of a reward-predicting cue (conditioned stimulus CS), but stop responding to the US when it is fully predicted <span class="citation" data-cites="Ljungberg1992 Schultz1997 Pan2005a">(<a href="References.html#ref-Ljungberg1992" role="doc-biblioref">Ljungberg et al., 1992</a>; <a href="References.html#ref-Pan2005a" role="doc-biblioref">Pan and Hyland, 2005</a>; <a href="References.html#ref-Schultz1997" role="doc-biblioref">Schultz et al., 1997</a>)</span>. If the reward is expected but omitted, VTA cells show a complete and long-lasting pause (or dip) in firing shortly after the time when the US was expected; if the reward is delivered earlier than expected, VTA cells respond phasically as if it were not predicted, but do not show a dip at the expected time <span class="citation" data-cites="Hollerman1998">(<a href="References.html#ref-Hollerman1998" role="doc-biblioref">Hollerman and Schultz, 1998</a>)</span>.</p>
<p>This phasic behavior linked to temporal expectation of reward (cancellation of US-related bursts after sufficient training, pause in firing after reward omission, normal bursts if the reward is delivered earlier) indicates that timing mechanisms play an important role in dopaminergic activation. Conversely, DA is well known to influence other timing processes, such as interval timing and duration estimation <span class="citation" data-cites="Coull2011 Kirkpatrick2013">(<a href="References.html#ref-Coull2011" role="doc-biblioref">Coull et al., 2011</a>; <a href="References.html#ref-Kirkpatrick2013" role="doc-biblioref">Kirkpatrick, 2013</a>)</span>. Reward magnitudes can alter the estimation of time in peak-interval procedures (where the consumatory response rate in anticipation of an expected reward usually peaks at the learned time), either leftward (the temporal estimation is earlier than what it really is) or rightward (later), the same effect being observed with elevated or reduced DA activity in SNc/VTA <span class="citation" data-cites="Galtress2009">(<a href="References.html#ref-Galtress2009" role="doc-biblioref">Galtress and Kirkpatrick, 2009</a>)</span>. Understanding the interaction between the reward/motivational systems and timing processes is therefore of critical importance <span class="citation" data-cites="Galtress2012 Kirkpatrick2013">(<a href="References.html#ref-Galtress2012" role="doc-biblioref">Galtress et al., 2012</a>; <a href="References.html#ref-Kirkpatrick2013" role="doc-biblioref">Kirkpatrick, 2013</a>)</span>. The objective of this article is to propose a neuro-computational model incorporating the afferent structures to the dopaminergic system which are involved in appetitive conditioning and to better describe the neural mechanisms leading to the observed temporal behaviour of dopaminergic neurons.</p>
<p>The <em>temporal difference</em> (TD) algorithm originally proposed by <span class="citation" data-cites="Sutton1981">(<a href="References.html#ref-Sutton1981" role="doc-biblioref">Sutton and Barto, 1981</a>)</span> has become an influential model linking DA activity to timing mechanisms <span class="citation" data-cites="Montague1996 Schultz1997">(<a href="References.html#ref-Montague1996" role="doc-biblioref">Montague et al., 1996</a>; <a href="References.html#ref-Schultz1997" role="doc-biblioref">Schultz et al., 1997</a>)</span>. TD is a unitary mechanism describing DA activity as a reward-prediction error: the difference between the reward expectation in a given state and the actually received reward. Early implementations of TD have used serial-compound representations to represent the presence of a stimulus over time, allowing to reproduce some aspects of DA firing during classical conditioning by chaining backwards in time the association between the CS and the US <span class="citation" data-cites="Suri1999 Suri2001">(<a href="References.html#ref-Suri1999" role="doc-biblioref">Suri and Schultz, 1999</a>, <a href="References.html#ref-Suri2001" role="doc-biblioref">2001</a>)</span>. This would predict a progressive backward shift of the US-related burst during learning, what is experimentally not the case, as the CS- and US-related bursts gradually increase and decrease with learning, respectively. Different temporal representations of the stimuli can overcome this issue. Using long eligibility traces (TD(<span class="math inline">\lambda</span>), <span class="citation" data-cites="Sutton1998">(<a href="References.html#ref-Sutton1998" role="doc-biblioref">Sutton and Barto, 1998</a>)</span>), the algorithm can be turned into a more advanced associative learning rule to better fit the experimental data <span class="citation" data-cites="Pan2005a">(<a href="References.html#ref-Pan2005a" role="doc-biblioref">Pan and Hyland, 2005</a>)</span>. Using a series of internal microstimuli growing weaker and more diffuse over time also allows to overcome this problem as well as to better capture DA activity when a reward is delivered earlier as predicted <span class="citation" data-cites="Ludvig2008">(<a href="References.html#ref-Ludvig2008" role="doc-biblioref">Ludvig et al., 2008</a>)</span>. An adequate temporal representation of stimuli can even be learned in an unsupervised manner through the use of long short-term memory (LSTM) networks <span class="citation" data-cites="Rivest2010 Rivest2013">(<a href="References.html#ref-Rivest2010" role="doc-biblioref">Rivest et al., 2010</a>; <a href="References.html#ref-Rivest2013" role="doc-biblioref">Rivest et al., 2013</a>)</span>. Overall, TD-based algorithms are an important model of DA activity, both because of their mathematical elegance and predictive power, and are widely used for explaining experimental data in decision-making (for example <span class="citation" data-cites="Daw2005 Samejima2007 Rao2010">Daw et al. (<a href="References.html#ref-Daw2005" role="doc-biblioref">2005</a>; <a href="References.html#ref-Rao2010" role="doc-biblioref">Rao, 2010</a>; <a href="References.html#ref-Samejima2007" role="doc-biblioref">Samejima and Doya, 2007</a>)</span>) and in neurorobotical systems (for example <span class="citation" data-cites="Sporns2002 Krichmar2013">(<a href="References.html#ref-Krichmar2013" role="doc-biblioref">Krichmar, 2013</a>; Sporns and Alexander; <a href="References.html#ref-Sporns2002" role="doc-biblioref">2002</a>)</span>).</p>
<p>Other models have been proposed to better explain the experimental data while improving the biological plausibility. One important class of models are the <em>dual-pathway</em> models, which hypothesize that the different components of DA activation are computed in segregated brain areas projecting onto the SNc/VTA <span class="citation" data-cites="Brown1999 Tan2008 OReilly2007 Hazy2010">(<a href="References.html#ref-Brown1999" role="doc-biblioref">Brown et al., 1999</a>; <a href="References.html#ref-Hazy2010" role="doc-biblioref">Hazy et al., 2010</a>; <a href="References.html#ref-OReilly2007" role="doc-biblioref">O’Reilly et al., 2007</a>; <a href="References.html#ref-Tan2008" role="doc-biblioref">Tan and Bullock, 2008</a>)</span>. These models share some common assumptions about the mechanisms, although the putative brain areas may differ: reward delivery provokes DA bursts through glutamatergic projections from the pedunculopontine nucleus (PPTN); the conditioning strength of the CS is first acquired in the amygdala or the ventral striatum and then transferred to the DA cells either directly or through PPTN; the cancellation of predicted US bursts and the dips at reward omission originate from the striosomes of the dorsal or ventral striatum which project inhibitorily to VTA/SNC. The origin of the latter signals, which have a strong temporal component, differ however between these models. The models by <span class="citation" data-cites="Brown1999">Brown et al. (<a href="References.html#ref-Brown1999" role="doc-biblioref">1999</a>)</span> and <span class="citation" data-cites="Tan2008">Tan and Bullock (<a href="References.html#ref-Tan2008" role="doc-biblioref">2008</a>)</span> consider that cells in the striosomes of the dorsal and ventral striatum implement an <em>intracellular spectral timing</em> mechanism <span class="citation" data-cites="Grossberg1989">(<a href="References.html#ref-Grossberg1989" role="doc-biblioref">Grossberg and Schmajuk, 1989</a>)</span>, where each cell in these populations has an internal calcium variable peaking at a given time after the CS onset and emits delayed spikes. The cell being active at reward delivery (signaled by the DA burst) becomes representative of the elapsed duration. The models by <span class="citation" data-cites="OReilly2007">O’Reilly et al. (<a href="References.html#ref-OReilly2007" role="doc-biblioref">2007</a>)</span> and <span class="citation" data-cites="Hazy2010">Hazy et al. (<a href="References.html#ref-Hazy2010" role="doc-biblioref">2010</a>)</span> more abstractly consider a ramping function peaking at the estimated reward delivery time, and originating from the cerebellum. How this timing signal from the cerebellum is adapted to different CS-US intervals is not explicitely modeled.</p>
<p>Spectral timing mechanisms have been observed in the cerebellum <span class="citation" data-cites="Fiala1996">(<a href="References.html#ref-Fiala1996" role="doc-biblioref">Fiala et al., 1996</a>)</span> but not in the striatum. The cerebellum is critically involved in aversive conditioning such as the rabbit eye-blink conditioning <span class="citation" data-cites="Christian2003 Thompson2009">(<a href="References.html#ref-Christian2003" role="doc-biblioref">Christian and Thompson, 2003</a>; <a href="References.html#ref-Thompson2009" role="doc-biblioref">Thompson and Steinmetz, 2009</a>)</span>, but its involvement in appetitive conditioning is still unknown (see <span class="citation" data-cites="Martin-Soelch2007">Martin-Soelch et al. (<a href="References.html#ref-Martin-Soelch2007" role="doc-biblioref">2007</a>)</span>). Moreover, the intracellular mechanisms necessary for spectral timing may not efficiently apply to the supra-second range used in most appetitive conditioning experiments <span class="citation" data-cites="Matell2004 Coull2011">(<a href="References.html#ref-Coull2011" role="doc-biblioref">Coull et al., 2011</a>; <a href="References.html#ref-Matell2004" role="doc-biblioref">Matell and Meck, 2004</a>)</span>. The neural substrate of temporal learning in dual-pathway models of the dopaminergic system needs further investigation.</p>
<p>The goal of the the present article is to investigate how far dual-pathway models of reward prediction can be adapted to take into account the recent wealth of experiments investigating timing processes in the brain <span class="citation" data-cites="Coull2011 Kirkpatrick2013">(<a href="References.html#ref-Coull2011" role="doc-biblioref">Coull et al., 2011</a>; <a href="References.html#ref-Kirkpatrick2013" role="doc-biblioref">Kirkpatrick, 2013</a>)</span>. Although most of them focus on operant conditioning, they point at a critical role of the striatum in learning supra-second durations. One of the most biologically plausible model of interval timing to date is the <em>Striatal-Beat Frequency</em> model <span class="citation" data-cites="Matell2000 Matell2004 Lustig2005">(<a href="References.html#ref-Lustig2005" role="doc-biblioref">Lustig et al., 2005</a>; <a href="References.html#ref-Matell2000" role="doc-biblioref">Matell and Meck, 2000</a>; <a href="References.html#ref-Matell2004" role="doc-biblioref">Matell and Meck, 2004</a>)</span>, which proposes that striatal neurons act as coincidence detectors, reacting maximally when a series of cortical oscillators, synchronized at CS onset, is in a particular configuration. We propose that a similar mechanism is used to control the temporal behavior of dopaminergic cells during appetitive conditioning.</p>
<p>We present a neuro-computational model incorporating many areas involved in appetitive conditioning and reward processing, including the amygdala, the ventral basal ganglia and various forebrain nuclei projecting to VTA/SNc. It focuses on the phasic components of dopaminergic activation and reproduces the behavior of VTA cells during conditioning, especially with respect to different reward magnitudes, reward omission or earlier delivery. However, it is not designed to address the tonic component of DA activation, nor the observed dependency of VTA firing on reward probability <span class="citation" data-cites="Fiorillo2003">(<a href="References.html#ref-Fiorillo2003" role="doc-biblioref">Fiorillo et al., 2003</a>)</span>. From the computational point of view, it provides a robust and autonomous mechanism to learn CS-US associations with variable durations.</p>
</section>
<section id="material-methods" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="material-methods"><span class="header-section-number">5.2</span> Material &amp; methods</h2>
<section id="neurobiological-assumptions" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="neurobiological-assumptions"><span class="header-section-number">5.2.1</span> Neurobiological assumptions</h3>
<section id="appetitive-delay-conditioning" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="appetitive-delay-conditioning">Appetitive delay conditioning</h4>
<p>The proposed model of dopaminergic activation during conditioning is restricted in its current form to appetitive conditioning, where the US is a physical reward such as food. Aversive conditioning, where the US is a painful stimulation or a frightening stimulus, engages similar structures - in particular, the amygdala, the ventral striatum and the dopaminergic system <span class="citation" data-cites="LeDoux2000 Delgado2008 Matsumoto2009">(<a href="References.html#ref-Delgado2008" role="doc-biblioref">Delgado et al., 2008</a>; <a href="References.html#ref-LeDoux2000" role="doc-biblioref">LeDoux, 2000</a>; <a href="References.html#ref-Matsumoto2009" role="doc-biblioref">Matsumoto and Hikosaka, 2009</a>)</span> - but the model does not aim at reproducing these effects. The cerebellum plays a much more important role in aversive than in appetitive conditioning <span class="citation" data-cites="Thompson2009">(<a href="References.html#ref-Thompson2009" role="doc-biblioref">Thompson and Steinmetz, 2009</a>)</span>. There is still a debate on whether the same DA cells are activated by appetitive and aversive rewards or if two segregated populations exist <span class="citation" data-cites="Lammel2012">(<a href="References.html#ref-Lammel2012" role="doc-biblioref">Lammel et al., 2012</a>)</span>.</p>
<p>The model is also limited to delay conditioning, where the CS is still physically present (visually or auditorily) when the US arrives. Trace conditioning introduces a temporal gap between the CS and the US. In this case, even small intervals can impair the learned association strength <span class="citation" data-cites="Raybuck2013">(<a href="References.html#ref-Raybuck2013" role="doc-biblioref">Raybuck and Lattal, 2013</a>)</span>. The medial prefrontal cortex and hippocampus are necessary for trace conditioning to take place, but not delay conditioning <span class="citation" data-cites="Ito2006 Walker2008 Wu2013">(<a href="References.html#ref-Ito2006" role="doc-biblioref">Ito et al., 2006</a>; <a href="References.html#ref-Walker2008" role="doc-biblioref">Walker and Steinmetz, 2008</a>; <a href="References.html#ref-Wu2013" role="doc-biblioref">Wu et al., 2013</a>)</span>. This indicates that working memory processes (either through sustained activation or synaptic traces) are involved in trace conditioning, what is not covered by this model. Some TD-based implementations are able to learn both delay and trace conditioning tasks: the model of <span class="citation" data-cites="Ludvig2008">Ludvig et al. (<a href="References.html#ref-Ludvig2008" role="doc-biblioref">2008</a>)</span> uses a series of temporal basis functions to represent the trace of the stimuli, what allows the TD algorithm to associate reward delivery to the correct timing. The model of <span class="citation" data-cites="Rivest2010 Rivest2013">Rivest et al. (<a href="References.html#ref-Rivest2010" role="doc-biblioref">2010</a>; <a href="References.html#ref-Rivest2013" role="doc-biblioref">Rivest et al., 2013</a>)</span> learns an adequate temporal representation for both CS and US using a long short-term memory (LSTM) network <span class="citation" data-cites="Hochreiter1997">(<a href="References.html#ref-Hochreiter1997" role="doc-biblioref">Hochreiter and Schmidhuber, 1997</a>)</span> which is able to fill an eventual gap between the CS and the US.</p>
<p>Dual-pathway models focus mainly on delay conditioning: <span class="citation" data-cites="Brown1999">Brown et al. (<a href="References.html#ref-Brown1999" role="doc-biblioref">1999</a>)</span> propose that a bistable representation of CS information, mimicking the sustained activation in the prefrontal cortex during working memory processes <span class="citation" data-cites="Funahashi1993">(<a href="References.html#ref-Funahashi1993" role="doc-biblioref">Funahashi et al., 1993</a>)</span>, could bridge the temporal gap between the CS and the US, while <span class="citation" data-cites="OReilly2007">O’Reilly et al. (<a href="References.html#ref-OReilly2007" role="doc-biblioref">2007</a>)</span> couple their model of DA activity with a neuro-computational model of working memory involving the prefrontal cortex and the basal ganglia in order to address trace conditioning <span class="citation" data-cites="OReilly2006">(<a href="References.html#ref-OReilly2006" role="doc-biblioref">O’Reilly and Frank, 2006</a>)</span>.</p>
<p>In the experiments shown in this article, the CS is an individual visual stimulus that activates specific clusters of cells in the inferotemporal cortex (IT). Object-level representations in IT allow to provide the prefrontal cortex, the amygdala and the basal ganglia with rich detailed representations of visual objects <span class="citation" data-cites="Tanaka2000">(<a href="References.html#ref-Tanaka2000" role="doc-biblioref">Tanaka, 2000</a>)</span>. However, inputs to the model could be easily adapted to auditory inputs. The US is a food reward, activating the lateral hypothalamus (LH). Neurons in LH are activated by the specific taste components of a single reward, proportionally to their magnitude <span class="citation" data-cites="Nakamura1986">(<a href="References.html#ref-Nakamura1986" role="doc-biblioref">Nakamura and Ono, 1986</a>)</span>. Rewards are therefore represented by a combination of tastes (for example fat, sugar, salt, umami, as in the MOTIVATOR model of <span class="citation" data-cites="Dranias2008">Dranias et al. (<a href="References.html#ref-Dranias2008" role="doc-biblioref">2008</a>)</span>) allowing to distinguish different rewards from each other by their nature instead of only their relative magnitude.</p>
</section>
<section id="role-of-vta-and-forebrain-structures" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="role-of-vta-and-forebrain-structures">Role of VTA and forebrain structures</h4>
<p>The midbrain dopaminergic system is predominantly composed of the SNc and VTA. VTA plays a specific role in the facilitation of approach behaviors and incentive learning <span class="citation" data-cites="Fields2007">(<a href="References.html#ref-Fields2007" role="doc-biblioref">Fields et al., 2007</a>)</span>, while SNc is more involved in motor and cognitive processes, although this functional distinction is more based on anatomical considerations than direct observations <span class="citation" data-cites="Haber2003">(<a href="References.html#ref-Haber2003" role="doc-biblioref">Haber, 2003</a>)</span>. The proposed model focuses on VTA activation during conditioning because of its central role in the reward circuitry <span class="citation" data-cites="Sesack2010">(<a href="References.html#ref-Sesack2010" role="doc-biblioref">Sesack and Grace, 2010</a>)</span>, but it is not excluded that a similar behaviour is observed in SNc because of the spiraling structure of striato-nigro-striatal pathways <span class="citation" data-cites="Haber2000">(<a href="References.html#ref-Haber2000" role="doc-biblioref">Haber et al., 2000</a>)</span>.</p>
<p>Dopaminergic neurons in VTA exhibit a relatively low tonic activity (around 5Hz), but react phasically with a short-latency (<span class="math inline">&lt;</span> 100ms), short-duration (<span class="math inline">&lt;</span> 200ms) burst of high activity in response to unpredicted rewards, aversive, salient or novel stimuli <span class="citation" data-cites="Mirenowicz1994 Schultz1993 Horvitz2000 Redgrave2008">(<a href="References.html#ref-Horvitz2000" role="doc-biblioref">Horvitz, 2000</a>; <a href="References.html#ref-Mirenowicz1994" role="doc-biblioref">Mirenowicz and Schultz, 1994</a>; <a href="References.html#ref-Redgrave2008" role="doc-biblioref">Redgrave et al., 2008</a>; <a href="References.html#ref-Schultz1993" role="doc-biblioref">Schultz et al., 1993</a>)</span>. After appetitive conditioning, the same cells also react phasically to reward-predicting stimuli <span class="citation" data-cites="Schultz1997">(<a href="References.html#ref-Schultz1997" role="doc-biblioref">Schultz et al., 1997</a>)</span>. These phasic bursts of activity for both unpredicted rewards and reward-predicting cues are dependent on glutamatergic activation by PPTN <span class="citation" data-cites="Dormont1998 Lokwan1999 Pan2005">(<a href="References.html#ref-Dormont1998" role="doc-biblioref">Dormont et al., 1998</a>; <a href="References.html#ref-Lokwan1999" role="doc-biblioref">Lokwan et al., 1999</a>; <a href="References.html#ref-Pan2005" role="doc-biblioref">Pan et al., 2005</a>)</span>, which is itself driven by inputs from LH and the central nucleus of the amygdala (CE) <span class="citation" data-cites="Semba1992">(<a href="References.html#ref-Semba1992" role="doc-biblioref">Semba and Fibiger, 1992</a>)</span>. Excitatory inputs from the prefrontal cortex (PFC) to VTA, PPTN and LH exert a regulatory role on this bursting behavior <span class="citation" data-cites="Fields2007 Geisler2008">(<a href="References.html#ref-Fields2007" role="doc-biblioref">Fields et al., 2007</a>; <a href="References.html#ref-Geisler2008" role="doc-biblioref">Geisler and Wise, 2008</a>)</span> and regulate plasticity in VTA <span class="citation" data-cites="Wolf2004">(<a href="References.html#ref-Wolf2004" role="doc-biblioref">Wolf et al., 2004</a>)</span>.</p>
<p>The mechanisms underlying inhibitory control of VTA are less clear. VTA receives predominantly GABAergic synapses from the ventral basal ganglia (BG), especially from the ventromedial shell of the nucleus accumbens (NAcc) and the ventral pallidum (VP) <span class="citation" data-cites="Zahm1990 Usuda1998">(<a href="References.html#ref-Usuda1998" role="doc-biblioref">Usuda et al., 1998</a>; <a href="References.html#ref-Zahm1990" role="doc-biblioref">Zahm and Heimer, 1990</a>)</span>. These inhibitory projections are known to control the number of DA neurons in VTA able to switch from an hyperpolarized state to an irregular spontaneous firing rate around 5Hz. There is also a large number of GABAergic neurons in VTA (around 30%) but they predominantly project outside VTA <span class="citation" data-cites="Carr2000">(<a href="References.html#ref-Carr2000" role="doc-biblioref">Carr and Sesack, 2000</a>)</span>. A recently labeled area posterior to the VTA, the rostromedial tegmental nucleus (RMTg), has been shown to provide a strong GABAergic inhibition on dopaminergic VTA cells, able to produce the dip observed at reward omission <span class="citation" data-cites="Jhou2009 Lavezzi2011 Bourdy2012">(<a href="References.html#ref-Bourdy2012" role="doc-biblioref">Bourdy and Barrot, 2012</a>; <a href="References.html#ref-Jhou2009" role="doc-biblioref">Jhou et al., 2009</a>; <a href="References.html#ref-Lavezzi2011" role="doc-biblioref">Lavezzi and Zahm, 2011</a>)</span>. Neurons in RMTg are excited by aversive events and reward omission, and this activation is provoked by excitatory projections from the lateral habenula (LHb) which is activated in the same conditions <span class="citation" data-cites="Hikosaka2008 Balcita2011 Bromberg-Martin2011 Hong2011">(<a href="References.html#ref-Balcita2011" role="doc-biblioref">Balcita-Pedicino et al., 2011</a>; <a href="References.html#ref-Bromberg-Martin2011" role="doc-biblioref">Bromberg-Martin and Hikosaka, 2011</a>; <a href="References.html#ref-Hikosaka2008" role="doc-biblioref">Hikosaka et al., 2008</a>; <a href="References.html#ref-Hong2011" role="doc-biblioref">Hong et al., 2011</a>)</span>.</p>
</section>
<section id="role-of-the-amygdala" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="role-of-the-amygdala">Role of the amygdala</h4>
<p>The amygdala is long known for its involvement in acquiring and expressing auditory fear conditioning <span class="citation" data-cites="LeDoux2000">(<a href="References.html#ref-LeDoux2000" role="doc-biblioref">LeDoux, 2000</a>)</span>. Neurons in the basolateral amygdala (BLA), the major input structure of the amygdala, learn to associate CS and US representation, based either on thalamic or cortical information <span class="citation" data-cites="Doyere2003">(<a href="References.html#ref-Doyere2003" role="doc-biblioref">Doyère et al., 2003</a>)</span>, with long-term potentiation being modulated by dopaminergic innervation from VTA <span class="citation" data-cites="Bissiere2003">(<a href="References.html#ref-Bissiere2003" role="doc-biblioref">Bissière et al., 2003</a>)</span>. The output structure of the amygdala, the central nucleus of the amygdala (CE) is critical for expressing fear conditioning (conditioned responses), through its projections on various brainstem nuclei <span class="citation" data-cites="Koo2004">(<a href="References.html#ref-Koo2004" role="doc-biblioref">Koo et al., 2004</a>)</span>.</p>
<p>However, the amygdala is now recognized to be also involved in appetitive conditioning and reward processing <span class="citation" data-cites="Baxter2002 Murray2007">(<a href="References.html#ref-Baxter2002" role="doc-biblioref">Baxter and Murray, 2002</a>; <a href="References.html#ref-Murray2007" role="doc-biblioref">Murray, 2007</a>)</span>. The amygdala and LH both react to the palability of rewards, suggesting either common afferences in the brainstem, a direct projection from LH to BLA <span class="citation" data-cites="Sah2003">(<a href="References.html#ref-Sah2003" role="doc-biblioref">Sah et al., 2003</a>)</span> or an indirect one through the gustatory thalamus, as lesions of the gustatory brainstem nuclei abolish food-elicited responses in both LH and the amygdala <span class="citation" data-cites="Nishijo2000">(<a href="References.html#ref-Nishijo2000" role="doc-biblioref">Nishijo et al., 2000</a>)</span>. In this model, we assume a direct projection from LH to BLA, but how the amygdala gets access to the value of a food reward is still not clear.</p>
<p>BLA neurons have been shown to respond proportionally to reward magnitude <span class="citation" data-cites="Bermudez2010">(<a href="References.html#ref-Bermudez2010" role="doc-biblioref">Bermudez and Schultz, 2010</a>)</span>. They also respond to both reward-predicting cues and the associated rewards, with a sustained activation during the delay <span class="citation" data-cites="Ono1995 Nishijo2008">(<a href="References.html#ref-Nishijo2008" role="doc-biblioref">Nishijo et al., 2008</a>; <a href="References.html#ref-Ono1995" role="doc-biblioref">Ono et al., 1995</a>)</span>. This places the BLA at a central position for learning CS-US associations, or more precisely associating the value of the US to the sensory representation of the CS. This information is transferred to CE, which is able to activate VTA, either through direct projections <span class="citation" data-cites="Fudge2000">(<a href="References.html#ref-Fudge2000" role="doc-biblioref">Fudge and Haber, 2000</a>)</span> - although they are quite weak and have only been observed in primates -, or more likely indirectly through excitation of PPTN <span class="citation" data-cites="Semba1992 Lee2011">(<a href="References.html#ref-Lee2011" role="doc-biblioref">Lee et al., 2011</a>; <a href="References.html#ref-Semba1992" role="doc-biblioref">Semba and Fibiger, 1992</a>)</span>.</p>
</section>
<section id="role-of-the-ventral-basal-ganglia" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="role-of-the-ventral-basal-ganglia">Role of the ventral basal ganglia</h4>
<p>The ventral BG plays a critical role in learning goal-oriented behaviors and is considered as an interface between the limbic and motor systems, as it receives converging inputs from the amygdala, hippocampus and prefrontal cortex <span class="citation" data-cites="Nicola2007 Humphries2010">(<a href="References.html#ref-Humphries2010" role="doc-biblioref">Humphries and Prescott, 2010</a>; <a href="References.html#ref-Nicola2007" role="doc-biblioref">Nicola, 2007</a>)</span>. Its major input structure, the ventral striatum, is mostly composed of the nucleus accumbens (NAcc), itself decomposed into core and shell territories, but also extends without a clear demarkation into the caudate nucleus and the putamen, accounting for around 20% of the whole striatum <span class="citation" data-cites="Haber2010">(<a href="References.html#ref-Haber2010" role="doc-biblioref">Haber and Knutson, 2010</a>)</span>. It is primarily composed of GABAergic medium-spiny projection neurons (MSN, 90%), as well as tonically-active cholinergic neurons (TAN) and GABAergic interneurons. MSN neurons project on the ventral pallidum (VP), VTA, SNc, LH and PPTN. They receive inputs from VP, VTA, LH, BLA and the subiculum (part of the hippocampal formation) <span class="citation" data-cites="Humphries2010 Sesack2010">(<a href="References.html#ref-Humphries2010" role="doc-biblioref">Humphries and Prescott, 2010</a>; <a href="References.html#ref-Sesack2010" role="doc-biblioref">Sesack and Grace, 2010</a>)</span>.</p>
<p>NAcc is involved in learning the incentive motivational value of rewards <span class="citation" data-cites="Robbins1996 Nicola2007 Galtress2010">(<a href="References.html#ref-Galtress2010" role="doc-biblioref">Galtress and Kirkpatrick, 2010</a>; <a href="References.html#ref-Nicola2007" role="doc-biblioref">Nicola, 2007</a>; <a href="References.html#ref-Robbins1996" role="doc-biblioref">Robbins and Everitt, 1996</a>)</span>. Excitatory inputs from the BLA have been shown necessary to promote reward-seeking behaviors and enable the cue-evoked excitation of NAcc during operant conditioning. NAcc is also involved in Pavlovian reward learning, with single neurons being phasically activated by both CS and US after sufficient training <span class="citation" data-cites="Day2007">(<a href="References.html#ref-Day2007" role="doc-biblioref">Day and Carelli, 2007</a>)</span>. Learning in NAcc has been shown to depend strongly on dopaminergic innervation from VTA <span class="citation" data-cites="Eyny2003">(<a href="References.html#ref-Eyny2003" role="doc-biblioref">Eyny and Horvitz, 2003</a>)</span>.</p>
<p>VP, the output structure of the ventral BG, is also strongly involved in reward processing and reward expectation <span class="citation" data-cites="Smith2009 Tachibana2012">(<a href="References.html#ref-Smith2009" role="doc-biblioref">Smith et al., 2009</a>; <a href="References.html#ref-Tachibana2012" role="doc-biblioref">Tachibana and Hikosaka, 2012</a>)</span>. It receives GABAergic projections from NAcc, excitatory projections from PPTN, and projects to SNc/VTA, LHb, RMTg and the mediodorsal nucleus of the thalamus (MD) <span class="citation" data-cites="Hallanger1988 Jhou2009 Haber2010">(<a href="References.html#ref-Haber2010" role="doc-biblioref">Haber and Knutson, 2010</a>; <a href="References.html#ref-Hallanger1988" role="doc-biblioref">Hallanger and Wainer, 1988</a>; <a href="References.html#ref-Jhou2009" role="doc-biblioref">Jhou et al., 2009</a>)</span>. During classical conditioning, VP cells are excited by reward-predicting cues and the associated reward when the reward is large, but inhibited by small rewards <span class="citation" data-cites="Tindell2004">(<a href="References.html#ref-Tindell2004" role="doc-biblioref">Tindell et al., 2004</a>)</span>. The NAcc <span class="math inline">\rightarrow</span> VP pathway is therefore considered a major route for disinhibiting efferent structures at CS onset and reward delivery and guide reward-orienting behaviors <span class="citation" data-cites="Sesack2010">(<a href="References.html#ref-Sesack2010" role="doc-biblioref">Sesack and Grace, 2010</a>)</span>.</p>
<p>Regarding the involvement of the ventral BG in timing, the current evidence is rather controversial. Two lesion studies showed no involvement of NAcc in the timing of instrumental responding <span class="citation" data-cites="Meck2006 Galtress2010">(<a href="References.html#ref-Galtress2010" role="doc-biblioref">Galtress and Kirkpatrick, 2010</a>; <a href="References.html#ref-Meck2006" role="doc-biblioref">Meck, 2006</a>)</span>, but <span class="citation" data-cites="Singh2011">Singh et al. (<a href="References.html#ref-Singh2011" role="doc-biblioref">2011</a>)</span> showed that lesions of NAcc induce a deficit in learning the timing of Pavlovian responses. The NAcc and the medial caudate nucleus robustly activate during reward anticipation <span class="citation" data-cites="Deadwyler2004">(<a href="References.html#ref-Deadwyler2004" role="doc-biblioref">Deadwyler et al., 2004</a>)</span>, while the rostroventral putamen most reliably deactivates in response to nonreward delivery <span class="citation" data-cites="McClure2003 ODoherty2003">(<a href="References.html#ref-McClure2003" role="doc-biblioref">McClure et al., 2003</a>; <a href="References.html#ref-ODoherty2003" role="doc-biblioref">O’Doherty et al., 2003</a>)</span>. Lesions of NAcc have recently been shown to disrupt reinforcement-omission effects <span class="citation" data-cites="Judice-Daher2013">(<a href="References.html#ref-Judice-Daher2013" role="doc-biblioref">Judice-Daher and Bueno, 2013</a>)</span>. However, no cellular recordings have yet shown that NAcc cells react specifically to reward omission.</p>
<p>In this model, we form the hypothesis that a subset of NAcc cells learns the precise time when a reward is expected and gets activated when it is omitted. Recent advances in the neurobiology of interval timing show that a similar mechanism is likely to occur in the dorsal striatum during peak-interval tasks <span class="citation" data-cites="Matell2004 Coull2011">(<a href="References.html#ref-Coull2011" role="doc-biblioref">Coull et al., 2011</a>; <a href="References.html#ref-Matell2004" role="doc-biblioref">Matell and Meck, 2004</a>)</span>. The <em>Striatal-Beat Frequency</em> model <span class="citation" data-cites="Matell2000 Lustig2005">(<a href="References.html#ref-Lustig2005" role="doc-biblioref">Lustig et al., 2005</a>; <a href="References.html#ref-Matell2000" role="doc-biblioref">Matell and Meck, 2000</a>)</span> has proposed that striatal cells act as coincidence detectors, learning to react to a particular configuration of cortical inputs when a DA burst occurs and to signal the temporal expectation of reward. In this framework, cortical inputs oscillate at various frequencies in the alpha range (8-13Hz) and are synchronized at cue-onset. This provides an unique population code for the time elapsed since cue onset, so striatal cells can learn to react to a specific duration through dopamine-modulated long-term potentiation (LTP) or depression (LTD) <span class="citation" data-cites="Calabresi2007 Shen2008">(<a href="References.html#ref-Calabresi2007" role="doc-biblioref">Calabresi et al., 2007</a>; <a href="References.html#ref-Shen2008" role="doc-biblioref">Shen et al., 2008</a>)</span>. We consider a similar mechanism here for learning CS-US interval durations in NAcc.</p>
<p>Synaptic plasticity at corticostriatal synapses depends on the polarization of the membrane potential: in the hyperpolarized state (-90mV, called the down-state), striatal cells exhibit mostly LTD at active synapses; in the depolarized state (-60mV, the up-state), these cells exhibit LTP or LTD depending on the extracellular dopamine level <span class="citation" data-cites="Calabresi2007 Shen2008">(<a href="References.html#ref-Calabresi2007" role="doc-biblioref">Calabresi et al., 2007</a>; <a href="References.html#ref-Shen2008" role="doc-biblioref">Shen et al., 2008</a>)</span>. Neurons in NAcc exhibit these up- and down-states <span class="citation" data-cites="ODonnell1995">(<a href="References.html#ref-ODonnell1995" role="doc-biblioref">O’Donnell and Grace, 1995</a>)</span>, and the transition from the down-state to the up-state depends either on phasic DA release from VTA <span class="citation" data-cites="Gruber2003 Goto2005">(<a href="References.html#ref-Goto2005" role="doc-biblioref">Goto and Grace, 2005</a>; <a href="References.html#ref-Gruber2003" role="doc-biblioref">Gruber et al., 2003</a>)</span>, afferent input from the ventral subiculum of the hippocampus <span class="citation" data-cites="ODonnell1995">(<a href="References.html#ref-ODonnell1995" role="doc-biblioref">O’Donnell and Grace, 1995</a>)</span> or a conjunction of medial prefrontal cortex and amygdala inputs <span class="citation" data-cites="McGinty2009">(<a href="References.html#ref-McGinty2009" role="doc-biblioref">McGinty and Grace, 2009</a>)</span>. This mechanism is thought to help restricting striatal firing to the exact time when reward is expected: NAcc cells are brought in the up-state by DA bursts at reward delivery, allowing the to learn the precise cortical pattern. After learning the same cell could be brought in the up-state only by this cortical pattern (in conjunction with BLA inputs), even if VTA is not bursting <span class="citation" data-cites="Matell2004">(<a href="References.html#ref-Matell2004" role="doc-biblioref">Matell and Meck, 2004</a>)</span>.</p>
</section>
</section>
<section id="the-proposed-model" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="the-proposed-model"><span class="header-section-number">5.2.2</span> The proposed model</h3>
<section id="overview" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="overview">Overview</h4>
<div id="fig-finr:model" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-finr:model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/finr/01.jpg" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-finr:model-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.1: Functional description of the model. Pointed arrows represent excitatory connections, rounded arrows represent inhibitory projections. Dashed lines represent learnable connections, while solid represent fixed connections. LH signals US delivery to BLA <span class="citation" data-cites="Sah2003">(<a href="References.html#ref-Sah2003" role="doc-biblioref">Sah et al., 2003</a>)</span> and PPTN <span class="citation" data-cites="Semba1992">(<a href="References.html#ref-Semba1992" role="doc-biblioref">Semba and Fibiger, 1992</a>)</span>. IT encode a visual representation of the CS, which activates BLA <span class="citation" data-cites="Cheng1997">(<a href="References.html#ref-Cheng1997" role="doc-biblioref">Cheng et al., 1997</a>)</span> and vmPFC <span class="citation" data-cites="Carmichael1995">(<a href="References.html#ref-Carmichael1995" role="doc-biblioref">Carmichael and Price, 1995</a>)</span>. BLA learns to associates the CS and US representations under the modulatory influence of the DA released by VTA <span class="citation" data-cites="Bissiere2003">(<a href="References.html#ref-Bissiere2003" role="doc-biblioref">Bissière et al., 2003</a>)</span> and projects on CE <span class="citation" data-cites="LeDoux2000">(<a href="References.html#ref-LeDoux2000" role="doc-biblioref">LeDoux, 2000</a>)</span> which excites PPTN <span class="citation" data-cites="Semba1992">(<a href="References.html#ref-Semba1992" role="doc-biblioref">Semba and Fibiger, 1992</a>)</span>. The excitatory projection from PPTN to VTA is able to provoke phasic DA bursts <span class="citation" data-cites="Lokwan1999">(<a href="References.html#ref-Lokwan1999" role="doc-biblioref">Lokwan et al., 1999</a>)</span>. NAcc MSN neurons receives excitatory projections from BLA <span class="citation" data-cites="Ambroggi2008">(<a href="References.html#ref-Ambroggi2008" role="doc-biblioref">Ambroggi et al., 2008</a>)</span> and vmPFC <span class="citation" data-cites="Haber2003">(<a href="References.html#ref-Haber2003" role="doc-biblioref">Haber, 2003</a>)</span> and learning is modulated by DA release from VTA <span class="citation" data-cites="Robbins1996">(<a href="References.html#ref-Robbins1996" role="doc-biblioref">Robbins and Everitt, 1996</a>)</span>. They inhibit VTA dopaminergic neurons <span class="citation" data-cites="Usuda1998">(<a href="References.html#ref-Usuda1998" role="doc-biblioref">Usuda et al., 1998</a>)</span> and VP <span class="citation" data-cites="Zahm1990">(<a href="References.html#ref-Zahm1990" role="doc-biblioref">Zahm and Heimer, 1990</a>)</span>. VP also receives excitatory projections from PPTN <span class="citation" data-cites="Hallanger1988">(<a href="References.html#ref-Hallanger1988" role="doc-biblioref">Hallanger and Wainer, 1988</a>)</span> and inhibits both LHb and RMTg <span class="citation" data-cites="Haber2010">(<a href="References.html#ref-Haber2010" role="doc-biblioref">Haber and Knutson, 2010</a>)</span>. LHb excites RMTg <span class="citation" data-cites="Balcita2011">(<a href="References.html#ref-Balcita2011" role="doc-biblioref">Balcita-Pedicino et al., 2011</a>)</span> which in turn inhibits VTA <span class="citation" data-cites="Jhou2009">(<a href="References.html#ref-Jhou2009" role="doc-biblioref">Jhou et al., 2009</a>)</span>. Abbreviations: LH lateral hypothalamus; IT inferotemporal cortex; BLA basolateral nucleus of the amygdala; CE central nucleus of the amygdala; vmPFC ventromedial prefrontal cortex; PPTN pedunculopontine nucleus; VTA ventral tegmental area; NAcc nucleus accumbens; VP ventral pallidum; LHb lateral habenula; RMTg rostromedial tegmental nucleus.
</figcaption>
</figure>
</div>
<p>In this section, we will explain the major flows of information and learning in the model before describing more precisely the details of the model, depicted on <a href="#fig-finr:model" class="quarto-xref">Figure&nbsp;<span>5.1</span></a>. Most experiments in this article will concern the concurrent learning of three different CS-US associations, each using different visual and gustatory representations, and with different CS-US intervals (see <a href="#sec-finr:inputs" class="quarto-xref"><span>Section 5.2.2.1</span></a>). The first phase of learning represents sensitization to the rewards, by presenting each reward individually ten times. The US representation activates a set of cells in LH, depending of the basic tastes composing it, what in turn activates the US-selective population of PPTN, provoking a phasic DA burst in VTA which gates learning in BLA. After sufficient exposure to each reward, BLA has self-organized to represent them individually by the activation of a single cell. Meanwhile, BLA progressively learns to activate CE, which in turn activates the CS-selective population of PPTN (<a href="#fig-finr:model" class="quarto-xref">Figure&nbsp;<span>5.1</span></a>). However, when reward is delivered, the preceding activation of the US-selective population inhibits activation in the CS-selective one. During the sensitization phase, a similar self-organizatory mechanism occurs in NAcc: individual rewards become represented by different single neurons.</p>
<p>The second phase of learning concerns conditioning <em>per se</em> with distinct trials for each CS-US association: an initially neutral visual stimulus (CS) activates a distributed representation in IT, which lasts for a fixed duration before the US is delivered. This visual representation projects onto BLA, and, through DA-modulated learning in BLA at reward-delivery, becomes able through repetitive pairing to activate the same BLA cell that would be activated by the US alone. Homeostatic regulation in BLA ensures that the BLA activity at CS onset has the same amplitude as the reward-related activity. CS-related activation in BLA becomes able to activate CE, which becomes able to provoke VTA bursts through excitation of PPTN. This mechanism is sufficient to explain the progressive phasic DA bursts in VTA at CS onset during learning.</p>
<p>In parallel, CS onset activates a bank of oscillators in the ventromedial prefrontal cortex (vmPFC) at different frequencies. During conditioning, the phasic DA burst at US delivery brings the corresponding NAcc cell into the up-state, allowing it to become selective to the precise configuration of cortical oscillators corresponding to the elapsed duration since CS onset. This progressive activation at US delivery diminishes the amplitude of the US-related VTA burst through the direct NAcc <span class="math inline">\rightarrow</span> VTA inhibitory projection. Meanwhile, NAcc learns to inhibit VP at reward delivery, what could potentially lead to the disinhibition of LHb, provoking a dip of activity in VTA through RMTg. However, reward delivery activates the US-selective population of PPTN, which excites VP: the inhibitory influence of NAcc is counterbalanced by PPTN, what leaves VP above its baseline level and avoid unwanted inhibition of VTA.</p>
<p>After a sufficient number of conditioning trials, we investigate reward omission, where the CS is presented for the usual duration, but not the US. In this case, one NAcc cell goes into the up-state when the reward is expected because of its strong vmPFC input at this time and inhibits VP. This inhibition is then not counterbalanced anymore by US-related PPTN activation, so this disinhibits LHb, activates RMTg and finally provokes a strong inhibition of VTA, bringing it below baseline for a certain duration (the dip).</p>
</section>
<section id="computational-principles" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="computational-principles">Computational principles</h4>
<p>Each area in the proposed model is composed of a given number of computational units, where each unit computes the mean activity of a population of neurons. The dynamics of each unit is described by the evolution of its time-dependent firing rate <span class="citation" data-cites="Dayan2001">(<a href="References.html#ref-Dayan2001" role="doc-biblioref">Dayan and Abbott, 2001</a>)</span>. The firing rate <span class="math inline">r(t)</span> of an unit is a positive scalar describing the instantaneous number of spikes per second emitted by neurons in the corresponding population. In this model, it is taken to be the positive part of the so-called <em>membrane potential</em> <span class="math inline">m(t)</span> of the unit, which follows a first order differential equation depending on the firing rate of other units. In this model, the absolute value of the firing rate is usually restricted to the range <span class="math inline">[0, 1]</span> through homeostatic regulation of learning (see for example <a href="#eq-finr:alpha" class="quarto-xref">Equation&nbsp;<span>5.12</span></a>), where 1 represents the maximal instantaneous firing rate that the considered type of cell can have. Typical units in the model are governed by <a href="#eq-finr:basic" class="quarto-xref">Equation&nbsp;<span>5.1</span></a> and <a href="#eq-finr:basic_rate" class="quarto-xref">Equation&nbsp;<span>5.2</span></a>:</p>
<p><span id="eq-finr:basic"><span class="math display">
    \tau \cdot \frac{dm(t)}{dt} + m(t) =  g_{\text{exc}}(t) - g_{\text{inh}}(t) + B +\eta(t)
\tag{5.1}</span></span></p>
<p><span id="eq-finr:basic_rate"><span class="math display">
    r(t) = (m(t))^+
\tag{5.2}</span></span></p>
<p>where <span class="math inline">\tau</span> is the time constant of the cell (expressed in milliseconds), <span class="math inline">B</span> is its baseline activity, <span class="math inline">\eta(t)</span> an additive noise term chosen randomly at each time step from an uniform distribution between -0.1 and 0.1, <span class="math inline">g_{\text{exc}}(t)</span> and <span class="math inline">g_{\text{inh}}(t)</span> being the weighted sum of excitatory and inhibitory afferent firing rates, respectively. <span class="math inline">()^+</span> is the positive function, which only keeps the positive part of the operand and outputs 0 when it is negative. In the rest of this article, we will only describe how the membrane potential <span class="math inline">m(t)</span> of each unit evolves, the corresponding firing rate being always the positive part.</p>
<p>Units in this model can differentially integrate their inputs depending on their assigned type (here <span class="math inline">\text{exc}</span>, <span class="math inline">\text{inh}</span>, <span class="math inline">\text{mod}</span> and <span class="math inline">\text{dopa}</span>). This type corresponds either to the neurotransmitter type (<span class="math inline">\text{exc}</span> and <span class="math inline">\text{mod}</span> represent glutamergic synapses, <span class="math inline">\text{inh}</span> GABAergic ones and <span class="math inline">\text{dopa}</span> represents dopaminergic receptors) or the region of origin (<span class="math inline">\text{exc}</span> and <span class="math inline">\text{mod}</span> connections have both an excitatory effect but arise from different areas and are integrated differently).</p>
<p>For a given type of synapses, the weighted sum of of inputs is defined by <a href="#eq-finr:weightedsum" class="quarto-xref">Equation&nbsp;<span>5.3</span></a>:</p>
<p><span id="eq-finr:weightedsum"><span class="math display">
    g_{\text{type}}(t) = \sum_i^{\text{type}} w_i(t) \cdot r_i(t)
\tag{5.3}</span></span></p>
<p>where <span class="math inline">i</span> is the index of a synapse of this type, <span class="math inline">r_i(t)</span> the firing rate of the presynaptic neuron at time <span class="math inline">t</span> and <span class="math inline">w_i(t)</span> the weight of the connection (or synaptic efficiency).</p>
<p>Some computational principles in this model rely on the conversion of the onset of a tonic input <span class="math inline">x(t)</span> (reward delivery, CS presentation) into a short-term phasic component. For convenience, we define here a function <span class="math inline">\Phi_{\tau, K}(x)</span> allowing this transformation according to <a href="#eq-finr:mean" class="quarto-xref">Equation&nbsp;<span>5.4</span></a> and <a href="#eq-finr:phasicfunction" class="quarto-xref">Equation&nbsp;<span>5.5</span></a>:</p>
<p><span id="eq-finr:mean"><span class="math display">
    \tau \cdot \frac{d\bar{x}(t)}{dt} + \bar{x}(t) = x(t)
\tag{5.4}</span></span></p>
<p><span id="eq-finr:phasicfunction"><span class="math display">
    \Phi_{\tau, k}(x(t)) = (x(t) - k \cdot \bar{x}(t) )^+
\tag{5.5}</span></span></p>
<p><span class="math inline">\bar{x}(t)</span> integrates the input <span class="math inline">x(t)</span> with a time constant <span class="math inline">\tau</span>, while <span class="math inline">\Phi_\tau(x(t))</span> represents the positive part of the difference between <span class="math inline">x(t)</span> and <span class="math inline">\bar{x}(t)</span>. <span class="math inline">k</span> is a parameter controlling which proportion of the input will be kept on the long-term (if <span class="math inline">k=0</span> the tonic component is preserved, if <span class="math inline">k=1</span> <span class="math inline">\phi_{\tau, k}(x(t))</span> will converge towards zero). If <span class="math inline">x(t)</span> is for example an Heaviside function (switching from 0 to 1 at <span class="math inline">t=0</span>), <span class="math inline">\Phi_{\tau, 0}(x(t))</span> will display a localized bump of activation with a maximum at <span class="math inline">t=\tau</span>, as depicted on <a href="#fig-finr:alphafunction" class="quarto-xref">Figure&nbsp;<span>5.2</span></a>.</p>
<div id="fig-finr:alphafunction" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-finr:alphafunction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/finr/02.jpg" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-finr:alphafunction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.2: Temporal profile of the phasic function <span class="math inline">\Phi_{\tau, k}(x)</span> defined by <a href="#eq-finr:phasicfunction" class="quarto-xref">Equation&nbsp;<span>5.5</span></a>}. At <span class="math inline">t=0</span>, the Heaviside input <span class="math inline">x(t)</span> goes from 0 to 1. The temporal profile of five phasic functions <span class="math inline">\Phi_{\tau, k}(x)</span> with <span class="math inline">\tau=50</span> ms and <span class="math inline">k</span> ranging from 0 to 1 is displayed. If <span class="math inline">k=0</span>, the phasifunction is a simple leaky integrator with time constant <span class="math inline">\tau</span>. If <span class="math inline">k=1</span>, the output of the filter is a localized bump peaking at <span class="math inline">t=\tau</span> and converging towards 0.
</figcaption>
</figure>
</div>
<p>Another useful function is the threshold function, which outputs 1 when the input exceeds a threshold <span class="math inline">\Gamma</span>, 0 otherwise (<a href="#eq-finr:threshold" class="quarto-xref">Equation&nbsp;<span>5.6</span></a>):</p>
<p><span id="eq-finr:threshold"><span class="math display">
    \Delta_{\Gamma}(x) = \begin{cases} 0 \qquad \text{if} \quad x &lt; \Gamma \\
                                  1 \qquad \text{otherwise.} \end{cases}
\tag{5.6}</span></span></p>
<p>The learning rules used in the model derive from the Hebbian learning rule. The simplest variant of this learning rule in the model is a thresholded version described in <a href="#eq-finr:hebbian" class="quarto-xref">Equation&nbsp;<span>5.7</span></a>. The evolution over time of the weight <span class="math inline">w_{i,j}(t)</span> of a synapse between the neuron <span class="math inline">i</span> in population <span class="math inline">\text{pre}</span> (presynaptic neuron) and the neuron <span class="math inline">j</span> of population <span class="math inline">\text{post}</span> (postsynaptic neuron) is governed by:</p>
<p><span id="eq-finr:hebbian"><span class="math display">
    \epsilon \cdot \frac{dw_{i, j}(t)}{dt} = (r^i_{\text{pre}}(t) - \theta_{\text{pre}} )^+ \cdot (r^j_{\text{post}}(t) - \theta_{\text{post}} )^+
\tag{5.7}</span></span></p>
<p>where <span class="math inline">r^i_{\text{pre}}(t)</span> and <span class="math inline">r^j_{\text{post}}(t)</span> are the pre- and post-synaptic firing rates, <span class="math inline">\theta_{\text{pre}}</span> and <span class="math inline">\theta_{\text{post}}</span> are fixed thresholds, and <span class="math inline">\epsilon</span> is the learning rate. The thresholds can be adjusted to take baseline firing rates into account and restrict learning to significant deviations from this baseline. Weight values are restricted to the range <span class="math inline">[w_\text{min}, w_\text{max}]</span>, where <span class="math inline">w_\text{min}</span> is usually 0.</p>
<p>Another learning rule used in the model derives from the covariance learning rule <span class="citation" data-cites="Dayan2001 Vitay2010 Schroll2012">(<a href="References.html#ref-Dayan2001" role="doc-biblioref">Dayan and Abbott, 2001</a>; <a href="References.html#ref-Schroll2012" role="doc-biblioref">Schroll et al., 2012</a>; <a href="References.html#ref-Vitay2010" role="doc-biblioref">Vitay and Hamker, 2010</a>)</span>. In this framework, only those cells whose firing rate is significantly above the mean firing rate in their respective population can participate to learning. The evolution over time of the weights is described by <a href="#eq-finr:covariance" class="quarto-xref">Equation&nbsp;<span>5.8</span></a>:</p>
<p><span id="eq-finr:covariance"><span class="math display">
    \epsilon \cdot \frac{dw_{i, j}(t)}{dt} = (r^i_{\text{pre}}(t) - \bar{r}_{\text{pre}}(t) )^+ \cdot (r^j_{\text{post}}(t) - \bar{r}_{\text{post}}(t))^+
\tag{5.8}</span></span></p>
<p>where <span class="math inline">\bar{r}_{\text{pre}}(t)</span> and <span class="math inline">\bar{r}_{\text{post}}(t)</span> are the average firing rate in the pre- and post-synaptic populations, respectively. This mean activity allows to adapt more dynamically the learning behavior between two populations. Dopamine-modulated learning rules will be described in the rest of the text, together with the corresponding populations (BLA and NAcc). The parameters of all learning rules are described in <a href="#tbl-projections" class="quarto-xref">Table&nbsp;<span>5.2</span></a>.</p>
<p>All equations in the model are solved using the forward Euler method, with a time step of 1 ms. The model is implemented in the neurosimulator ANNarchy (Artificial Neural Network architect), which combines a Python interface to a high-performance parallel simulation kernel in C++.</p>
</section>
<section id="sec-finr:inputs" class="level4" data-number="5.2.2.1">
<h4 data-number="5.2.2.1" class="anchored" data-anchor-id="sec-finr:inputs"><span class="header-section-number">5.2.2.1</span> Representation of inputs</h4>
<p>The network is presented with two kinds of inputs: the visual representation of the CS and the gustatory representation of the US. In this article, we will concurrently learn three CS-US associations (CS1+US1, CS2+US2, CS3+US3), with different parameters (magnitude and time interval) in order to show the robustness of the model. Other combinations of magnitude and duration provoke similar results of the model.</p>
<p>The CS are represented by a three-dimensional binary vector, where each element represents the presence (resp. absence) of the corresponding CS with a value of 1 (resp. 0). The US are represented by a four-dimensional vector, where each element represents a single taste component (for example salt, sugar, fat and umami as in <span class="citation" data-cites="Dranias2008">(<a href="References.html#ref-Dranias2008" role="doc-biblioref">Dranias et al., 2008</a>)</span>). As shown in <a href="#tbl-inputs" class="quarto-xref">Table&nbsp;<span>5.1</span></a>, there is an overlap between the different tastes of the US, rendering harder the task to distinguish them. Moreover, each US representation is multiplied by a magnitude, representing the quantity of food delivered. In this article, this magnitude is the same for all tastes composing the US.</p>
<div id="tbl-inputs" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-inputs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;5.1: Definition of the inputs to the model. Each CS-US association is defined by unique CS and US vectors. During conditioning, rewards are presented with a certain magnitude, and after a certain delay after CS onset.
</figcaption>
<div aria-describedby="tbl-inputs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<thead>
<tr class="header">
<th>Number</th>
<th>CS</th>
<th>US</th>
<th>Magnitude</th>
<th>Interval (s)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><span class="math inline">[1, 0, 0]</span></td>
<td><span class="math inline">[1, 1, 0, 0]</span></td>
<td>0.8</td>
<td>2</td>
</tr>
<tr class="even">
<td>2</td>
<td><span class="math inline">[0, 1, 0]</span></td>
<td><span class="math inline">[1, 0, 1, 0]</span></td>
<td>0.5</td>
<td>3</td>
</tr>
<tr class="odd">
<td>3</td>
<td><span class="math inline">[0, 0, 1]</span></td>
<td><span class="math inline">[1, 0, 1, 1]</span></td>
<td>1.0</td>
<td>4</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>A conditioning trial is composed of a first reset interval of 1 second where no input is given to the network (all elements of the CS and US representations are set to 0). At time <span class="math inline">t=1s</span>, the CS representation is set to the corresponding vector. This input is maintained for a given duration, whose value depend on the CS-US association (2 seconds for CS1-US1, 3 seconds for CS2-US2, 4 for CS3-US3). These different interval durations are chosen to show that the network can indeed learn different CS-US intervals without any modification, but different combinations would lead to similar results.</p>
<p>Once the delay is elapsed, the US representation is set for 1 second, with the CS representation maintained. In extinction trials, the US representation is not set. After this duration of one second, all elements of the CS and US representations are reset to 0, and the network can settle for one more second, so the duration of one trial is equal to the interval plus 3 seconds.</p>
<p>The visual input to the model is represented by the population IT, composed of 9 units. The CS representations activate different neurons in IT with a specific one-to-many pattern: one element of the CS vector activates exactly 3 units in IT (called a cluster), without overlap. This activation is excitatory, with a fixed weight value of 1.0 (see <a href="#tbl-projections" class="quarto-xref">Table&nbsp;<span>5.2</span></a> for the weight value of all projections.). Each neuron in IT has a membrane potential governed by <a href="#eq-finr:excitatory" class="quarto-xref">Equation&nbsp;<span>5.9</span></a>, with the firing rate being its positive part (<a href="#eq-finr:basic_rate" class="quarto-xref">Equation&nbsp;<span>5.2</span></a>):</p>
<p><span id="eq-finr:excitatory"><span class="math display">
    \tau \cdot \frac{dm(t)}{dt} +  m(t) = g_{\text{exc}}(t) + \eta(t)
\tag{5.9}</span></span></p>
<p>with <span class="math inline">\tau = 10</span> ms, <span class="math inline">\eta(t)</span> randomly chosen at each time step in <span class="math inline">[-0.1, 0.1]</span> and <span class="math inline">g_{\text{exc}}(t)</span> the input from the CS representation. The gustatory inputs are similarly represented by LH, with a one-to-one projection (one neuron in LH represents one element of the US representation). Thus, neurons in LH are also governed by <a href="#eq-finr:excitatory" class="quarto-xref">Equation&nbsp;<span>5.9</span></a>, with <span class="math inline">\tau = 10</span> ms.</p>
<div id="tbl-projections" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-projections-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;5.2: Parameters of the projections in the model. Pre and Post describe the pre- and post-synaptic populations, respectively. Type denotes the type of the synapses in the projection, as they are differentially integrated by the postsynaptic neurons (exc, inh, mod, dopa). Pattern denotes the projection pattern between the pre- and post-synaptic populations: all-to-all means that all post-synaptic neurons receive connections from all presynaptic neurons; one-to-one means that each postsynaptic neuron receives exactly one connection from the pre-synaptic population, without overlap. one-to-many and many-to-many refer to specific projection patterns for the clusters in IT, please refer to <a href="#sec-finr:inputs" class="quarto-xref"><span>Section 5.2.2.1</span></a> for a description. Eq represents the number of the equation governing plasticity in the projection. Weight describe the initial value for the weight of each synapse (non-learnable connections keep this value through the simulation). <span class="math inline">w_{\text{min}}</span> is the minimal value that a learnable weight can take during learning, while <span class="math inline">w_{\text{max}}</span> is the maximal value (if any). The other parameters correspond to the respective equations of the learning rules, please refer to them for details.
</figcaption>
<div aria-describedby="tbl-projections-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 2%">
<col style="width: 6%">
<col style="width: 12%">
<col style="width: 6%">
<col style="width: 16%">
<col style="width: 5%">
<col style="width: 10%">
<col style="width: 11%">
<col style="width: 2%">
<col style="width: 10%">
<col style="width: 2%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="header">
<th>Pre</th>
<th>Post</th>
<th>Type</th>
<th>Pattern</th>
<th>Eq.</th>
<th>Weight</th>
<th><span class="math inline">[w_{  ext{min}}, w_{\text{max}}]</span></th>
<th><span class="math inline">\epsilon</span></th>
<th><span class="math inline">\theta_{\text{pre}}</span></th>
<th><span class="math inline">\theta_{\text{post}}</span></th>
<th><span class="math inline">K</span></th>
<th><span class="math inline">\tau_{\text{dopa}}</span></th>
<th><span class="math inline">k</span></th>
<th><span class="math inline">\tau_\alpha</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>VIS</td>
<td>IT</td>
<td>exc</td>
<td>one-to-many</td>
<td>-</td>
<td>1.0</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="even">
<td>GUS</td>
<td>LH</td>
<td>exc</td>
<td>one-to-one</td>
<td>-</td>
<td>1.0</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="odd">
<td>LH</td>
<td>BLA</td>
<td>exc</td>
<td>all-to-all</td>
<td><a href="#eq-finr:dacovariance" class="quarto-xref">Equation&nbsp;<span>5.11</span></a></td>
<td><span class="math inline">0.3 \pm 0.2</span></td>
<td><span class="math inline">[0, -]</span></td>
<td>100</td>
<td>-</td>
<td>-</td>
<td>10</td>
<td>100</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>IT</td>
<td>BLA</td>
<td>mod</td>
<td>all-to-all</td>
<td><a href="#eq-finr:dacopy" class="quarto-xref">Equation&nbsp;<span>5.13</span></a></td>
<td>0.0</td>
<td>-</td>
<td>300</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="odd">
<td>BLA</td>
<td>BLA</td>
<td>inh</td>
<td>all-to-all</td>
<td><a href="#eq-finr:covariance" class="quarto-xref">Equation&nbsp;<span>5.8</span></a></td>
<td>0.5</td>
<td><span class="math inline">[0,  3]</span></td>
<td>100</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="even">
<td>BLA</td>
<td>CE</td>
<td>exc</td>
<td>all-to-all</td>
<td>-</td>
<td>1.0</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="odd">
<td>CE</td>
<td>PPTN</td>
<td>exc</td>
<td>all-to-one</td>
<td>-</td>
<td>1.5</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="even">
<td>LH</td>
<td>PPTN</td>
<td>exc</td>
<td>all-to-one</td>
<td>-</td>
<td>0.75</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="odd">
<td>PPTN</td>
<td>PPTN</td>
<td>inh</td>
<td>all-to-all</td>
<td>-</td>
<td>2</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="even">
<td>PPTN</td>
<td>VTA</td>
<td>exc</td>
<td>all-to-all</td>
<td>-</td>
<td>1.5</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="odd">
<td>PPTN</td>
<td>VP</td>
<td>exc</td>
<td>all-to-all</td>
<td>-</td>
<td>0.5</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="even">
<td>VP</td>
<td>RMTg</td>
<td>inh</td>
<td>all-to-all</td>
<td>-</td>
<td>1</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="odd">
<td>VP</td>
<td>LHb</td>
<td>inh</td>
<td>all-to-all</td>
<td>-</td>
<td>3</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="even">
<td>LHb</td>
<td>RMTg</td>
<td>exc</td>
<td>all-to-all</td>
<td>-</td>
<td>1.5</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="odd">
<td>RMTg</td>
<td>VTA</td>
<td>inh</td>
<td>all-to-all</td>
<td>-</td>
<td>1.0</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="even">
<td>IT</td>
<td>vmPFC</td>
<td>exc</td>
<td>many-to-many</td>
<td>-</td>
<td>0.3</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="odd">
<td>vmPFC</td>
<td>NAcc</td>
<td>mod</td>
<td>all-to-all</td>
<td><a href="#eq-finr:dacovariance" class="quarto-xref">Equation&nbsp;<span>5.11</span></a></td>
<td>0</td>
<td><span class="math inline">[-0.2, -]</span></td>
<td>50</td>
<td>-</td>
<td>-</td>
<td>5</td>
<td>10</td>
<td>1</td>
<td>10</td>
</tr>
<tr class="even">
<td>BLA</td>
<td>NAcc</td>
<td>exc</td>
<td>one-to-one</td>
<td>-</td>
<td>0.3</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="odd">
<td>VTA</td>
<td>NAcc</td>
<td>dopa</td>
<td>all-to-all</td>
<td>-</td>
<td>0.5</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="even">
<td>NAcc</td>
<td>NAcc</td>
<td>inh</td>
<td>all-to-all</td>
<td><a href="#eq-finr:covariance" class="quarto-xref">Equation&nbsp;<span>5.8</span></a></td>
<td>0.5</td>
<td><span class="math inline">[0, 1]</span></td>
<td>1000</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="odd">
<td>NAcc</td>
<td>VP</td>
<td>inh</td>
<td>all-to-all</td>
<td><a href="#eq-finr:hebbian" class="quarto-xref">Equation&nbsp;<span>5.7</span></a></td>
<td>0</td>
<td><span class="math inline">[0, 2]</span></td>
<td>100</td>
<td>0</td>
<td>0.5</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr class="even">
<td>NAcc</td>
<td>VTA</td>
<td>inh</td>
<td>all-to-all</td>
<td><a href="#eq-finr:hebbian" class="quarto-xref">Equation&nbsp;<span>5.7</span></a></td>
<td>0</td>
<td><span class="math inline">[0, 2]</span></td>
<td>500</td>
<td>0</td>
<td>0</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="amygdala" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="amygdala">Amygdala</h4>
<p>The amygdala is decomposed into its input structure, BLA, and its output structure, CE. BLA receives visual information from IT, gustatory information from LH and dopaminergic innervation from VTA. Its role is to learn to associate the CS and US representations: a BLA cell which was previously activated by the food reward alone, proportionally to its magnitude <span class="citation" data-cites="Bermudez2010">(<a href="References.html#ref-Bermudez2010" role="doc-biblioref">Bermudez and Schultz, 2010</a>)</span>, should become activated with the same firing rate at CS onset, indicating a transfer of the value of the US to the CS.</p>
<div id="fig-finr:model-detailed" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-finr:model-detailed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/finr/03.jpg" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-finr:model-detailed-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.3: Neural network description of the model. Pointed arrows represent excitatory or dopaminergic synapses, while rounded arrows represent inhibitory synapses. The black curved triangles represent connections from all units of a given population to a single cell. The type of the connection (exc, mod, inh, dopa) is added next to the arrow. Lateral inhibitory connections within BLA and NAcc are only partially represented for simplicity. BLA is composed of 36 units, whose activation is defined by <a href="#eq-finr:bla" class="quarto-xref">Equation&nbsp;<span>5.10</span></a>. Each unit receives excitatory connections from all LH units (<span class="math inline">g_{\text{exc}}(t)</span>), modulated connections from all IT units (<span class="math inline">g_{\text{mod}}(t)</span>), one dopaminergic connection from VTA (<span class="math inline">g_{\text{dopa}}(t)</span>) and inhibitory connections from all other BLA units (<span class="math inline">g_{\text{inh}}(t)</span>). Each of the 3 banks of 50 oscillators in vmPFC receives excitatory connections (<span class="math inline">g_{\text{exc}}(t)</span>) from a specific cluster of 3 units in IT representing a given CS. NAcc is composed of 36 units, whose activation is defined by equation <a href="#eq-finr:nacc" class="quarto-xref">Equation&nbsp;<span>5.16</span></a>. Each unit receives a single excitatory connection from BLA (<span class="math inline">g_{\text{exc}}(t)</span>), excitatory connections from all units of vmPFC (<span class="math inline">g_{\text{mod}}(t)</span>), one dopaminergic connection from VTA (<span class="math inline">g_{\text{dopa}}(t)</span>) and inhibitory connections from all other NAcc units (<span class="math inline">g_{\text{inh}}(t)</span>). The other populations are composed of single units, integrating excitatory or inhibitory inputs.
</figcaption>
</figure>
</div>
<p>As depicted on <a href="#fig-finr:model-detailed" class="quarto-xref">Figure&nbsp;<span>5.3</span></a>, the BLA is composed of 36 units, reciprocally connected with each other through inhibitory connections (inh). Excitatory connections from LH (exc) interact with the excitatory ones from IT (labeled as mod): when no LH activation is present, a neuron can be activated solely by its excitatory inputs from IT; when LH is activated, inputs from IT do not drive the cell response. Such a non-linear interaction between different inputs may be mediated through the somatostatin-containing interneurons in BLA, which are able to suppress excitatory inputs to pyramidal cell distal dendrites (presumably from the cortex), but let them react to the inputs from LH <span class="citation" data-cites="Muller2007">(<a href="References.html#ref-Muller2007" role="doc-biblioref">Muller et al., 2007</a>)</span>. A BLA unit in this model therefore averages the behavior of pyramidal excitatory neurons, somatostatin- and parvalbumin-containing inhibitory interneurons into a single equation.</p>
<p>The membrane potential of each cell is driven by <a href="#eq-finr:bla" class="quarto-xref">Equation&nbsp;<span>5.10</span></a>:</p>
<p><span id="eq-finr:bla"><span class="math display">
    \tau \cdot \frac{dm(t)}{dt} +  m(t) = \Phi_{ \tau_{\text{exc}}, k } ( g_{\text{exc}}(t) ) + ( 1 - \Delta_{\Gamma}(g_{\text{exc}}(t)) ) \cdot \Phi_{\tau_{\text{mod}}, k} ( g_{\text{mod}}(t) ) - g_{\text{inh}}(t) + \eta(t)
\tag{5.10}</span></span></p>
<p>where <span class="math inline">\tau = 10</span> ms is the time constant of the cell, <span class="math inline">\tau_{\text{exc}} = \tau_{\text{mod}} = 500</span> ms are the integration constants for the phasic functions of inputs, <span class="math inline">k=0.8</span> is a parameter ensuring that the cell still responds with a significant firing rate after the phasic component is processed, <span class="math inline">\Gamma = 0.1</span> is a threshold on the excitatory inputs ensuring that modulated inputs from IT can only drive the cell’s activity when the input from LH is absent. The effect of this complex equation will be explained with more details in <a href="#sec-finr:results-amygdala" class="quarto-xref"><span>Section 5.3.1</span></a>.</p>
<p>CE is composed of a single unit, receiving excitatory inputs from all BLA units. Its membrane potential is driven by <a href="#eq-finr:excitatory" class="quarto-xref">Equation&nbsp;<span>5.9</span></a>, with <span class="math inline">\tau = 10</span> ms. As only one unit is active at a time in BLA because of lateral inhibition, CE simply copies activity in BLA, regardless the CS-US association.</p>
<p>Learning occurs in BLA for three types of connections: the excitatory input from LH, the modulated input from IT and the inhibitory lateral connections between the BLA neurons. The learning procedure is composed of two phases: in the sensitization phase, the US are presented alone, without any CS. This allows BLA to learn to represent each US by a single neuron. In the conditioning phase, learning in the LH <span class="math inline">\rightarrow</span> BLA pathway is reduced. This represents the fact that the formation of food reward representations in BLA is a much slower process than the conditioning sessions.</p>
<p>Excitatory connections from LH to BLA are learned with a dopamine-modulated covariance-based learning, with the addition of a homeostatic mechanism to ensure the weights do not increase infinitely. The evolution of these weights is described by <a href="#eq-finr:dacovariance" class="quarto-xref">Equation&nbsp;<span>5.11</span></a>:</p>
<p><span id="eq-finr:dacovariance"><span class="math display">
    \epsilon \cdot \frac{dw_{i, j}(t)}{dt} =  
            K \cdot \Phi_{\tau_{\text{dopa}}, k} (g_{\text{DA}}(t) ) \cdot \text{OR}(r^i_{\text{pre}}(t) - \bar{r}_{\text{pre}}(t), r^j_{\text{post}}(t) - \bar{r}_{\text{post}}(t))
            - \alpha^j(t) \cdot r^j_{\text{post}}(t)^2 \cdot w_{i, j}(t)  
\tag{5.11}</span></span></p>
<p>with <span class="math inline">\epsilon = 100</span> in the sensitization phase and <span class="math inline">10000</span> in the conditioning phase, <span class="math inline">K=10</span>, <span class="math inline">\tau_{\text{dopa}}=100</span> ms, <span class="math inline">k=1</span>. In the first term of the equation, the covariance term is modulated by a value depending on the dopaminergic activity in VTA. This allows DA extracellular levels to influence the induction of LTP in BLA, as experimentally observed <span class="citation" data-cites="Bissiere2003">(<a href="References.html#ref-Bissiere2003" role="doc-biblioref">Bissière et al., 2003</a>)</span>. It is filtered through the phasic function <span class="math inline">\Phi_{\tau_{\text{DA}}, k} (g_{\text{dopa}}(t))</span> with <span class="math inline">k=1</span>, so that DA-mediated learning only takes temporarily place when DA is significantly above its baseline, i.e.&nbsp;during a phasic burst of activation.</p>
<p>This first term also differs from the covariance learning rule described by <a href="#eq-finr:covariance" class="quarto-xref">Equation&nbsp;<span>5.8</span></a>, as it uses a <span class="math inline">\text{OR}(x, y)</span> function, being <span class="math inline">\text{OR}(x, y) = x \cdot y</span> if <span class="math inline">x &gt;0</span> or <span class="math inline">y&gt;0</span> and <span class="math inline">\text{OR}(x, y) = 0</span> if both <span class="math inline">x&lt;0</span> and <span class="math inline">y&lt;0</span>. If both cells are significantly more activated than their respective population, the term is positive and LTP is engaged. If only one cell is significantly active (either pre- or post-synaptic), the term is negative and LTD appears (homo- or hetero-synaptic LTD, respectively). This simple behavior allows to develop a high selectivity for specific patterns in the presynaptic population. In the case where both cells are inactive (<span class="math inline">r^i_{\text{pre}}(t) &lt; \bar{r}_{\text{pre}}(t)</span> and <span class="math inline">r^j_{\text{post}}(t) &lt; \bar{r}_{\text{post}}(t)</span>), the covariance term would be positive but we set it artificially to 0, in order to avoid that silent neurons build up strong connections.</p>
<p>The second term of the learning rule implements a regularization term derived from the Oja learning rule <span class="citation" data-cites="Oja1982">(<a href="References.html#ref-Oja1982" role="doc-biblioref">Oja, 1982</a>)</span> ensuring that the postsynaptic activity does not increase indefinitely during learning <span class="citation" data-cites="Vitay2010 Schroll2012">(<a href="References.html#ref-Schroll2012" role="doc-biblioref">Schroll et al., 2012</a>; <a href="References.html#ref-Vitay2010" role="doc-biblioref">Vitay and Hamker, 2010</a>)</span>. This mechanism implements homeostatic plasticity whose role is to keep neurons in an energetically efficient mode <span class="citation" data-cites="Turrigiano2008">(<a href="References.html#ref-Turrigiano2008" role="doc-biblioref">Turrigiano, 2008</a>)</span>. As formulated in <a href="#eq-finr:alpha" class="quarto-xref">Equation&nbsp;<span>5.12</span></a>, the regularization term <span class="math inline">\alpha(t)</span> becomes positive whenever the postsynaptic neuron fires above a certain threshold, thereby down-scaling the most active connections to this neuron:</p>
<p><span id="eq-finr:alpha"><span class="math display">
    \tau_\alpha \frac{d\alpha^j(t)}{dt} + \alpha^j(t) = (r^j_{\text{post}}(t) - r_{\text{max}}) ^+
\tag{5.12}</span></span></p>
<p><span class="math inline">r_{\text{max}} = 1</span> being the postsynaptic firing rate above which regularization is engaged.</p>
<p>The modulated projection from IT to BLA follows a different learning rule: its principle is that this projection should learn to activate a BLA neuron with the same strength as the corresponding US. Learning is also modulated by dopamine release, as described by <a href="#eq-finr:dacopy" class="quarto-xref">Equation&nbsp;<span>5.13</span></a>:</p>
<p><span id="eq-finr:dacopy"><span class="math display">
    \epsilon \cdot \frac{dw_{i, j}(t)}{dt} =  \Delta_{\Gamma_{\text{dopa}}} (g^j_{\text{dopa}}(t) ) \cdot (r^i_{\text{pre}}(t) - \bar{r}_{\text{pre}}(t) ) \cdot (r^j_{\text{post}}(t) - \bar{r}_{\text{post}}(t)) \cdot (g^j_{\text{exc}}(t) - g^j_{\text{mod}}(t))^+
\tag{5.13}</span></span></p>
<p>with <span class="math inline">\Gamma_{\text{dopa}}=0.3</span> being a threshold on VTA activity. The term <span class="math inline">(g_{\text{exc}}(t) - g_{\text{mod}}(t))^+</span> ensures that the modulated projections stop learning whenever their net effect on a postsynaptic neuron exceeds the one of the excitatory projection from LH during DA bursts.</p>
<p>Lateral inhibitory connections between BLA cells are learned according to the covariance-based learning rule described in <a href="#eq-finr:covariance" class="quarto-xref">Equation&nbsp;<span>5.8</span></a>, forcing competition between the cells and ensuring that only one BLA cell is active for a single stimulus.</p>
</section>
<section id="pedunculopontine-nucleus" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="pedunculopontine-nucleus">Pedunculopontine nucleus</h4>
<p>PPTN is involved in generating phasic DA bursts in VTA for both reward-predicting cues and rewards through direct glutamatergic projections <span class="citation" data-cites="Pan2005">(<a href="References.html#ref-Pan2005" role="doc-biblioref">Pan et al., 2005</a>)</span>. Two different populations of PPTN neurons signal CS- and US-related signals to VTA <span class="citation" data-cites="Kobayashi2007">(<a href="References.html#ref-Kobayashi2007" role="doc-biblioref">Kobayashi and Okada, 2007</a>)</span>. In the model, PPTN is therefore composed of two units, one receiving US information from LH, the other CS information from CE, as depicted on <a href="#fig-finr:model-detailed" class="quarto-xref">Figure&nbsp;<span>5.3</span></a>. These two neurons are moreover inhibiting each other, so that only one is active at a given time. The dynamics of these neurons are described by the same <a href="#eq-finr:pptn" class="quarto-xref">Equation&nbsp;<span>5.14</span></a>, the only difference being the origin of the excitatory information:</p>
<p><span id="eq-finr:pptn"><span class="math display">\tau \cdot \frac{dm(t)}{dt} +  m(t) = \Phi_{ \tau_{\text{exc}}, k } ( g_{\text{exc}}(t) ) - g_{\text{inh}}(t) + \eta(t)
\tag{5.14}</span></span></p>
<p>with <span class="math inline">\tau=10</span> ms, <span class="math inline">\tau_{\text{exc}} = 50</span> ms and <span class="math inline">k=1</span>.</p>
</section>
<section id="ventromedial-prefrontal-cortex" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="ventromedial-prefrontal-cortex">Ventromedial prefrontal cortex</h4>
<p>As in the Striatal-beat frequency model <span class="citation" data-cites="Matell2004">(<a href="References.html#ref-Matell2004" role="doc-biblioref">Matell and Meck, 2004</a>)</span>, we model the cortical inputs to NAcc by a bank of oscillators synchronized at CS onset. Each CS is represented by a group of 50 units oscillating at various frequencies between 2 and 8 Hz. Indeed, enhanced top-down synchrony in the extended theta band has been observed between vmPFC and NAcc during reward anticipation <span class="citation" data-cites="Cohen2012">(<a href="References.html#ref-Cohen2012" role="doc-biblioref">Cohen et al., 2012</a>)</span>.</p>
<p>As three CS are used in the experiments presented in this article, there are three banks of 50 units, each activated by the corresponding cluster in IT. When the sum of excitatory inputs exceeds a given threshold <span class="math inline">T_{\text{start}} = 0.8</span>, the current time <span class="math inline">t</span> of the simulation is stored in the variable <span class="math inline">t_0</span>, and the membrane potential of each unit varies according to the <a href="#eq-finr:vmpfc" class="quarto-xref">Equation&nbsp;<span>5.15</span></a>:</p>
<p><span id="eq-finr:vmpfc"><span class="math display">
    \tau \cdot \frac{dm(t)}{dt} +  m(t) = \frac{1 + sin(2\pi\cdot f \cdot (t-t_0) + \varphi)}{2}
\tag{5.15}</span></span></p>
<p>with <span class="math inline">\tau = 1</span> ms, <span class="math inline">f</span> the frequency of the oscillator randomly chosen at the beginning of the simulation in the range <span class="math inline">[2, 8]</span> (uniform distribution) and <span class="math inline">\varphi</span> the phase of the oscillator randomly chosen in the range <span class="math inline">[0, \pi]</span>. When the excitatory input falls below a threshold <span class="math inline">T_{\text{stop}} = 0.2</span>, the membrane potential is set to 0. Contrary to the rest of the network, this mechanism is not biologically plausible, but it abstracts the behavior of a coupled network of excitatory and inhibitory neurons, all activated by CS onset and interacting with different synaptic strengths and delays.</p>
</section>
<section id="nucleus-accumbens" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="nucleus-accumbens">Nucleus accumbens</h4>
<p>As described by <a href="#fig-finr:model-detailed" class="quarto-xref">Figure&nbsp;<span>5.3</span></a>, NAcc is composed of 36 units, integrating excitatory inputs from BLA with a one-to-one pattern (each NAcc neuron receives a connection from only one neuron in BLA), excitatory inputs from vmPFC (all-to-all), dopaminergic inputs from VTA and lateral inhibitory connections forcing competition between NAcc cells. Their membrane potential can be either in a hyperpolarized <em>down-state</em> or in a depolarized <em>up-state</em>, depending on several factors: 1) spontaneous transition from the down-state to the up-state have been described, exhibiting rhythmic delta-frequency (0.5-2Hz) activities in freely moving rats <span class="citation" data-cites="Leung1993">(<a href="References.html#ref-Leung1993" role="doc-biblioref">Leung and Yim, 1993</a>)</span>; 2) Phasic DA release from VTA can bring NAcc neurons in the up-state <span class="citation" data-cites="Gruber2003 Goto2005">(<a href="References.html#ref-Goto2005" role="doc-biblioref">Goto and Grace, 2005</a>; <a href="References.html#ref-Gruber2003" role="doc-biblioref">Gruber et al., 2003</a>)</span>; 3) Massive input from the prefrontal cortex (together with hippocampal input, not modeled here) can also force this transition <span class="citation" data-cites="McGinty2009">(<a href="References.html#ref-McGinty2009" role="doc-biblioref">McGinty and Grace, 2009</a>)</span>.</p>
<p>Consequently, each unit of NAcc has an additional input variable <span class="math inline">s(t)</span> describing its current state, taking the value <span class="math inline">-0.9</span> in the down-state and <span class="math inline">-0.4</span> in the up-state. Its effect is that the neuron can more easily have a non-zero firing rate in the up-state than in the down-state. The membrane potential of each NAcc cell evolves according to the <a href="#eq-finr:nacc" class="quarto-xref">Equation&nbsp;<span>5.16</span></a>:</p>
<p><span id="eq-finr:nacc"><span class="math display">
    \tau \cdot \frac{dm(t)}{dt} +  m(t) = g_{\text{exc}}(t) - g_{\text{inh}}(t) + g_{\text{dopa}}(t)+ s(t) + \eta(t)
\tag{5.16}</span></span></p>
<p>with <span class="math inline">\tau=10</span> ms. The corresponding firing rate is restricted to the range <span class="math inline">[0, 1.1]</span>. Transitions between the two states are followed by another variable <span class="math inline">s_{\text{time}}(t)</span>, which integrates <span class="math inline">s(t)</span> over time, as described by the <a href="#eq-finr:updown" class="quarto-xref">Equation&nbsp;<span>5.17</span></a>:</p>
<p><span id="eq-finr:updown"><span class="math display">
    \tau \cdot \frac{ds_{\text{time}}(t)}{dt} +  s_{\text{time}}(t) = s(t)
\tag{5.17}</span></span></p>
<p>with <span class="math inline">\tau=450</span> ms. The role of the variable <span class="math inline">s_{\text{time}}(t)</span> is to ensure spontaneous transitions between the up- and down-states in the absence of external inputs or dopaminergic activation. Transitions from the down-state to the up-state are provoked by one of the following events:</p>
<ul>
<li>The activity of VTA exceeds a threshold <span class="math inline">\Gamma_{\text{dopa}} = 0.3</span>;</li>
<li>Excitatory inputs <span class="math inline">g_{\text{exc}}(t)</span> exceed the threshold <span class="math inline">\Gamma_{\text{glut}} = 1</span>;</li>
<li>The variable <span class="math inline">s_{\text{time}}(t)</span> exceeds the threshold <span class="math inline">\Gamma_{\text{up}}  = -0.45</span>.</li>
</ul>
<p>Transitions from the up-state to the down-state are provoked by the combination of these two conditions:</p>
<ul>
<li>The activity of VTA is below the threshold <span class="math inline">\Gamma_{\text{dopa}} = 0.3</span>;</li>
<li>The variable <span class="math inline">s_{\text{time}}(t)</span> is below the threshold <span class="math inline">\Gamma_{\text{down}} = -0.85</span>.</li>
</ul>
<p>The role of the variable <span class="math inline">s_{\text{time}}(t)</span> is therefore to ensure spontaneous transitions from the down-state to the up-state, regardless other inputs. It also ensures that the NAcc cell stays long enough in the up-state before going back to the down-state when the other inputs fade away.</p>
<p>The mechanism proposed to exhibit up- and down-state fluctuations in our model of NAcc is a phenomenological abstraction of the underlying biological components, sufficient to reproduce some of their functional properties. A more detailed modeling approach is needed to better describe and understand the observed patterns in the context of temporal prediction. It could rely on existing biophysically-detailed models of striatal spiny neurons, studying the effects on membrane bistability of slow and fast potassium currents <span class="citation" data-cites="Gruber2003">(<a href="References.html#ref-Gruber2003" role="doc-biblioref">Gruber et al., 2003</a>)</span>, NMDA/AMPA receptors ratio <span class="citation" data-cites="Wolf2005">(<a href="References.html#ref-Wolf2005" role="doc-biblioref">Wolf et al., 2005</a>)</span> or D1-receptor activation <span class="citation" data-cites="Humphries2009">(<a href="References.html#ref-Humphries2009" role="doc-biblioref">Humphries et al., 2009</a>)</span>, for example.</p>
<p>Excitatory inputs from vmPFC are learned using the same dopamine-modulated learning rule as the LH <span class="math inline">\rightarrow</span> BLA projection, described by <a href="#eq-finr:dacovariance" class="quarto-xref">Equation&nbsp;<span>5.11</span></a> and <a href="#eq-finr:alpha" class="quarto-xref">Equation&nbsp;<span>5.12</span></a>, with <span class="math inline">\epsilon=50</span>, <span class="math inline">K=5</span>, <span class="math inline">\tau_{\text{dopa}}=10</span> ms, <span class="math inline">k=1</span>, <span class="math inline">\tau_{\alpha}=10</span> ms and <span class="math inline">r_{\text{max}} = 1</span>. This three-factors rule covers some known effects of dopamine on corticostriatal learning <span class="citation" data-cites="Reynolds2002 Calabresi2007 Shen2008">(<a href="References.html#ref-Calabresi2007" role="doc-biblioref">Calabresi et al., 2007</a>; <a href="References.html#ref-Reynolds2002" role="doc-biblioref">Reynolds and Wickens, 2002</a>; <a href="References.html#ref-Shen2008" role="doc-biblioref">Shen et al., 2008</a>)</span>: phasic DA release potentiates learning; LTP requires both DA release, presynaptic activity and postsynaptic depolarization; strong presynaptic activation when the postsynaptic cell is in the down-state leads to LTD. The third condition of the learning rule, called heterosynaptic LTD where only the post-synaptic cell is active but not the pre-synaptic one, has not been observed in the striatum but in the hippocampus <span class="citation" data-cites="Doyere1997">(<a href="References.html#ref-Doyere1997" role="doc-biblioref">Doyere et al., 1997</a>)</span>. However, low-frequency stimulation at 1 Hz engage LTD at corticostriatal synapses <span class="citation" data-cites="Fino2005">(<a href="References.html#ref-Fino2005" role="doc-biblioref">Fino et al., 2005</a>)</span>, so such a mechanism can not be ruled outgnote. The known influence of dopamine depletion on corticostriatal learning is not used in this model.</p>
<p><span class="math inline">\tau_{\alpha}</span> is set very low, restricting learning to the early phase of the dopaminergic burst of VTA activity. The weights between vmPFC and NAcc are allowed to become negative (<span class="math inline">w_{\text{min}}=-0.2</span>) to reflect the role of accumbal interneurons (TANs and GABAergic) in timing processes <span class="citation" data-cites="Apicella2009 Coull2011">(<a href="References.html#ref-Apicella2009" role="doc-biblioref">Apicella et al., 2009</a>; <a href="References.html#ref-Coull2011" role="doc-biblioref">Coull et al., 2011</a>)</span>. This particularity is essential for the adequate temporal response of NAcc neurons. Inhibitory lateral connections between NAcc cells are learned according to the covariance-based learning rule described by <a href="#eq-finr:covariance" class="quarto-xref">Equation&nbsp;<span>5.8</span></a>.</p>
</section>
<section id="ventral-pallidum" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="ventral-pallidum">Ventral Pallidum</h4>
<p>During classical conditioning, VP cells are excited by large rewards and the cues predicting them, but are inhibited by small rewards <span class="citation" data-cites="Tindell2004">(<a href="References.html#ref-Tindell2004" role="doc-biblioref">Tindell et al., 2004</a>)</span>. While the major source of inhibition is clearly NAcc, the source of excitation is still unknown. Based on known anatomical connections, we hypothesize that this phasic excitation is transmitted by PPTN <span class="citation" data-cites="Hallanger1988">(<a href="References.html#ref-Hallanger1988" role="doc-biblioref">Hallanger and Wainer, 1988</a>)</span>. However, when a reward is fully predicted and delivered, NAcc is activated and cancels the excitation provided by PPTN. We propose a mechanism where VP is inhibited by NAcc activation unless excitatory inputs from PPTN are present. This shunting mechanism is described by <a href="#eq-finr:vp" class="quarto-xref">Equation&nbsp;<span>5.18</span></a> governing the membrane potential of the single unit in VP:</p>
<p><span id="eq-finr:vp"><span class="math display">
    \tau \cdot \frac{dm(t)}{dt} +  m(t) = g_{\text{exc}}(t) - \Delta_{\Gamma} (g_{\text{exc}}(t)) \cdot g_{\text{inh}}(t) + B + \eta(t)
\tag{5.18}</span></span></p>
<p>where <span class="math inline">\tau=10</span> ms, <span class="math inline">B=0.5</span> is the baseline activity of the VP neuron and <span class="math inline">\Gamma=0.1</span> is a threshold on excitatory inputs. The inhibitory projection from NAcc is learned according to the thresholded Hebbian learning rule described by the <a href="#eq-finr:hebbian" class="quarto-xref">Equation&nbsp;<span>5.7</span></a>.</p>
</section>
<section id="lateral-habenula" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="lateral-habenula">Lateral Habenula</h4>
<p>LHb is activated by aversive stimuli and reward omission <span class="citation" data-cites="Hikosaka2008 Hong2011">(<a href="References.html#ref-Hikosaka2008" role="doc-biblioref">Hikosaka et al., 2008</a>; <a href="References.html#ref-Hong2011" role="doc-biblioref">Hong et al., 2011</a>)</span>. In this model, signaling of reward omission is provoked by disinhibition from VP: when VP is inhibited by NAcc at the expected time of reward delivery, it stops inhibiting LHb and allows it to fire. As the source of excitatory inputs to LHb is still not clear, we simply consider in this model that the single LHb cell has a very high baseline activity, which is normally cancelled by the tonic inhibition of VP, as expressed by <a href="#eq-finr:lhb" class="quarto-xref">Equation&nbsp;<span>5.19</span></a>:</p>
<p><span id="eq-finr:lhb"><span class="math display">
    \tau \cdot \frac{dm(t)}{dt} +  m(t) = - g_{\text{inh}}(t) + B + \eta(t)
\tag{5.19}</span></span></p>
<p>with <span class="math inline">\tau=10</span> ms and <span class="math inline">B=1</span>.</p>
</section>
<section id="rostromedial-tegmental-nucleus" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="rostromedial-tegmental-nucleus">Rostromedial tegmental nucleus</h4>
<p>While most RMTg neurons are activated by aversive events, some also respond to reward omission. They are inhibited by rewards and reward-predicting stimuli <span class="citation" data-cites="Jhou2009">(<a href="References.html#ref-Jhou2009" role="doc-biblioref">Jhou et al., 2009</a>)</span>. The excitation at reward omission has been shown to come from LHb glutamatergic inputs <span class="citation" data-cites="Balcita2011 Hong2011">(<a href="References.html#ref-Balcita2011" role="doc-biblioref">Balcita-Pedicino et al., 2011</a>; <a href="References.html#ref-Hong2011" role="doc-biblioref">Hong et al., 2011</a>)</span>. In this model, the single unit of RMTg is under the tonic inhibition from VP <span class="citation" data-cites="Jhou2009">(<a href="References.html#ref-Jhou2009" role="doc-biblioref">Jhou et al., 2009</a>)</span>, and can become activated when excitatory inputs from LHb are present, as formulated by the <a href="#eq-finr:rmtg" class="quarto-xref">Equation&nbsp;<span>5.20</span></a>:</p>
<p><span id="eq-finr:rmtg"><span class="math display">
    \tau \cdot \frac{dm(t)}{dt} +  m(t) = g_{\text{exc}}(t) - g_{\text{inh}}(t) + \eta(t)
\tag{5.20}</span></span></p>
<p>with <span class="math inline">\tau= 10</span> ms.</p>
</section>
<section id="ventral-tegmental-area" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="ventral-tegmental-area">Ventral tegmental area</h4>
<p>The final stage of the model is a single dopaminergic unit in VTA. It receives excitatory inputs from PPTN, inhibitory inputs from RMTg and modulatory inhibitory inputs from NAcc. The excitatory inputs can progressively be canceled by the modulatory inputs, as the US becomes temporally predictable by NAcc. Additionally, RMTg inputs can provoke a prolonged inhibition of the VTA cell below baseline if no reward is present. This is reflected by the <a href="#eq-finr:vta" class="quarto-xref">Equation&nbsp;<span>5.21</span></a>:</p>
<p><span id="eq-finr:vta"><span class="math display">
    \tau \cdot \frac{dm(t)}{dt} +  m(t) = g_{\text{exc}}(t) * (1- \Phi_{\tau_{\text{mod}}, k}(g_{\text{mod}}(t)))- (1- \Delta_{\Gamma}(g_{\text{exc}}(t))) \cdot \Phi_{\tau_{\text{inh}}, k}(g_{\text{inh}}(t)) + B + \eta(t)
\tag{5.21}</span></span></p>
<p>with <span class="math inline">\tau=10</span> ms, <span class="math inline">\tau_{\text{mod}}=300</span> ms, <span class="math inline">k=1</span>, <span class="math inline">\Gamma=0.1</span>, <span class="math inline">\tau_{\text{inh}}=30</span> ms and <span class="math inline">B=0.2</span>. Modulatory inputs from NAcc are learned according to the learning rule defined in <a href="#eq-finr:hebbian" class="quarto-xref">Equation&nbsp;<span>5.7</span></a>}.</p>
</section>
</section>
</section>
<section id="results" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="results"><span class="header-section-number">5.3</span> Results</h2>
<p>Most experiments in this section concern the concurrent learning of the three CS-US associations described in <a href="#tbl-inputs" class="quarto-xref">Table&nbsp;<span>5.1</span></a>. The learning procedure is split into two phases: the sensitization phase, where each US is presented alone for 10 trials, and the conditioning phase, where the CS and US are presented together for 15 trials. The three CS-US associations are intermingled in ascending order for simplicity, but a randomized order would not change the results. The organization of each trial is described in <a href="#sec-finr:inputs" class="quarto-xref"><span>Section 5.2.2.1</span></a>.</p>
<section id="sec-finr:results-amygdala" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1" class="anchored" data-anchor-id="sec-finr:results-amygdala"><span class="header-section-number">5.3.1</span> CS-US associations in the amygdala</h3>
<p><a href="#fig-finr:bla_conditioning" class="quarto-xref">Figure&nbsp;<span>5.4</span></a> shows the firing rate of single BLA cells during the first (top row) and fifteenth (bottom row) trials of the conditioning phase, for each of the three CS-US associations. After the sensitization phase, only one cell in BLA is selective for each US because of the increased competition induced by antihebbian learning in the lateral connections within BLA. The activity of these US-specific neurons only is displayed, the other cells having a firing rate close to 0.</p>
<div id="fig-finr:bla_conditioning" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-finr:bla_conditioning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/finr/04.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-finr:bla_conditioning-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.4: Timecourse of the activity of different BLA cells before and after conditioning. Activities for the CS1-US1, CS2-US2 and CS3-US3 associations are represented from left to right in panels , and , respectively. For each figure, the horizontal blue line represents the presentation of the CS, while the red line represents the presentation of the US. The top row shows the evolution of the firing rate of a single BLA neuron over time during the first trial of conditioning. Because of the sensitization phase and the lateral inhibition in BLA, there is only one cell in the population which represents each US. During the first trial, this cell gets maximally activated at the time of reward delivery (3, 4 and 5 seconds after the start of the trial, respectively), and its firing rate decreases because of the adaptation of excitatory inputs in BLA, before returning to baseline when the US is removed after 1 second. All other cells in BLA are not activated. The bottom row shows the activity of the same cells during the fifteenth trial of conditioning. They now show an increase of activity when the CS appears (1 second after the start of the trial), reaching a maximum of similar amplitude as the response evoked by the US, and slowly decreasing to a baseline of about 20% of this maximal activity. When the reward is delivered, they increase their firing rate similarly a in the first trial.
</figcaption>
</figure>
</div>
<p>During the first conditioning trial, each BLA cell is activated only at reward delivery, with an amplitude proportional to the magnitude of the US. It reaches a peak shortly after US onset and slowly decreases to a small baseline because of the phasic integration of LH inputs described in <a href="#eq-finr:bla" class="quarto-xref">Equation&nbsp;<span>5.10</span></a>. During the late conditioning trial, the same cells are activated by the onset of the corresponding CS. Their firing rate also reaches a peak shortly after CS onset, with a magnitude proportional to the reward magnitude (see <a href="#sec-finr:rewardmagnitude" class="quarto-xref"><span>Section 5.3.4</span></a> for further discussion) and slowly decays to around 20% of their peak amplitude, due to the temporal integration of IT inputs in <a href="#eq-finr:bla" class="quarto-xref">Equation&nbsp;<span>5.10</span></a>. However, these cells are still phasically excited by the delivery of the predicted reward.</p>
<p>This behavior of single BLA cells during conditioning is in agreement with the known dependency of BLA activity on reward magnitude <span class="citation" data-cites="Bermudez2010">(<a href="References.html#ref-Bermudez2010" role="doc-biblioref">Bermudez and Schultz, 2010</a>)</span> as well as with the observed firing rate of individual BLA neurons for both CS and US <span class="citation" data-cites="Ono1995 Maren2004">(<a href="References.html#ref-Maren2004" role="doc-biblioref">Maren and Quirk, 2004</a>; <a href="References.html#ref-Ono1995" role="doc-biblioref">Ono et al., 1995</a>)</span>. As CE simply sums up BLA activity in our model, the response profile in CE is similar during conditioning, although not specific to the CS-US association. This means that the CE <span class="math inline">\rightarrow</span> PPTN <span class="math inline">\rightarrow</span> VTA pathway is able to signal the onset of specific reward-predicting cues to VTA and generate the corresponding phasic burst, as observed experimentally <span class="citation" data-cites="Lokwan1999 Fudge2000">(<a href="References.html#ref-Fudge2000" role="doc-biblioref">Fudge and Haber, 2000</a>; <a href="References.html#ref-Lokwan1999" role="doc-biblioref">Lokwan et al., 1999</a>)</span>.</p>
</section>
<section id="sec-finr:results-vta" class="level3" data-number="5.3.2">
<h3 data-number="5.3.2" class="anchored" data-anchor-id="sec-finr:results-vta"><span class="header-section-number">5.3.2</span> Timecourse of activity in VTA</h3>
<div id="fig-finr:vta_timecourse" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-finr:vta_timecourse-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/finr/05.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-finr:vta_timecourse-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.5: Timecourse of the activity of the VTA cell during conditioning. The activity for the three CS-US associations is displayed from left to right in panels , and , respectively. For each figure, the horizontal blue line represents the presentation of the CS, while the red line represents the presentation of the US. The first row represents the activity of VTA during the first trial of conditioning, the second row during the fifth trial, the third during the fifteenth trial. They show a progressive reduction of the amplitude of the US-related burst, while the CS-related burst appears early in learning. The fourth row shows the activity of the VTA cell when the reward is delivered one second earlier than previously associated. It shows that the VTA cell responds to rewards delivered earlier with the same activation as for unpredicted rewards. The fifth row shows omission trials: the CS is presented normally, but the US is omitted. The VTA cell shows a phasic pause in firing at the time when reward was expected.
</figcaption>
</figure>
</div>
<p><a href="#fig-finr:vta_timecourse" class="quarto-xref">Figure&nbsp;<span>5.5</span></a> shows the temporal evolution of VTA activity during several conditioning trials for the three CS-US associations. The first row shows its activity during the first conditioning trial. As expected, the VTA cell only fires transiently at reward delivery, with an amplitude proportional to the reward magnitude. This phasic excitation is provoked by the LH <span class="math inline">\rightarrow</span> PPTN <span class="math inline">\rightarrow</span> VTA pathway.</p>
<p>The second and third rows show VTA activity during the fifth and fifteenth conditioning trials for each association. The DA cell shows very early in learning a phasic burst of activity at CS onset. In parallel, the amplitude of the US-related burst progressively decreses until an almost complete cancellation at the fifteenth trial. This pattern of evolution is in accordance of the observations of <span class="citation" data-cites="Pan2005a">Pan and Hyland (<a href="References.html#ref-Pan2005a" role="doc-biblioref">2005</a>)</span> showing that the CS- and US-related bursts of DA activation coexist in the early phases of training. Simple disconnection experiments show that the CS-related phasic bursts are dependent on the CE <span class="math inline">\rightarrow</span> PPTN <span class="math inline">\rightarrow</span> VTA pathway, while the cancellation of the US-related bursts is dependent on the modulatory projection from NAcc to VTA.</p>
<p>After 15 conditioning trials for each association have been executed, two additional trials are performed to test the functional properties of the model. The first additional trial (fourth row of <a href="#fig-finr:vta_timecourse" class="quarto-xref">Figure&nbsp;<span>5.5</span></a>) consists in early delivery of reward: the US previously paired with the CS is presented one second earlier than usual (i.e.&nbsp;1s after CS onset instead of 2s for the CS1-US1 association, 2s for CS2-US2 and 3s for CS3-US3). The CS presentation stops with the end of the US. In this case the VTA cell reacts phasically to reward delivery with the same amplitude as for an unpredicted reward, instead of the diminished burst observed when the reward is presented at the expected time. This is in accordance with the experimental findings of <span class="citation" data-cites="Hollerman1998">Hollerman and Schultz (<a href="References.html#ref-Hollerman1998" role="doc-biblioref">1998</a>)</span>.</p>
<p>In the second type of additional trial (fifth row of <a href="#fig-finr:vta_timecourse" class="quarto-xref">Figure&nbsp;<span>5.5</span></a>), each CS is presented normally but the US is omitted. Shortly after the expected delivery time (around 50ms), the VTA cell receives a strong phasic inhibition bringing its firing rate to 0 for a prolonged period of time. This activation dip is provoked by the NAcc <span class="math inline">\rightarrow</span> VP <span class="math inline">\rightarrow</span> LHb <span class="math inline">\rightarrow</span> RMTg <span class="math inline">\rightarrow</span> VTA pathway. This behavior is in accordance with the reward-prediction error interpretation of VTA activity during conditioning <span class="citation" data-cites="Schultz1997 Fiorillo2003">(<a href="References.html#ref-Fiorillo2003" role="doc-biblioref">Fiorillo et al., 2003</a>; <a href="References.html#ref-Schultz1997" role="doc-biblioref">Schultz et al., 1997</a>)</span>.</p>
</section>
<section id="evolution-of-vta-activity-during-conditioning" class="level3" data-number="5.3.3">
<h3 data-number="5.3.3" class="anchored" data-anchor-id="evolution-of-vta-activity-during-conditioning"><span class="header-section-number">5.3.3</span> Evolution of VTA activity during conditioning</h3>
<div id="fig-finr:vta_evolution" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-finr:vta_evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/finr/06.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-finr:vta_evolution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.6: Evolution of the maximal activity in VTA during conditioning. For each of the three associations (panels , and , respectively), the maximal activity of the VTA cell at CS onset (in blue) and at reward delivery (in red) is plotted for each trial of the conditioning phase. These values are computed by taking the maximum value of the firing rate of the VTA cell in a small time window (<span class="math inline">\pm</span> 100ms) around CS onset and reward delivery. The panels show the relative speed at which the CS-related bursts appear and the one at which the US-related bursts are canceled.
</figcaption>
</figure>
</div>
<p>In this section, we take a closer look at the evolution of phasic activities in VTA during the conditioning process. <a href="#fig-finr:vta_evolution" class="quarto-xref">Figure&nbsp;<span>5.6</span></a> shows the evolution of US- and CS-related activation in BLA over the 15 conditioning trials, for each of the three associations. The amplitude of the CS-related (in blue) and US-related (in red) bursts is computed by taking the maximal firing rate of the VTA cell in a small time window (<span class="math inline">\pm</span> 100ms) around CS and US onsets, respectively.</p>
<p>Panels <strong>(A)</strong> and <strong>(C)</strong> (corresponding to rewards of magnitude 0.8 and 1.0, respectively) show that the CS-related bursts, initially nonexistent as the baseline activity of VTA is 0.2, quickly rise in a few trials to reach up a limit dependent on the reward magnitude. The US-related bursts show the opposite pattern: the amplitude is initially dependent on the reward magnitude, but is progressively decreases to a value close to the VTA baseline. One can observe that the cancellation is not total, the maximal value of US-related bursts being between 0.3 and 0.4, while the baseline activity is 0.2. However the duration of the phasic is also reduced from approximately 200ms for unpredicted rewards to 50ms for fully predicted rewards, so the total amount of dopamine released can be considered relatively low. This aspect will be discussed in <a href="#sec-finr:discussion-bio" class="quarto-xref"><span>Section 5.4.2</span></a>.</p>
<p>Panel <strong>(B)</strong>, corresponding to a reward magnitude of 0.5, shows a different behavior. While the CS-related burst still increases to reach a maximum equal to the initial US-related burst (although more slowly), the cancellation of the US is both slower and not total. This suggests that reward magnitude influences conditioned responses in VTA in a non-linear manner. This will be further investigated in the following section. Altogether, the results show that the cancellation of the US-related VTA activation happens well after the appearance of CS-related bursts, what is consistent with the experimental data <span class="citation" data-cites="Pan2005a">(<a href="References.html#ref-Pan2005a" role="doc-biblioref">Pan and Hyland, 2005</a>)</span>.</p>
</section>
<section id="sec-finr:rewardmagnitude" class="level3" data-number="5.3.4">
<h3 data-number="5.3.4" class="anchored" data-anchor-id="sec-finr:rewardmagnitude"><span class="header-section-number">5.3.4</span> Influence of reward magnitude on conditioning</h3>
<div id="fig-finr:us_magnitude" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-finr:us_magnitude-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/finr/07.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-finr:us_magnitude-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.7: Dependency of the activity in BLA and VTA on reward magnitude. Panel shows the maximal firing rate in BLA around CS-onset and reward delivery during the first and last trial of conditioning, for different reward magnitudes. For each value of the reward magnitude, the CS1-US1 association is presented 15 times, and the maximal activity in BLA around CS-onset (between 900 and 1100ms after the start of each trial) and reward delivery (between 3900 and 4100ms after the start of the trial) is recorded. The experiment is repeated 10 times (without different initial values), and the mean (solid line) and standard deviation (colored area) of these measurements are plotted. The blue dotted line shows the maximal activity at CS-onset during the first trial, which does not depend on reward magnitude, as no learning has taken place yet. The red dotted line shows the maximal activity at reward delivery during the first trial, which is proportional to the reward magnitude because of learning in the LH <span class="math inline">\rightarrow</span> BLA projection during the sensitization phase. For the last trial of conditioning, the blue and red solid lines show the dependency on reward magnitude of the maximal activity in BLA at CS onset and reward delivery, respectively. While the US-related response is proportional to the reward, the CS-related activity only appears for reward magnitudes bigger than 0.1. Panel shows the dependency on reward magnitude of the VTA bursts in the same conditions (blue dotted = CS onset at trial 1, red dotted = US delivery at trial 1, blue solid = CS onset at trial 15, red solid = US delivery at trial 15). While there are no CS-related bursts during trial 1, the US-related burst is proportional to reward magnitude. A similar relationship can be observed for the CS-related burst at the end of learning. However the US-related burst after learning shows a different pattern: small rewards (magnitude smaller than 0.4) elicit burst proportionally to their magnitude, but bigger rewards elicit strongly attenuated bursts, showing that the cancellation of US-related bursts is dependent on reward magnitude.
</figcaption>
</figure>
</div>
<p>In order to study the influence of reward magnitude on VTA activity, we modified the conditioning procedure. In this section, only one CS-US association (CS1-US1, with an interval of 2 seconds between the CS and US) is learned by the network, but the reward magnitude is varied linearly between 0 and 1 instead of the previous value 0.8. For each value of the reward magnitude, a different network performs the sensitization and conditioning tasks for this particular association. Activities in BLA and VTA are recorded during the first and fifteenth conditioning trials, and the maximal activity of VTA and BLA cells at CS and US onsets (computed within a time window of <span class="math inline">\pm</span> 100 ms) is shown on <a href="#fig-finr:us_magnitude" class="quarto-xref">Figure&nbsp;<span>5.7</span></a>, averaged for 10 different networks. <a href="#fig-finr:us_magnitude" class="quarto-xref">Figure&nbsp;<span>5.7</span></a> A shows the dependency of US- and CS-related activation in BLA on reward magnitude, while <a href="#fig-finr:us_magnitude" class="quarto-xref">Figure&nbsp;<span>5.7</span></a> B shows the reward-magnitude dependency of VTA bursts.</p>
<p>During the first trial of conditioning, there is logically no CS-related activity in BLA and VTA (blue dotted line), regardless the reward magnitude, as conditioning has not taken place yet. The US-related activity (red dotted line) shows a linear dependency on reward magnitude in both VTA and BLA. This is explained by the linear encoding of reward magnitude in LH: a more precise model of food-related activation in LH may change this property.</p>
<p>During the last trial of conditioning, the CS elicits strong phasic activity in both BLA and VTA (blue solid line), which is roughly proportional to the reward magnitude: additive noise plays an important role in the learning dynamics of the model, what explains that different networks may exhibit slightly different results. This is in accordance with the observation that CS-elicited DA bursts increase monotonically with the magnitude of expected rewards <span class="citation" data-cites="Tobler2005">(<a href="References.html#ref-Tobler2005" role="doc-biblioref">Tobler et al., 2005</a>)</span>.</p>
<p>The situation is more contrasted regarding the US-related activation after conditioning (red solid line): while BLA still phasically responds linearly to the US magnitude (see also <a href="#fig-finr:bla_conditioning" class="quarto-xref">Figure&nbsp;<span>5.4</span></a>), the cancellation of reward-delivery bursts in VTA only occurs if the reward magnitude is high enough (above 0.4). This cancellation is dependent on learning in NAcc, which is itself dependent on DA release by VTA. Small rewards do not provoke sufficiently high VTA bursts to modulate striatal processing and learning. While there is no direct evidence of such an effect of reward magnitude on US cancellation, this effect is in agreement with the known influence of reinforcer magnitude on the emergence of conditioned responding <span class="citation" data-cites="Morris2006">(<a href="References.html#ref-Morris2006" role="doc-biblioref">Morris and Bouton, 2006</a>)</span> or peak-interval tasks <span class="citation" data-cites="Ludvig2007">(<a href="References.html#ref-Ludvig2007" role="doc-biblioref">Ludvig et al., 2007</a>)</span>, which are dependent on learning in the striatum.</p>
</section>
<section id="timing-mechanism-in-nacc" class="level3" data-number="5.3.5">
<h3 data-number="5.3.5" class="anchored" data-anchor-id="timing-mechanism-in-nacc"><span class="header-section-number">5.3.5</span> Timing mechanism in NAcc</h3>
<div id="fig-finr:nacc_omission" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-finr:nacc_omission-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/finr/08.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-finr:nacc_omission-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.8: Timecourse of the internal variables of a single NAcc neuron during a reward omission trial. After the conditioning phase, CS2 is presented alone. The NAcc neuron which was selective for US2 during conditioning is recorded: its membrane potential <span class="math inline">m(t)</span> in red, the weighted sum of excitatory inputs from vmPFC in blue and its up- or down-state <span class="math inline">s(t)</span> in green. The firing rate of the neuron is the positive part of the membrane potential: the firing rate becomes only non-zero shortly at the time where reward is expected but omitted.
</figcaption>
</figure>
</div>
<p>An important functional aspect of the model is the inducement of dips in VTA when a predicted reward is omitted. It relies on the ability of specific NAcc cells to learn the CS-US interval duration based on inputs from the synchronized oscillators in vmPFC, gated by the dopaminergic bursts of VTA.</p>
<p><a href="#fig-finr:nacc_omission" class="quarto-xref">Figure&nbsp;<span>5.8</span></a> shows the evolution of several internal variables of one NAcc cell during reward omission. This cell is selective for the US2 because of the corresponding input from BLA. After successful learning of the CS2-US2 association (15 trials), CS2 is presented alone while we record the temporal evolution of 1) the membrane potential of this cell (governed by <a href="#eq-finr:nacc" class="quarto-xref">Equation&nbsp;<span>5.16</span></a>, red line), 2) the weighted sum of excitatory inputs from vmPFC (blue line) and 3) its up- or down-state <span class="math inline">s(t)</span> (green line). For simplicity, its firing rate is not depicted, as it is only the positive part of the membrane potential.</p>
<p>When the CS appears one second after the start of the trial, the CS-evoked VTA burst brings the cell into the up-state, while the cortical oscillators start influencing the membrane potential. However, this excitation is not sufficient to bring the membrane potential above the threshold and activate the cell. During the delay period, the cell switches between down- and up-states based on the internal dynamics of the variable <span class="math inline">s_{\text{time}}(t)</span> (<a href="#eq-finr:updown" class="quarto-xref">Equation&nbsp;<span>5.17</span></a>). The sum of inputs from vmPFC oscillate during this period, but is never strong enough to activate the cell. However, at the time when the US is expected (4 seconds after the beginning of the trial), these inputs become able to bring the cell into the up-state, what results in a membrane potential well above threshold and provokes a short burst of the firing rate, although the US is not delivered.</p>
<p>This mechanism is very similar to the Striatal-Beat Frequency model proposed by <span class="citation" data-cites="Matell2004">Matell and Meck (<a href="References.html#ref-Matell2004" role="doc-biblioref">2004</a>)</span>, although based on a different implementation (different number of cortical oscillators, different frequency range and different learning rule). The weighted sum of cortical inputs, which peaks for the cortical pattern describing the learned interval, fluctuates a lot during the delay period. In particular, there are several peaks during the delay period corresponding to different harmonics (<span class="math inline">\frac{1}{2}, \frac{1}{3}...</span>). As suggested in <span class="citation" data-cites="Matell2004">(<a href="References.html#ref-Matell2004" role="doc-biblioref">Matell and Meck, 2004</a>)</span>, the up- and down-states are necessary to avoid spurious activation of NAcc during this period, what would lead to unwanted VTA dips, especially at the beginning of learning. In the early conditioning trials, the vmPFC input is too weak to bring the NAcc cell into the up-state, which is only dependent on phasic DA bursts at reward delivery. As in the Striatal-Beat Frequency, we do not precisely model how the cortical oscillators could be synchronized at CS onset: it is a simple threshold on visual inputs from IT. A more detailed model is necessary to generate these oscillations, perhaps through the opening of a vmPFC <span class="math inline">\rightarrow</span> ventral BG <span class="math inline">\rightarrow</span> medial thalamus <span class="math inline">\rightarrow</span> vmPFC loop, gated by the VTA burst at CS onset.</p>
</section>
<section id="acquisition-rate-of-temporal-prediction" class="level3" data-number="5.3.6">
<h3 data-number="5.3.6" class="anchored" data-anchor-id="acquisition-rate-of-temporal-prediction"><span class="header-section-number">5.3.6</span> Acquisition rate of temporal prediction</h3>
<div id="fig-finr:vta_dips" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-finr:vta_dips-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/finr/09.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-finr:vta_dips-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.9: Apparition of VTA dips during conditioning. For the three CS-US associations (panels , and , respectively), the panel represents what would happen in VTA (red) and NAcc (blue) if the reward were omitted directly after each conditioning trial. Learning is shut off during these omission trials. The red line shows the minimal activity in VTA during these omission trials. After the first few conditioning trials, this minimal activity is around the baseline (0.2), but quickly becomes equal to 0, denoting the appearance of the strong phasic inhibition of VTA at reward omission. The blue line shows the emergence of activity in NAcc at reward omission. The speed at which the timing prediction appears in the ventral BG depends on reward magnitude.
</figcaption>
</figure>
</div>
<p>In order to study the speed at which the CS-US interval is learned in NAcc, we designed a different conditioning schedule. After sensitization to the three US, the 15 conditioning trials per association are alternated with omission trials, i.e.&nbsp;each CS-US trial is immediately followed by the CS alone. All learning rules are disabled during these omission trials, as we only want to use the CS as a probe to measure the acquisition rate: we want to study what would happen if the reward were omitted earlier in the conditioning process.</p>
<p><a href="#fig-finr:vta_dips" class="quarto-xref">Figure&nbsp;<span>5.9</span></a> shows the maximal activity in NAcc (blue line) and the minimal activity in VTA (red line) during these omission trials for each CS-US association (<strong>(A)</strong>, <strong>(B)</strong> and <strong>(C)</strong>). One can observe that NAcc becomes quickly able to react for an omitted reward (after only 2 conditioning trials for CS3, 3 for CS1 and 7 for CS2). The speed of learning is therefore dependent on reward magnitude, what is due to the dopaminergic modulation of cortico-striatal learning: smaller rewards generate smaller VTA bursts, inducing less LTP in the NAcc. The VTA dips are directly dependent on this learning: as soon as NAcc is able to get activated for omitted rewards, the minimal activity in VTA at reward omission switches from the VTA baseline activity (0.2) to 0, indicating that VTA successfully signals reward omission.</p>
<p>This result is in accordance with experiments showing that the time interval from CS onset to US delivery is learned very rapidly at the start of training <span class="citation" data-cites="Balsam2002">(<a href="References.html#ref-Balsam2002" role="doc-biblioref">Balsam et al., 2002</a>)</span>. Although reward magnitude was long considered as playing only a minor role in acquisition speed during conditioning <span class="citation" data-cites="Gallistel2000">(<a href="References.html#ref-Gallistel2000" role="doc-biblioref">Gallistel and Gibbon, 2000</a>)</span>, more recent experiments showed that it influences the number of trials needed by an animal to exhibit conditioned responses during both appetitive and aversive conditioning <span class="citation" data-cites="Morris2006">(<a href="References.html#ref-Morris2006" role="doc-biblioref">Morris and Bouton, 2006</a>)</span> and that it speeds up learning of discrimination tasks <span class="citation" data-cites="Rose2009">(<a href="References.html#ref-Rose2009" role="doc-biblioref">Rose et al., 2009</a>)</span>. In accordance with these results, our model predicts that the ability to signal negative reward-prediction errors is learned faster when the reward magnitude is high.</p>
</section>
<section id="time-course-of-forebrain-nuclei" class="level3" data-number="5.3.7">
<h3 data-number="5.3.7" class="anchored" data-anchor-id="time-course-of-forebrain-nuclei"><span class="header-section-number">5.3.7</span> Time course of forebrain nuclei</h3>
<div id="fig-finr:all_timecourse" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-finr:all_timecourse-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/finr/10.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-finr:all_timecourse-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.10: Timecourse of activity in different areas of the model. Panel shows the activity during the last conditioning trial of the CS1-US1 association, while panel shows what happen during reward omission after learning (CS1 alone). The first row shows the inputs to the network, with the blue line showing the mean activity in the IT cluster corresponding to CS1, while the black line shows the mean activity for the neurons of LH representing US1. The second row shows the timecourse of the VTA cell during these trials, similar to what is shown on <a href="#fig-finr:vta_timecourse" class="quarto-xref">Figure&nbsp;<span>5.5</span></a>. The third row shows activity in CE, which matches the already observed timecourse in BLA during conditioning on <a href="#fig-finr:bla_conditioning" class="quarto-xref">Figure&nbsp;<span>5.4</span></a>. The fourth row depicts the timecourse of activity in PPTN, with the blue line showing the unit responding to CS onset (with inputs from CE) and the black the one responsive the US (with inputs from LH). The fourth, fifth, sixth, seventh and eighth rows depicts the maximal activity in NAcc, VP, LHb and RMTg, respectively. Please refer to the text for how these activations relate to each other.
</figcaption>
</figure>
</div>
<p>In order to better understand how the different nuclei in the model interact during conditioning and reward omission, <a href="#fig-finr:all_timecourse" class="quarto-xref">Figure&nbsp;<span>5.10</span></a> shows the time course of activity of several populations during the fifteenth conditioning trial of CS1-US1 (<a href="#fig-finr:all_timecourse" class="quarto-xref">Figure&nbsp;<span>5.10</span></a> A), followed by the omission of US1 (<a href="#fig-finr:all_timecourse" class="quarto-xref">Figure&nbsp;<span>5.10</span></a> B). The first row depicts the inputs to the networks, with the blue line showing the mean activity in the IT cluster selective for CS1 and the black line showing the mean activity of the LH neurons representing US1. As previously shown, VTA (second row) exhibits a phasic burst at CS onset on both trials, but barely reacts after learning when the reward is delivered, while it is strongly inhibited when the reward is omitted. The CS-driven burst is due to associative learning in the amygdala, what is reflected in the activity of the CE unit (third row). The transient activation of CE excites the CS-selective population in PPTN (fourth row, in blue), which in turn generates the phasic VTA burst and excites VP (sixth row). The excitation of VP increases the inhibition on LHb (seventh row) and RMTg (eighth row), which therefore remain silent.</p>
<p>When the reward is delivered (<a href="#fig-finr:all_timecourse" class="quarto-xref">Figure&nbsp;<span>5.10</span></a> A), LH activates directly the US-selective population of PPTN (fourth row, in black), but also the amygdala (reflected in the excitation of CE). However, the strong competition between the CS- and US-related populations of PPTN results in the phasic activation of the US group only (as it receives LH inputs slightly before the CS group gets activated by CE, which is a disynaptic pathway and therefore slower). The US group of PPTN activates VTA and VP similarly. At the same time, NAcc gets activated by the reward delivery, through its inputs from BLA and vmPFC, in conjunction with the phasic VTA burst bringing the cell into the up-state. NAcc is then able to cancel the VTA burst through its direct modulatory projection. NAcc also inhibits strongly VP, but this inhibition is canceled by the excitatory projection from PPTN to VP. VP therefore keeps inhibiting LHb and RMTg, and no VTA dip is observed.</p>
<p>When the reward is omitted (<a href="#fig-finr:all_timecourse" class="quarto-xref">Figure&nbsp;<span>5.10</span></a> B), PPTN does not receive inputs from LH or CE. The activation of NAcc at the expected time of reward delivery is now able to inhibit strongly VP, what releases LHb and RMTg from its strong tonic inhibition. LHb becomes transiently activated, exciting RMTg which can provoke a complete pause in VTA firing.</p>
<p>Although not directly comparable to recorded firing rates, the displayed time courses are in agreement with several observations, such as the activation of two different populations of PPTN neurons for reward-predictive cues and rewards <span class="citation" data-cites="Pan2005">(<a href="References.html#ref-Pan2005" role="doc-biblioref">Pan et al., 2005</a>)</span>, the activation at reward omission of LHb <span class="citation" data-cites="Hikosaka2008 Hong2011">(<a href="References.html#ref-Hikosaka2008" role="doc-biblioref">Hikosaka et al., 2008</a>; <a href="References.html#ref-Hong2011" role="doc-biblioref">Hong et al., 2011</a>)</span> and RMTg <span class="citation" data-cites="Jhou2009">(<a href="References.html#ref-Jhou2009" role="doc-biblioref">Jhou et al., 2009</a>)</span>, or the activation of VP for large reward-predicting cues and rewards <span class="citation" data-cites="Tindell2004 Smith2009">(<a href="References.html#ref-Smith2009" role="doc-biblioref">Smith et al., 2009</a>; <a href="References.html#ref-Tindell2004" role="doc-biblioref">Tindell et al., 2004</a>)</span>. VP is also inhibited at reward omission, what is consistent with the observed inhibition of some VP cells when small rewards is received during a session where larger rewards are available <span class="citation" data-cites="Tachibana2012">(<a href="References.html#ref-Tachibana2012" role="doc-biblioref">Tachibana and Hikosaka, 2012</a>)</span>.</p>
</section>
</section>
<section id="discussion" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="discussion"><span class="header-section-number">5.4</span> Discussion</h2>
<p>We have proposed a neuro-computational model of the afferent system to the dopaminergic area VTA, which is able to reproduce several observations on VTA’s behavior during appetitive conditioning: progressive appearance of phasic bursts of activity at CS onset, progressive diminution of the amplitude of the phasic bursts elicited by primary rewards, strong phasic inhibition at the time when reward is expected but not delivered <span class="citation" data-cites="Schultz1997 Fiorillo2003 Pan2005a">(<a href="References.html#ref-Fiorillo2003" role="doc-biblioref">Fiorillo et al., 2003</a>; <a href="References.html#ref-Pan2005a" role="doc-biblioref">Pan and Hyland, 2005</a>; <a href="References.html#ref-Schultz1997" role="doc-biblioref">Schultz et al., 1997</a>)</span>. Cancellation of US-related bursts and inhibition at reward omission both rely on learning of the duration of the CS-US interval in the NAcc, which influences VTA either directly or through the output structures of the ventral BG. This is in accordance with experiments showing that rewards delivered earlier than expected provoke a very high amplitude VTA burst which would have been canceled if delivered at the learned time <span class="citation" data-cites="Hollerman1998">(<a href="References.html#ref-Hollerman1998" role="doc-biblioref">Hollerman and Schultz, 1998</a>)</span>. Furthermore, the model reproduces the dependency on reward magnitude of the activities in BLA <span class="citation" data-cites="Bermudez2010">(<a href="References.html#ref-Bermudez2010" role="doc-biblioref">Bermudez and Schultz, 2010</a>)</span> and VTA <span class="citation" data-cites="Tobler2005">(<a href="References.html#ref-Tobler2005" role="doc-biblioref">Tobler et al., 2005</a>)</span>.</p>
<p>There are several aspects of reward processing and dopaminergic activity which are not covered by this model: the model is limited in its current form to classical conditioning and does not specifically address instrumental conditioning or goal-directed learning. However, Pavlovian-to-Instrumental transfer of learning, which is known to be particularly dependent on NAcc, is thought to be a critical component of goal-directed learning <span class="citation" data-cites="Cardinal2002 Corbit2011">(<a href="References.html#ref-Cardinal2002" role="doc-biblioref">Cardinal et al., 2002</a>; <a href="References.html#ref-Corbit2011" role="doc-biblioref">Corbit and Balleine, 2011</a>)</span> and the proposed model is a first step towards understanding these processes. Consequently, the model does not incorporate yet the known effects of the tonic component of VTA activity, which is thought to modulate motivation and engage reward-directed behaviors <span class="citation" data-cites="Daw2006 Niv2007">(<a href="References.html#ref-Daw2006" role="doc-biblioref">Daw et al., 2006</a>; <a href="References.html#ref-Niv2007" role="doc-biblioref">Niv et al., 2007</a>)</span>, and focuses only on the phasic components of VTA activity.</p>
<p>Three dimensions are particularly relevant in reward processing: reward magnitude, reward probability and time, with NAcc having been shown crucial in the adequate response to each of these dimensions <span class="citation" data-cites="Stopper2011">(<a href="References.html#ref-Stopper2011" role="doc-biblioref">Stopper and Floresco, 2011</a>)</span>. The proposed model focuses on reward-magnitude and time, leaving reward probability to further work. Manipulating reward probability will require to investigate the effect of VTA dips on learning in BLA and NAcc, with the extreme end of the spectrum being extinction of conditioning <span class="citation" data-cites="Tye2010">(<a href="References.html#ref-Tye2010" role="doc-biblioref">Tye et al., 2010</a>)</span>.</p>
<p>Within these validity boundaries, the model is able to make several testable predictions, among which the fact that VTA dips should only appear for sufficiently big rewards, or that the number of trials needed to observe US-related burst cancellation should be proportional to reward magnitude. It also predicts that at least a subpopulation of NAcc (presumably in the shell part) should be activated by reward omission. This prediction will be further discussed in the rest of the section.</p>
<p>From the neuro-computational point of view, the model is fully autonomous: it only learns from the relative timecourse of CS and US inputs. Apart from the distinction between the sensitization and conditioning phases, no additional mechanism such as a central executive is required to control learning in any of its populations. It relies only on the numerical integration of a set of interdependent dynamical equations, in conjunction with sensory inputs. Moreover, the neural mechanisms employed provide scalability, as multiple CS-US associations can be learned in parallel, depending on the number of neurons in BLA and NAcc. Future work will address its integration on a neurorobotical platform with realistic inputs.</p>
<section id="relation-to-other-work" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="relation-to-other-work"><span class="header-section-number">5.4.1</span> Relation to other work</h3>
<p>Early implementations of the TD algorithm used a unitary backward chaining mechanism using serial-compound temporal representations of the CS, where the value of the reward is progressively transferred to the previous time step (or state), until it corresponds to CS onset <span class="citation" data-cites="Montague1996 Schultz1997 Suri1999">(<a href="References.html#ref-Montague1996" role="doc-biblioref">Montague et al., 1996</a>; <a href="References.html#ref-Schultz1997" role="doc-biblioref">Schultz et al., 1997</a>; <a href="References.html#ref-Suri1999" role="doc-biblioref">Suri and Schultz, 1999</a>)</span>. For each time step of the conditioning sequence, DA represents a reward prediction error, i.e.&nbsp;the discrepancy between the amount of predicted reward and the actually received reward. Unless very long eligibility traces are used, TD predicts that DA bursts will gradually shift backwards in time from reward delivery to CS onset, what is not observed experimentally <span class="citation" data-cites="Pan2005a">(<a href="References.html#ref-Pan2005a" role="doc-biblioref">Pan and Hyland, 2005</a>)</span>. This also implies that the mechanism should work for any higher-order conditioning task, transferring the phasic burst to the earliest predictor of reward. In practice, only second-order conditioning has been observed, as noted in <span class="citation" data-cites="Hazy2010">(<a href="References.html#ref-Hazy2010" role="doc-biblioref">Hazy et al., 2010</a>)</span>. It however explains phenomenologically many aspects of DA activity during conditioning and has been used with great success in action-selection and decision-making frameworks as long as the action space is not too large, but its mapping on brain structures is problematic.</p>
<p><span class="citation" data-cites="Ludvig2008">Ludvig et al. (<a href="References.html#ref-Ludvig2008" role="doc-biblioref">2008</a>)</span> introduced an alternative temporal representation of the stimuli for the TD(<span class="math inline">\lambda</span>) algorithm. A set of overlapping temporal basis functions is used to filter out an exponentially decreasing trace of the stimuli (both CS and US) and provide a coarse coding of the time elapsed since stimulus onset. The output of this microstimuli representation gradually becomes weaker and coarser as time goes. Using these representations as inputs, the TD(<span class="math inline">\lambda</span>) algorithm is able to learn a reward-prediction error signal, gradually responding positively to the CS while cancelling its response to the US. If the US is omitted, it exhibits a negative reward-prediction error, although much weaker than previous versions of TD. If the reward is delivered earlier than expected, it responds maximally to it but shows only a very small dip at the expected time, without the need for an explicit reset of the temporal representations (see below for a discussion). A later extension of this model <span class="citation" data-cites="Ludvig2009">(<a href="References.html#ref-Ludvig2009" role="doc-biblioref">Ludvig et al., 2009</a>)</span> incorporated an additional array of microstimuli signaling the presence of a stimulus in addition to its trace and was able to better explain the functional difference between delay and trace conditioning, as well as to make interesting predictions about the role of the hippocampus in trace conditioning.</p>
<p>The model of <span class="citation" data-cites="Rivest2010 Rivest2013">Rivest et al. (<a href="References.html#ref-Rivest2010" role="doc-biblioref">2010</a>; <a href="References.html#ref-Rivest2013" role="doc-biblioref">Rivest et al., 2013</a>)</span> used an interesting approach to provide a temporal representation of the stimuli to the TD(<span class="math inline">\lambda</span>) algorithm: a LSTM network <span class="citation" data-cites="Hochreiter1997">(<a href="References.html#ref-Hochreiter1997" role="doc-biblioref">Hochreiter and Schmidhuber, 1997</a>)</span> is used to learn a temporal representation of both CS and US based only on stimulus onset and the reward-prediction error signal. A LSTM network is composed of recurrent memory blocks, each integrating its inputs depending on an adaptive gating function. This allows to learn to represent the CS by ramping functions peaking just before US delivery, allowing the TD(<span class="math inline">\lambda</span>) to access an adaptively timed representation of the stimulus. This model exhibits all the expected temporal properties of the DA signal in both delay and trace conditioning without any explicit representation of the task. Although needing an irrealistic number of trials to converge and having a significant error rate, this model builds an interesting bridge between reward-prediction, timing and working memory processes.</p>
<p>The proposed model shares more assumptions with the dual-pathway models. The model of <span class="citation" data-cites="Brown1999">Brown et al. (<a href="References.html#ref-Brown1999" role="doc-biblioref">1999</a>)</span>, later extended by <span class="citation" data-cites="Tan2008">Tan and Bullock (<a href="References.html#ref-Tan2008" role="doc-biblioref">2008</a>)</span>, has been a very important step in overcoming the problems of TD, and many of its assumptions still hold true. It similarly considers that rewards provoke DA bursts (although in SNc rather than VTA, but this is more a labeling issue) through the LH <span class="math inline">\rightarrow</span> PPTN <span class="math inline">\rightarrow</span> SNc pathway. Reward-predicting cues progressively elicit burst firing through the NAcc <span class="math inline">\rightarrow</span> VP <span class="math inline">\rightarrow</span> PPTN <span class="math inline">\rightarrow</span> SNc pathway, while the striosomes of NAcc learn to generate lagged, adaptively timed signals inhibiting SNc at the time when reward is expected. The comparison between the predicted and received rewards occurs directly at the level of the dopaminergic cells, while it occurs in VP in our model, providing an explanation for the role of LHb and RMTg in reward omission. Moreover, this model hypothesizes a common NAcc <span class="math inline">\rightarrow</span> SNc pathway for both US-related burst cancellation and dips at reward omission, while they are functionally separated in our model. The major problem with the model of <span class="citation" data-cites="Brown1999">Brown et al. (<a href="References.html#ref-Brown1999" role="doc-biblioref">1999</a>)</span> and <span class="citation" data-cites="Tan2008">Tan and Bullock (<a href="References.html#ref-Tan2008" role="doc-biblioref">2008</a>)</span> in our view is the mechanism underlying the adaptively timed inhibitory learning in the striosomes of NAcc. The proposed intracellular spectral timing mechanism <span class="citation" data-cites="Grossberg1989 Fiala1996">(<a href="References.html#ref-Fiala1996" role="doc-biblioref">Fiala et al., 1996</a>; <a href="References.html#ref-Grossberg1989" role="doc-biblioref">Grossberg and Schmajuk, 1989</a>)</span>, relying on mGLUR1-mediated delayed Ca<span class="math inline">^{2+}</span> spikes with distinct time constants for each striosomal cell, indeed allows to learn specific duration in conjunction with DA bursts, but the maximal interval learnable by this mechanism is equal to the longest delayed spike possible, what is likely to lie in the sub-second range as in the cerebellum <span class="citation" data-cites="Fiala1996">(<a href="References.html#ref-Fiala1996" role="doc-biblioref">Fiala et al., 1996</a>)</span>. For the supra-second range, network-based oscillatory mechanisms such as the striatal-beat frequency model are more likely to be sufficiently efficient and robust to learn such delays <span class="citation" data-cites="Coull2011">(<a href="References.html#ref-Coull2011" role="doc-biblioref">Coull et al., 2011</a>)</span>.</p>
<p>The model called PVLV (Primary-Value and Learned-Value) initially proposed by <span class="citation" data-cites="OReilly2007">O’Reilly et al. (<a href="References.html#ref-OReilly2007" role="doc-biblioref">2007</a>)</span> and refined in <span class="citation" data-cites="Hazy2010">Hazy et al. (<a href="References.html#ref-Hazy2010" role="doc-biblioref">2010</a>)</span> builds up on these ideas. The primary value (PV, the value of the reward itself) and the learned value (LV, the value of the reward-predicting cue) during conditioning are computed by two different afferent systems to VTA, both with an excitatory and an inhibitory component. The excitatory PV system PVe signals reward delivery to VTA through a direct connection from LH to VTA, although a relay through PPTN would perform the same function as in our model. The excitatory LV system LVe learns to generate DA bursts at CS onset, through a direct projection from CE to VTA: as in our model, the amygdala learns to associate a sustained representation of the CS to the delivery of reward when the US-related burst (or dip) occurs. The inhibitory PV system PVi, composed of the striosomal neurons in NAcc, learns to cancel progressively US-related bursts, but in an almost time-independent manner: they use a ramping function activated by CS onset and peaking at reward delivery that modulates the reward prediction. The origin of such as signal is putatively in the cerebellum, but no details are provided on how such a signal could be adapted to different CS-US durations. Moreover, this implies that rewards given earlier than expected would still provoke attenuated DA bursts. Last, the inhibitory LV system LVi, also in the striosomes of NAcc, slowly learns to cancel CS-related bursts in order to avoid over-learning in auto-shaping experiments (where the CS becomes an incentive to action, what is not covered by our model). The main issue with this model is that timing mechanisms are only phenomenologically incorporated, what may be due to the fact that the equations governing neuronal activation and learning are discretized with a time step of 1 second, instead of 1 millisecond in the model of <span class="citation" data-cites="Brown1999">Brown et al. (<a href="References.html#ref-Brown1999" role="doc-biblioref">1999</a>)</span> or ours. However, this model explains several aspects of conditioning, including acquisition, extinction, blocking, overshadowing, conditioned inhibition and second-order conditioning. Furthermore, it has been successfully integrated into a wider functional model of working memory including the prefrontal cortex and the dorsal BG <span class="citation" data-cites="OReilly2006">(<a href="References.html#ref-OReilly2006" role="doc-biblioref">O’Reilly and Frank, 2006</a>)</span>.</p>
<p>Together with an extensive review of the functional and electrophysiological properties of the ventral basal ganglia, <span class="citation" data-cites="Humphries2010">Humphries and Prescott (<a href="References.html#ref-Humphries2010" role="doc-biblioref">2010</a>)</span> propose a neuro-computational model of how a specific subcircuit of the ventral BG, involving the shell part of NAcc (which integrates cortical, amygdalar and hippocampal inputs) and some part of VP, can selectively produce either bursts or dips in VTA, depending on the relative balance between the direct pathway (arising from NAcc cells carrying D1 receptors and projecting directly on VTA) and the indirect pathway (with NAcc neurons carrying both D1 and D2 receptors and projecting mainly on VP). In this framework, the prediction of a reward activates the direct pathway, what can either reduce the bursting amplitude or produce a dip in VTA, while the actual receipt of that reward activates the indirect pathway, canceling the influence of the direct pathway and allowing VTA bursts. While being more precise than our model on the functional role of NAcc cell subtypes, this model is limited to bursts or dips occurring at reward delivery (or at the time when reward is expected), but does not address the case of reward-predicting stimuli nor the issue of timing. This model has nevertheless the advantage of being understood equally well in the reward-prediction error framework of DA activity and in the action-outcome repertoire framework, which proposes that DA bursts primarily help associating an action with its delayed consequences <span class="citation" data-cites="Redgrave2008">(<a href="References.html#ref-Redgrave2008" role="doc-biblioref">Redgrave et al., 2008</a>)</span>.</p>
<p><span class="citation" data-cites="Chorley2011">Chorley and Seth (<a href="References.html#ref-Chorley2011" role="doc-biblioref">2011</a>)</span> proposed a dual-pathway model incorporating some concepts of the striatal-beat frequency model. It is composed of several populations of spiking point-neurons, subject to synaptic plasticity using a dopamine-modulated spike-timing dependent plasticity (STDP) learning rule <span class="citation" data-cites="Izhikevich2007">(<a href="References.html#ref-Izhikevich2007" role="doc-biblioref">Izhikevich, 2007</a>)</span>. In this model, the sensory representation of the US initially activates the DA population through an excitatory relay (either the subthalamic nucleus (STN) or the superior colliculus). The corresponding DA burst enables STDP learning between the sustained sensory representation of the CS and STN, what leads to a progressive bursting behavior in VTA at CS onset. In parallel, the inhibitory pathway to VTA, involving the prefrontal cortex and the striatum, learns to progressively cancel the US-related burst and, if reward is omitted, to strongly inhibit the VTA population. The mechanism for learning the CS-US interval is similar to the striatal-beat frequency hypothesis: CS onset activates a pre-recorded sequence of spikes in the prefrontal cortex (identical in each trial) and the striatum learns to react phasically to the precise pattern corresponding to the elapsed duration at US onset. This pre-recorded sequence of spikes is functionally equivalent to a set of neural oscillators synchronized at CS onset and expressing reproducible patterns at the population level. <span class="citation" data-cites="Oprisan2011">Oprisan and Buhusi (<a href="References.html#ref-Oprisan2011" role="doc-biblioref">2011</a>)</span> investigated a similar mechanism using Morris-Lecar neurons and showed that even noisy oscillators, with variable inter-spike intervals, are able to produce a population code for the elapsed duration since CS onset which can be detected by striatal coincidence detectors. The model of <span class="citation" data-cites="Chorley2011">Chorley and Seth (<a href="References.html#ref-Chorley2011" role="doc-biblioref">2011</a>)</span> is an elegant mechanism describing the evolution of DA bursts during conditioning as well as for earlier delivery of reward or reward omission. It does not however map very precisely on the brain’s architecture, nor take the effect of reward magnitude into account.</p>
</section>
<section id="sec-finr:discussion-bio" class="level3" data-number="5.4.2">
<h3 data-number="5.4.2" class="anchored" data-anchor-id="sec-finr:discussion-bio"><span class="header-section-number">5.4.2</span> Biological plausibility</h3>
<p>The structure of the proposed model is derived from known anatomical connections, and the used neural mechanisms are consistent with experimental data, either at the cellular or population level. It provides a minimal description of the network involved in controlling VTA activity during classical conditioning, with respect to a limited set of observations. However, there exists a certain number of other brain areas which are directly or indirectly involved in this process. Similarly, alternative mechanisms, especially for timing, might replace or complement the proposed ones. The purpose of this section is to discuss alternatives to the current assumptions.</p>
<p>One key assumption in the model is that there exists a subgroup of NAcc neurons, presumably in the striosomes (group of striatal neurons that project directly on SNc or VTA), which get activated at reward omission. The previously reviewed dual-pathway models also share this assumption, and justify it by observations that some cells in the ventral striatum display a ramping activity pattern, with firing rates almost linearly increasing from CS onset and peaking at the time when reward is expected <span class="citation" data-cites="Schultz1992 Deadwyler2004">(<a href="References.html#ref-Deadwyler2004" role="doc-biblioref">Deadwyler et al., 2004</a>; <a href="References.html#ref-Schultz1992" role="doc-biblioref">Schultz et al., 1992</a>)</span>. This indicates that the CS-US interval duration is indeed learned by NAcc cells, but raises the question of how such a ramping signal can be transformed into a phasic inhibition after reward is expected: direct inhibition of VTA by such ramping cells in NAcc should progressively reduce VTA firing as the time since CS onset increases, which is obviously not the case. Is there a still undiscovered group of NAcc cells firing only at reward delivery/omission, or do these ramping activities play a more complex role in the timing of CS-US intervals during conditioning? In the striatum, some cholinergic TAN interneurons show complex patterns (either excitation or inhibition) at reward omission <span class="citation" data-cites="Apicella2009">(<a href="References.html#ref-Apicella2009" role="doc-biblioref">Apicella et al., 2009</a>)</span>. As these cholinergic interneurons can disinhibit MSNs through the modulation of fast-spiking inhibitory interneurons and bring them in the up-state <span class="citation" data-cites="Coull2011">(<a href="References.html#ref-Coull2011" role="doc-biblioref">Coull et al., 2011</a>)</span>, it may provide a mechanism for the phasic activation of a subgroup of NAcc cells at reward omission. A more detailed model of the internal circuitry of NAcc is obviously needed.</p>
<p>Alternatively, ramping activities in the NAcc during the CS-US interval might complement or even replace such mechanisms. Such ramping activities have been also observed in the thalamus <span class="citation" data-cites="Komura2001">(<a href="References.html#ref-Komura2001" role="doc-biblioref">Komura et al., 2001</a>)</span> and prefrontal cortex <span class="citation" data-cites="Reutimann2004">(<a href="References.html#ref-Reutimann2004" role="doc-biblioref">Reutimann et al., 2004</a>)</span>, with the slope of the ramp being proportional to the duration. This suggests that a cortex - ventral basal ganglia - thalamus loop might be a good candidate to actually learn the CS-US interval duration with climbing activities, modulated by the dopamine level. Based on this idea, many models have been proposed for interval timing using neural integration or drift-diffusion models <span class="citation" data-cites="Durstewitz2004 Simen2011 Luzardo2013">(<a href="References.html#ref-Durstewitz2004" role="doc-biblioref">Durstewitz, 2004</a>; <a href="References.html#ref-Luzardo2013" role="doc-biblioref">Luzardo et al., 2013</a>; <a href="References.html#ref-Simen2011" role="doc-biblioref">Simen et al., 2011</a>)</span>. The model of <span class="citation" data-cites="Rivest2010 Rivest2013">(<a href="References.html#ref-Rivest2010" role="doc-biblioref">Rivest et al., 2010</a>; <a href="References.html#ref-Rivest2013" role="doc-biblioref">Rivest et al., 2013</a>)</span> is a good example of such a mechanism. However, how the maximal activity reached by such ramps is transformed into a precisely-timed phasic signal at reward omission still raises difficult technical questions, such as the effect of noise on the precision of neural integration, especially for long intervals, or the plausibility of the learning mechanisms.</p>
<p>In comparison to the other dual-pathway models, our model is to our knowledge the first to explicitly incorporate distinct origins for the cancellation of US-related bursts and for the dips at reward omission, although the idea was already proposed in <span class="citation" data-cites="Hazy2010">(<a href="References.html#ref-Hazy2010" role="doc-biblioref">Hazy et al., 2010</a>)</span> as a functional interpretation of the inhibitory component of the PV system PVi. As the authors noted, cancellation of a US-related burst must derive from an inhibitory signal occurring slightly in advance from the receipt of reward in order to be efficient, while the dips associated with omitted rewards occur clearly after the expected time, and the duration of these dips extends significantly longer than the corresponding bursts. They state that the first component is likely to be implemented by the direct inhibitory projection of NAcc on VTA, while the second results from a disinhibition of LHb by NAcc through a relay on VP, but the learning site of the CS-US duration is NAcc in both cases. This interpretation is consistent with our model. The question that arises is whether distinct subpopulations of NAcc participate in these two mechanisms: do the striosomes directly projecting to VTA exhibit ramping activity, thus being able to cancel US-related bursts in advance, while the matrix neurons, projecting to VP and therefore to the LHb/RMTg complex, exhibit a more phasic behavior and get activated only at reward delivery or omission, as predicted by the striatal-beat frequency model?</p>
<p>As observed experimentally <span class="citation" data-cites="Fiorillo2008">(<a href="References.html#ref-Fiorillo2008" role="doc-biblioref">Fiorillo et al., 2008</a>)</span>, the cancellation of the US-related bursts becomes weaker when the CS-US interval increases. We are not aware of any study reporting a similar effect of the interval duration on dips at reward omission. If not, this may support the idea that two different mechanisms govern the two types of inhibition: neural integration becomes less precise when the duration increases, as it becomes more difficult to detect when the maximum of the slope is attained, while coincidence detectors are more robust, provided that the oscillators are not too noisy <span class="citation" data-cites="Matell2004 Oprisan2011">(<a href="References.html#ref-Matell2004" role="doc-biblioref">Matell and Meck, 2004</a>; <a href="References.html#ref-Oprisan2011" role="doc-biblioref">Oprisan and Buhusi, 2011</a>)</span>.</p>
<p>An open issue with the coincidence detectors hypothesis is that corticostriatal learning is potentiated by DA bursts at reward delivery. Typical bursts in VTA are relatively long (150 to 200 ms), what implies that cortical oscillators with a frequency superior to 5 or 6 Hz can show a full period during the burst. In the model, the parameter <span class="math inline">\tau_{\text{dopa}}=10</span> ms representing the time constant of the phasic effect of DA on corticostriatal learning (<a href="#eq-finr:dacovariance" class="quarto-xref">Equation&nbsp;<span>5.11</span></a>) was artificially set to a very fast value to ensure that learning occurs at the very beginning of the burst. Slower values led to the situation where NAcc could only predict the occurrence of reward delivery at the end of the burst, what arrives too late to effectively cancel the burst. In the model of <span class="citation" data-cites="Chorley2011">(<a href="References.html#ref-Chorley2011" role="doc-biblioref">Chorley and Seth, 2011</a>)</span>, bursting behavior occurs in a time window of 50 ms, which, coupled to the precise timing properties of STDP when compared to Hebbian learning rules, allows a very sharp learning of the time elapsed since CS onset. How can very high oscillation frequencies (the original Striatal-Beat Frequency model uses oscillators in the delta range 8 to 13 Hz) accommodate with such large DA bursts is still an unresolved question.</p>
<p>In <a href="#sec-finr:results-vta" class="quarto-xref"><span>Section 5.3.2</span></a>, the earlier delivery of a reward lead to a VTA burst of the same amplitude as an expected reward, but not to a dip at the expected time, as observed experimentally <span class="citation" data-cites="Hollerman1998">(<a href="References.html#ref-Hollerman1998" role="doc-biblioref">Hollerman and Schultz, 1998</a>)</span>. This is only because the CS representation stops when the US disappears. If the CS were maintained for a longer duration, such a dip would in fact be observed as the oscillators in vmPFC would still signal the elapsed duration. There is a need for a reset mechanism stopping the oscillators at reward delivery. A possible pathway would involve a closed-loop between vmPFC and the ventral BG, with the inhibitory projection from VP on the mediodorsal nucleus of the thalamus (MD) being able to stop thalamo-cortical oscillations between MD and vmPFC at reward delivery. The problem of resetting temporal representations after reward delivery is common to many models (see <span class="citation" data-cites="Daw2006">Daw et al. (<a href="References.html#ref-Daw2006" role="doc-biblioref">2006</a>)</span> for a review), at the notable exception of the model of <span class="citation" data-cites="Ludvig2008">Ludvig et al. (<a href="References.html#ref-Ludvig2008" role="doc-biblioref">2008</a>)</span>.</p>
<p>Although successfully reproducing the known effects of reward magnitude on DA activity, the proposed model does not investigate the case where less reward than expected, instead of no reward at all. Experimentally, VP gets activated by large rewards and inhibited by small ones <span class="citation" data-cites="Tachibana2012">(<a href="References.html#ref-Tachibana2012" role="doc-biblioref">Tachibana and Hikosaka, 2012</a>)</span>, while LHb shows the opposite pattern <span class="citation" data-cites="Hikosaka2008">(<a href="References.html#ref-Hikosaka2008" role="doc-biblioref">Hikosaka et al., 2008</a>)</span>. Based on the current model, we propose that the comparison between predicted and received reward may be computed in VP through the competition between inhibitory inputs from NAcc and excitatory inputs from PPTN and is further transmitted to VTA either directly or through disinhibition of LHb and RMTg. A further refinement of the model in these areas may also shed some light on the influence of aversive stimuli, which are able to activate the lateral habenula and produce DA dips <span class="citation" data-cites="Matsumoto2007">(<a href="References.html#ref-Matsumoto2007" role="doc-biblioref">Matsumoto and Hikosaka, 2007</a>)</span> but also to generate bursts in some subpopulations of VTA <span class="citation" data-cites="Brischoux2009 Lammel2012">(<a href="References.html#ref-Brischoux2009" role="doc-biblioref">Brischoux et al., 2009</a>; <a href="References.html#ref-Lammel2012" role="doc-biblioref">Lammel et al., 2012</a>)</span>.</p>
<p>The subthalamic nucleus (STN) has been left out of the model, although it is part of the ventral BG. Like NAcc, its medial part receives cortical inputs from the medial prefontal cortex, but it projects excitatorily on the part of VP receiving connections from the core of NAcc. It has been shown to encode both reward magnitude, reward expectation and errors <span class="citation" data-cites="Darbaky2005 Lardeux2009">(<a href="References.html#ref-Darbaky2005" role="doc-biblioref">Darbaky et al., 2005</a>; <a href="References.html#ref-Lardeux2009" role="doc-biblioref">Lardeux et al., 2009</a>)</span> and is important for Pavlovian-to-Instrumental transfer of learning <span class="citation" data-cites="Winstanley2005">(<a href="References.html#ref-Winstanley2005" role="doc-biblioref">Winstanley et al., 2005</a>)</span>. STN may signal the motivational value of stimuli to VP, complementing the information received from PPTN. Future extension of this model to instrumental learning will have to investigate the role of STN more deeply.</p>
<p>Similarly, the cerebellum is a very important player in aversive conditioning, as in the eyeblink conditioning paradigm <span class="citation" data-cites="Christian2003 Thompson2009">(<a href="References.html#ref-Christian2003" role="doc-biblioref">Christian and Thompson, 2003</a>; <a href="References.html#ref-Thompson2009" role="doc-biblioref">Thompson and Steinmetz, 2009</a>)</span>. It has been left out of the model as its involvement in appetitive conditioning is still unknown. However, it is now acknowledged that the cerebellum and the basal ganglia communicate more with each other than initially thought: in particular, the cerebellum projects on thalamic nuclei which directly contact the striatum, especially the D2-type neurons of the indirect pathway <span class="citation" data-cites="Bostan2010">(<a href="References.html#ref-Bostan2010" role="doc-biblioref">Bostan and Strick, 2010</a>)</span>. How the BG and the cerebellum cooperate during conditioning still has to be explored.</p>
<p>The role of the ventral striatum in timing processes is also subject to debate. Several studies have shown that NAcc plays no important role in the timing of instrumental responding <span class="citation" data-cites="Meck2006 Galtress2010">(<a href="References.html#ref-Galtress2010" role="doc-biblioref">Galtress and Kirkpatrick, 2010</a>; <a href="References.html#ref-Meck2006" role="doc-biblioref">Meck, 2006</a>)</span>, contrarily to the timing of Pavlovian responses <span class="citation" data-cites="Singh2011">(<a href="References.html#ref-Singh2011" role="doc-biblioref">Singh et al., 2011</a>)</span>. However, both processes are interrelated, as they both rely on dopaminergic activation, while NAcc is considered as a crucial site for Pavlovian-to-Instrumental transfer of learning <span class="citation" data-cites="Corbit2011">(<a href="References.html#ref-Corbit2011" role="doc-biblioref">Corbit and Balleine, 2011</a>)</span>. The Striatal-Beat Frequency model was initially proposed for the timing of instrumental responses, and identified the dorsal striatum as a potential substrate for the coincidence detection. Are two sites of temporal learning really needed for such interdependent processes? <span class="citation" data-cites="Kirkpatrick2013">Kirkpatrick (<a href="References.html#ref-Kirkpatrick2013" role="doc-biblioref">2013</a>)</span> proposed a functional model of the interactions of timing and prediction error learning, where NAcc and BLA cooperate to compute the reward value, while the timing of the association itself is learned in the dorsal BG and transmitted to the DA system through its output GPi (internal segment of the globus pallidus). Indeed, the border regions of GPi, which is usually considered as composed of GABAergic neurons projecting to the thalamus, have been shown to send an excitatory projection on LHb, what can in turn produce DA dips <span class="citation" data-cites="Hong2008">(<a href="References.html#ref-Hong2008" role="doc-biblioref">Hong and Hikosaka, 2008</a>)</span>. These LHb-projecting neurons in GPi exhibit a negative reward-prediction error pattern, excited by reward omission and inhibited by large rewards, which is similar to the one in LHb but occurs slightly in advance. These border regions of GPi receive projections from both the dorsal and ventral striatum, so it is possible that both the dorsal and ventral parts of the BG cooperate to learn the temporal properties of both action-outcome and stimulus-reward associations.</p>
<p>The proposed model is also rather conservative regarding the role of the amygdala in timing: given that the amygdala is a key structure in acquiring, processing and storing Pavlovian associations and that timing is a fundamental component of conditioning, there should be some neural correlates of temporal processing in the amygdala. Several lines of evidence indeed suggests such an involvement, as reviewed in <span class="citation" data-cites="Diaz-Mataix2013">(<a href="References.html#ref-Diaz-Mataix2013" role="doc-biblioref">Dı́az-Mataix et al., 2013</a>)</span>. In particular, a subgroup of neurons in BLA exhibits a strong change in firing rate at the time when the US is expected but not delivered <span class="citation" data-cites="Belova2007">(<a href="References.html#ref-Belova2007" role="doc-biblioref">Belova et al., 2007</a>)</span>, while some others show anticipatory activity for the reward, proportional to the instantaneous reward delivery probability <span class="citation" data-cites="Bermudez2010">(<a href="References.html#ref-Bermudez2010" role="doc-biblioref">Bermudez and Schultz, 2010</a>)</span>. This phenomenon might be particularly relevant for extinction, where the prolonged absence of the US should decrease the conditioning strength associated to the CS <span class="citation" data-cites="Tye2010">(<a href="References.html#ref-Tye2010" role="doc-biblioref">Tye et al., 2010</a>)</span>. The question is now from where does this timing information come from. Is it only signaled by the dopaminergic projection from VTA to BLA, which is able to modulate both firing and learning in BLA, or do other structures such as the hippocampus or vmPFC play a role?</p>
<p>In our model, the CS-related bursts in VTA arise from the BLA <span class="math inline">\rightarrow</span> CE <span class="math inline">\rightarrow</span> PPTN pathway, both during and after learning. However, CE has been shown to be important for learning but not expressing approach to appetitive cues <span class="citation" data-cites="McDannald2004 Groshek2005">(<a href="References.html#ref-Groshek2005" role="doc-biblioref">Groshek et al., 2005</a>; <a href="References.html#ref-McDannald2004" role="doc-biblioref">McDannald et al., 2004</a>)</span>. One possibility is that associations learned in the amygdala are progressively transferred to the orbitofrontal or ventromedial prefrontal cortices, which are known to project excitatorily onto VTA <span class="citation" data-cites="Geisler2007">(<a href="References.html#ref-Geisler2007" role="doc-biblioref">Geisler et al., 2007</a>)</span>. It is indeed known that frontal-amygdalar interactions are necessary for the formation and use of expectancies of reinforcers in the guidance of goal-directed behavior <span class="citation" data-cites="Holland2004">(<a href="References.html#ref-Holland2004" role="doc-biblioref">Holland and Gallagher, 2004</a>)</span>. It is therefore possible that the value associated to a reward is first associated to the sensory features of the predicting CS in the amygdala (what can initially generate CS-related bursts) but that the prefrontal cortex progressively learns to compute the motivational value of the CS and activate the dopaminergic system with this information. The known inhibitory projection from the medial prefrontal cortex to BLA might provide a direct mechanism to implement this transfer of responsability <span class="citation" data-cites="Carmichael1995">(<a href="References.html#ref-Carmichael1995" role="doc-biblioref">Carmichael and Price, 1995</a>)</span>, while NAcc is at a central position to control their interplay <span class="citation" data-cites="ODonnell1995">(<a href="References.html#ref-ODonnell1995" role="doc-biblioref">O’Donnell and Grace, 1995</a>)</span>.</p>
</section>
<section id="conclusion" class="level3" data-number="5.4.3">
<h3 data-number="5.4.3" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">5.4.3</span> Conclusion</h3>
<p>We have proposed a neuro-computational model linking reward processing to timing processes by focusing on the observed activity patterns of dopaminergic neurons during Pavlovian conditioning. We isolated a group of brain areas involved in the different aspects of appetitive conditioning and built a network using known anatomical connections. The resulting neural network model reproduces several experimental observations, while providing a robust mechanism for classical conditioning which can be implemented on a robotical platform. Its structure provides a first step toward building biologically realistic models of instrumental responding by understanding how the dopaminergic signal can be generated. Future extensions of this model, especially by focusing on the ventral BG and the crucial role of NAcc, will allow to learn the motivational value of different stimuli by transferring the value of an outcome to the action associated to the stimulus. They will ultimately allow to study the neural substrates of goal-directed behavior and their relationship with neuromodulators such as dopamine.</p>
<section id="disclosureconflict-of-interest-statement" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="disclosureconflict-of-interest-statement">Disclosure/Conflict-of-Interest Statement</h4>
<p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p>
</section>
<section id="acknowledgement" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="acknowledgement">Acknowledgement</h4>
<p>The authors are partially funded by the Deutsche Forschungsgemeinschaft (DFG) grant HA2630/4-2 and clinical research group DFG HA2630/7-1.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-Ambroggi2008" class="csl-entry" role="listitem">
Ambroggi, F., Ishikawa, A., Fields, H. L., and Nicola, S. M. (2008). <span class="nocase">Basolateral amygdala neurons facilitate reward-seeking behavior by exciting nucleus accumbens neurons.</span> <em>Neuron</em> 59, 648–61. doi:<a href="https://doi.org/10.1016/j.neuron.2008.07.004">10.1016/j.neuron.2008.07.004</a>.
</div>
<div id="ref-Apicella2009" class="csl-entry" role="listitem">
Apicella, P., Deffains, M., Ravel, S., and Legallet, E. (2009). <span class="nocase">Tonically active neurons in the striatum differentiate between delivery and omission of expected reward in a probabilistic task context.</span> <em>Eur. J. Neurosci.</em> 30, 515–26. doi:<a href="https://doi.org/10.1111/j.1460-9568.2009.06872.x">10.1111/j.1460-9568.2009.06872.x</a>.
</div>
<div id="ref-Balcita2011" class="csl-entry" role="listitem">
Balcita-Pedicino, J. J., Omelchenko, N., Bell, R., and Sesack, S. R. (2011). <span class="nocase">The inhibitory influence of the lateral habenula on midbrain dopamine cells: ultrastructural evidence for indirect mediation via the rostromedial mesopontine tegmental nucleus.</span> <em>J. Comp. Neurol.</em> 519, 1143–64. doi:<a href="https://doi.org/10.1002/cne.22561">10.1002/cne.22561</a>.
</div>
<div id="ref-Balsam2002" class="csl-entry" role="listitem">
Balsam, P. D., Drew, M. R., and Yang, C. (2002). <span class="nocase">Timing at the Start of Associative Learning</span>. <em>Learn. Motiv.</em> 33, 141–155. Available at: <a href="http://www.sciencedirect.com/science/article/pii/S002396900191104X">http://www.sciencedirect.com/science/article/pii/S002396900191104X</a>.
</div>
<div id="ref-Baxter2002" class="csl-entry" role="listitem">
Baxter, M. G., and Murray, E. A. (2002). <span class="nocase">The amygdala and reward.</span> <em>Nat. Rev. Neurosci.</em> 3, 563–73. doi:<a href="https://doi.org/10.1038/nrn875">10.1038/nrn875</a>.
</div>
<div id="ref-Belova2007" class="csl-entry" role="listitem">
Belova, M. A., Paton, J. J., Morrison, S. E., and Salzman, C. D. (2007). <span class="nocase">Expectation modulates neural responses to pleasant and aversive stimuli in primate amygdala.</span> <em>Neuron</em> 55, 970–84. doi:<a href="https://doi.org/10.1016/j.neuron.2007.08.004">10.1016/j.neuron.2007.08.004</a>.
</div>
<div id="ref-Bermudez2010" class="csl-entry" role="listitem">
Bermudez, M. A., and Schultz, W. (2010). <span class="nocase">Reward magnitude coding in primate amygdala neurons.</span> <em>J. Neurophysiol.</em> 104, 3424–32. doi:<a href="https://doi.org/10.1152/jn.00540.2010">10.1152/jn.00540.2010</a>.
</div>
<div id="ref-Bissiere2003" class="csl-entry" role="listitem">
Bissière, S., Humeau, Y., and Lüthi, A. (2003). <span class="nocase">Dopamine gates LTP induction in lateral amygdala by suppressing feedforward inhibition.</span> <em>Nat. Neurosci.</em> 6, 587–92. doi:<a href="https://doi.org/10.1038/nn1058">10.1038/nn1058</a>.
</div>
<div id="ref-Bolam2000" class="csl-entry" role="listitem">
Bolam, J. P., Hanley, J. J., Booth, P. A., and Bevan, M. D. (2000). <span class="nocase">Synaptic organisation of the basal ganglia.</span> <em>J. Anat.</em> 196 ( Pt 4, 527–42. Available at: <a href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1468095\&amp;tool=pmcentrez\&amp;rendertype=abstract">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1468095\&amp;tool=pmcentrez\&amp;rendertype=abstract</a>.
</div>
<div id="ref-Bostan2010" class="csl-entry" role="listitem">
Bostan, A. C., and Strick, P. L. (2010). <span class="nocase">The cerebellum and basal ganglia are interconnected.</span> <em>Neuropsychol. Rev.</em> 20, 261–70. doi:<a href="https://doi.org/10.1007/s11065-010-9143-9">10.1007/s11065-010-9143-9</a>.
</div>
<div id="ref-Bourdy2012" class="csl-entry" role="listitem">
Bourdy, R., and Barrot, M. (2012). <span class="nocase">A new control center for dopaminergic systems: pulling the VTA by the tail.</span> <em>Trends Neurosci.</em> 35, 681–90. doi:<a href="https://doi.org/10.1016/j.tins.2012.06.007">10.1016/j.tins.2012.06.007</a>.
</div>
<div id="ref-Brischoux2009" class="csl-entry" role="listitem">
Brischoux, F., Chakraborty, S., Brierley, D. I., and Ungless, M. A. (2009). <span class="nocase">Phasic excitation of dopamine neurons in ventral VTA by noxious stimuli.</span> <em>Proc. Natl. Acad. Sci. U. S. A.</em> 106, 4894–9. doi:<a href="https://doi.org/10.1073/pnas.0811507106">10.1073/pnas.0811507106</a>.
</div>
<div id="ref-Bromberg-Martin2011" class="csl-entry" role="listitem">
Bromberg-Martin, E. S., and Hikosaka, O. (2011). <span class="nocase">Lateral habenula neurons signal errors in the prediction of reward information.</span> <em>Nat. Neurosci.</em> 14, 1209–1216. doi:<a href="https://doi.org/10.1038/nn.2902">10.1038/nn.2902</a>.
</div>
<div id="ref-Brown1999" class="csl-entry" role="listitem">
Brown, J., Bullock, D., and Grossberg, S. (1999). <span class="nocase">How the basal ganglia use parallel excitatory and inhibitory learning pathways to selectively respond to unexpected rewarding cues.</span> <em>J. Neurosci.</em> 19, 10502–11. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/10575046">http://www.ncbi.nlm.nih.gov/pubmed/10575046</a>.
</div>
<div id="ref-Calabresi2007" class="csl-entry" role="listitem">
Calabresi, P., Picconi, B., Tozzi, A., and Di Filippo, M. (2007). <span class="nocase">Dopamine-mediated regulation of corticostriatal synaptic plasticity.</span> <em>Trends Neurosci.</em> 30, 211–9. doi:<a href="https://doi.org/10.1016/j.tins.2007.03.001">10.1016/j.tins.2007.03.001</a>.
</div>
<div id="ref-Cardinal2002" class="csl-entry" role="listitem">
Cardinal, R. N., Parkinson, J. A., Hall, J., and Everitt, B. J. (2002). <span class="nocase">Emotion and motivation: the role of the amygdala, ventral striatum, and prefrontal cortex.</span> <em>Neurosci. Biobehav. Rev.</em> 26, 321–52. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/12034134">http://www.ncbi.nlm.nih.gov/pubmed/12034134</a>.
</div>
<div id="ref-Carmichael1995" class="csl-entry" role="listitem">
Carmichael, S. T., and Price, J. L. (1995). <span class="nocase">Sensory and premotor connections of the orbital and medial prefrontal cortex of macaque monkeys.</span> <em>J. Comp. Neurol.</em> 363, 642–664. doi:<a href="https://doi.org/10.1002/cne.903630409">10.1002/cne.903630409</a>.
</div>
<div id="ref-Carr2000" class="csl-entry" role="listitem">
Carr, D. B., and Sesack, S. R. (2000). <span class="nocase">GABA-containing neurons in the rat ventral tegmental area project to the prefrontal cortex.</span> <em>Synapse</em> 38, 114–23. doi:<a href="https://doi.org/10.1002/1098-2396(200011)38:2<114::AID-SYN2>3.0.CO;2-R">10.1002/1098-2396(200011)38:2&lt;114::AID-SYN2&gt;3.0.CO;2-R</a>.
</div>
<div id="ref-Cheng1997" class="csl-entry" role="listitem">
Cheng, K., Saleem, K. S., and Tanaka, K. (1997). <span class="nocase">Organization of corticostriatal and corticoamygdalar projections arising from the anterior inferotemporal area TE of the macaque monkey: a Phaseolus vulgaris leucoagglutinin study.</span> <em>J. Neurosci.</em> 17, 7902–25. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/9315910">http://www.ncbi.nlm.nih.gov/pubmed/9315910</a>.
</div>
<div id="ref-Chorley2011" class="csl-entry" role="listitem">
Chorley, P., and Seth, A. K. (2011). <span class="nocase">Dopamine-signaled reward predictions generated by competitive excitation and inhibition in a spiking neural network model.</span> <em>Front. Comput. Neurosci.</em> 5, 21. doi:<a href="https://doi.org/10.3389/fncom.2011.00021">10.3389/fncom.2011.00021</a>.
</div>
<div id="ref-Christian2003" class="csl-entry" role="listitem">
Christian, K. M., and Thompson, R. F. (2003). <span class="nocase">Neural substrates of eyeblink conditioning: acquisition and retention.</span> <em>Learn. Mem.</em> 10, 427–55. doi:<a href="https://doi.org/10.1101/lm.59603">10.1101/lm.59603</a>.
</div>
<div id="ref-Cohen2012" class="csl-entry" role="listitem">
Cohen, M. X., Bour, L., Mantione, M., Figee, M., Vink, M., Tijssen, M. A. J., et al. (2012). <span class="nocase">Top-down-directed synchrony from medial frontal cortex to nucleus accumbens during reward anticipation.</span> <em>Hum Brain Mapp</em> 33, 246–252. doi:<a href="https://doi.org/10.1002/hbm.21195">10.1002/hbm.21195</a>.
</div>
<div id="ref-Corbit2011" class="csl-entry" role="listitem">
Corbit, L. H., and Balleine, B. W. (2011). <span class="nocase">The general and outcome-specific forms of Pavlovian-instrumental transfer are differentially mediated by the nucleus accumbens core and shell.</span> <em>J. Neurosci.</em> 31, 11786–94. doi:<a href="https://doi.org/10.1523/JNEUROSCI.2711-11.2011">10.1523/JNEUROSCI.2711-11.2011</a>.
</div>
<div id="ref-Coull2011" class="csl-entry" role="listitem">
Coull, J. T., Cheng, R.-K., and Meck, W. H. (2011). <span class="nocase">Neuroanatomical and neurochemical substrates of timing.</span> <em>Neuropsychopharmacology</em> 36, 3–25. doi:<a href="https://doi.org/10.1038/npp.2010.113">10.1038/npp.2010.113</a>.
</div>
<div id="ref-Darbaky2005" class="csl-entry" role="listitem">
Darbaky, Y., Baunez, C., Arecchi, P., Legallet, E., and Apicella, P. (2005). <span class="nocase">Reward-related neuronal activity in the subthalamic nucleus of the monkey.</span> <em>Neuroreport</em> 16, 1241–4. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/16012357">http://www.ncbi.nlm.nih.gov/pubmed/16012357</a>.
</div>
<div id="ref-Daw2006" class="csl-entry" role="listitem">
Daw, N. D., Courville, A. C., Tourtezky, D. S., and Touretzky, D. S. (2006). <span class="nocase">Representation and timing in theories of the dopamine system.</span> <em>Neural Comput.</em> 18, 1637–77. doi:<a href="https://doi.org/10.1162/neco.2006.18.7.1637">10.1162/neco.2006.18.7.1637</a>.
</div>
<div id="ref-Daw2005" class="csl-entry" role="listitem">
Daw, N. D., Niv, Y., and Dayan, P. (2005). <span class="nocase">Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control.</span> <em>Nat. Neurosci.</em> 8, 1704–11. doi:<a href="https://doi.org/10.1038/nn1560">10.1038/nn1560</a>.
</div>
<div id="ref-Day2007" class="csl-entry" role="listitem">
Day, J. J., and Carelli, R. M. (2007). <span class="nocase">The nucleus accumbens and Pavlovian reward learning.</span> <em>Neuroscientist</em> 13, 148–59. doi:<a href="https://doi.org/10.1177/1073858406295854">10.1177/1073858406295854</a>.
</div>
<div id="ref-Dayan2001" class="csl-entry" role="listitem">
Dayan, P., and Abbott, L. F. (2001). <em><span class="nocase">Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems</span></em>. The MIT Press Available at: <a href="http://dl.acm.org/citation.cfm?id=1205781">http://dl.acm.org/citation.cfm?id=1205781</a>.
</div>
<div id="ref-Deadwyler2004" class="csl-entry" role="listitem">
Deadwyler, S. A., Hayashizaki, S., Cheer, J., and Hampson, R. E. (2004). <span class="nocase">Reward, memory and substance abuse: functional neuronal circuits in the nucleus accumbens</span>. <em>Neurosci. Biobehav. Rev.</em> 27, 703–711. Available at: <a href="http://www.sciencedirect.com/science/article/pii/S0149763403001520">http://www.sciencedirect.com/science/article/pii/S0149763403001520</a>.
</div>
<div id="ref-Delgado2008" class="csl-entry" role="listitem">
Delgado, M. R., Li, J., Schiller, D., and Phelps, E. A. (2008). <span class="nocase">The role of the striatum in aversive learning and aversive prediction errors.</span> <em>Philos. Trans. R. Soc. Lond. B. Biol. Sci.</em> 363, 3787–800. doi:<a href="https://doi.org/10.1098/rstb.2008.0161">10.1098/rstb.2008.0161</a>.
</div>
<div id="ref-Diaz-Mataix2013" class="csl-entry" role="listitem">
Dı́az-Mataix, L., Tallot, L., and Doyère, V. (2013). <span class="nocase">The amygdala: A potential player in timing CS–US intervals</span>. <em>Behav. Processes</em>. Available at: <a href="http://www.sciencedirect.com/science/article/pii/S0376635713001824">http://www.sciencedirect.com/science/article/pii/S0376635713001824</a>.
</div>
<div id="ref-Dormont1998" class="csl-entry" role="listitem">
Dormont, J. F., Condé, H., and Farin, D. (1998). <span class="nocase">The role of the pedunculopontine tegmental nucleus in relation to conditioned motor performance in the cat. I. Context-dependent and reinforcement-related single unit activity.</span> <em>Exp. Brain Res.</em> 121, 401–10. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/9746146">http://www.ncbi.nlm.nih.gov/pubmed/9746146</a>.
</div>
<div id="ref-Doyere1997" class="csl-entry" role="listitem">
Doyere, V., Srebro, B., and Laroche, S. (1997). <span class="nocase">Heterosynaptic LTD and Depotentiation in the Medial Perforant Path of the Dentate Gyrus in the Freely Moving Rat</span>. <em>J Neurophysiol</em> 77, 571–578. Available at: <a href="http://jn.physiology.org/content/77/2/571.long">http://jn.physiology.org/content/77/2/571.long</a>.
</div>
<div id="ref-Doyere2003" class="csl-entry" role="listitem">
Doyère, V., Schafe, G. E., Sigurdsson, T., and LeDoux, J. E. (2003). <span class="nocase">Long-term potentiation in freely moving rats reveals asymmetries in thalamic and cortical inputs to the lateral amygdala.</span> <em>Eur. J. Neurosci.</em> 17, 2703–15. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/12823477">http://www.ncbi.nlm.nih.gov/pubmed/12823477</a>.
</div>
<div id="ref-Dranias2008" class="csl-entry" role="listitem">
Dranias, M. R., Grossberg, S., and Bullock, D. (2008). <span class="nocase">Dopaminergic and non-dopaminergic value systems in conditioning and outcome-specific revaluation.</span> <em>Brain Res.</em> 1238, 239–87. doi:<a href="https://doi.org/10.1016/j.brainres.2008.07.013">10.1016/j.brainres.2008.07.013</a>.
</div>
<div id="ref-Durstewitz2004" class="csl-entry" role="listitem">
Durstewitz, D. (2004). <span class="nocase">Neural representation of interval time.</span> <em>Neuroreport</em> 15, 745–9. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/15073507">http://www.ncbi.nlm.nih.gov/pubmed/15073507</a>.
</div>
<div id="ref-Eyny2003" class="csl-entry" role="listitem">
Eyny, Y. S., and Horvitz, J. C. (2003). <span class="nocase">Opposing roles of D1 and D2 receptors in appetitive conditioning.</span> <em>J. Neurosci.</em> 23, 1584–7. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/12629161">http://www.ncbi.nlm.nih.gov/pubmed/12629161</a>.
</div>
<div id="ref-Fiala1996" class="csl-entry" role="listitem">
Fiala, J. C., Grossberg, S., and Bullock, D. (1996). <span class="nocase">Metabotropic Glutamate Receptor Activation in Cerebellar Purkinje Cells as Substrate for Adaptive Timing of the Classically Conditioned Eye-Blink Response</span>. <em>J. Neurosci.</em> 16, 3760–3774. Available at: <a href="http://www.jneurosci.org/content/16/11/3760.abstract?ijkey=697c406d79d57535ce2655e34eacf49875ac9778\&amp;keytype2=tf\_ipsecsha">http://www.jneurosci.org/content/16/11/3760.abstract?ijkey=697c406d79d57535ce2655e34eacf49875ac9778\&amp;keytype2=tf\_ipsecsha</a>.
</div>
<div id="ref-Fields2007" class="csl-entry" role="listitem">
Fields, H. L., Hjelmstad, G. O., Margolis, E. B., and Nicola, S. M. (2007). <span class="nocase">Ventral tegmental area neurons in learned appetitive behavior and positive reinforcement.</span> <em>Annu. Rev. Neurosci.</em> 30, 289–316. doi:<a href="https://doi.org/10.1146/annurev.neuro.30.051606.094341">10.1146/annurev.neuro.30.051606.094341</a>.
</div>
<div id="ref-Fino2005" class="csl-entry" role="listitem">
Fino, E., Glowinski, J., and Venance, L. (2005). <span class="nocase">Bidirectional activity-dependent plasticity at corticostriatal synapses.</span> <em>J. Neurosci.</em> 25, 11279–87. doi:<a href="https://doi.org/10.1523/JNEUROSCI.4476-05.2005">10.1523/JNEUROSCI.4476-05.2005</a>.
</div>
<div id="ref-Fiorillo2008" class="csl-entry" role="listitem">
Fiorillo, C. D., Newsome, W. T., and Schultz, W. (2008). <span class="nocase">The temporal precision of reward prediction in dopamine neurons.</span> <em>Nat. Neurosci.</em> doi:<a href="https://doi.org/10.1038/nn.2159">10.1038/nn.2159</a>.
</div>
<div id="ref-Fiorillo2003" class="csl-entry" role="listitem">
Fiorillo, C. D., Tobler, P. N., and Schultz, W. (2003). <span class="nocase">Discrete coding of reward probability and uncertainty by dopamine neurons.</span> <em>Science</em> 299, 1898–902. doi:<a href="https://doi.org/10.1126/science.1077349">10.1126/science.1077349</a>.
</div>
<div id="ref-Fudge2000" class="csl-entry" role="listitem">
Fudge, J. L., and Haber, S. N. (2000). <span class="nocase">The central nucleus of the amygdala projection to dopamine subpopulations in primates.</span> <em>Neuroscience</em> 97, 479–94. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/10828531">http://www.ncbi.nlm.nih.gov/pubmed/10828531</a>.
</div>
<div id="ref-Funahashi1993" class="csl-entry" role="listitem">
Funahashi, S., Chafee, M. V., and Goldman-Rakic, P. S. (1993). <span class="nocase">Prefrontal neuronal activity in rhesus monkeys performing a delayed anti-saccade task.</span> <em>Nature</em> 365, 753–6. doi:<a href="https://doi.org/10.1038/365753a0">10.1038/365753a0</a>.
</div>
<div id="ref-Gallistel2000" class="csl-entry" role="listitem">
Gallistel, C. R., and Gibbon, J. (2000). <span class="nocase">Time, rate, and conditioning.</span> <em>Psychol. Rev.</em> 107, 289–344. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/10789198">http://www.ncbi.nlm.nih.gov/pubmed/10789198</a>.
</div>
<div id="ref-Galtress2009" class="csl-entry" role="listitem">
Galtress, T., and Kirkpatrick, K. (2009). <span class="nocase">Reward value effects on timing in the peak procedure</span>. <em>Learn. Motiv.</em> 40, 109–131. Available at: <a href="http://www.sciencedirect.com/science/article/pii/S0023969008000258">http://www.sciencedirect.com/science/article/pii/S0023969008000258</a>.
</div>
<div id="ref-Galtress2010" class="csl-entry" role="listitem">
Galtress, T., and Kirkpatrick, K. (2010). <span class="nocase">The role of the nucleus accumbens core in impulsive choice, timing, and reward processing.</span> <em>Behav. Neurosci.</em> 124, 26–43. doi:<a href="https://doi.org/10.1037/a0018464">10.1037/a0018464</a>.
</div>
<div id="ref-Galtress2012" class="csl-entry" role="listitem">
Galtress, T., Marshall, A. T., and Kirkpatrick, K. (2012). <span class="nocase">Motivation and timing: clues for modeling the reward system.</span> <em>Behav. Processes</em> 90, 142–53. doi:<a href="https://doi.org/10.1016/j.beproc.2012.02.014">10.1016/j.beproc.2012.02.014</a>.
</div>
<div id="ref-Geisler2007" class="csl-entry" role="listitem">
Geisler, S., Derst, C., Veh, R. W., and Zahm, D. S. (2007). <span class="nocase">Glutamatergic afferents of the ventral tegmental area in the rat.</span> <em>J. Neurosci.</em> 27, 5730–43. doi:<a href="https://doi.org/10.1523/JNEUROSCI.0012-07.2007">10.1523/JNEUROSCI.0012-07.2007</a>.
</div>
<div id="ref-Geisler2008" class="csl-entry" role="listitem">
Geisler, S., and Wise, R. A. (2008). <span class="nocase">Functional implications of glutamatergic projections to the ventral tegmental area.</span> <em>Rev. Neurosci.</em> 19, 227–44. Available at: <a href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2735573\&amp;tool=pmcentrez\&amp;rendertype=abstract">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2735573\&amp;tool=pmcentrez\&amp;rendertype=abstract</a>.
</div>
<div id="ref-Goldman-Rakic1992" class="csl-entry" role="listitem">
Goldman-Rakic, P. S., Lidow, M. S., Smiley, J. F., and Williams, M. S. (1992). <span class="nocase">The anatomy of dopamine in monkey and human prefrontal cortex.</span> <em>J. Neural Transm. Suppl.</em> 36, 163–77. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/1527516">http://www.ncbi.nlm.nih.gov/pubmed/1527516</a>.
</div>
<div id="ref-Goto2005" class="csl-entry" role="listitem">
Goto, Y., and Grace, A. A. (2005). <span class="nocase">Dopaminergic modulation of limbic and cortical drive of nucleus accumbens in goal-directed behavior.</span> <em>Nat. Neurosci.</em> 8, 805–12. doi:<a href="https://doi.org/10.1038/nn1471">10.1038/nn1471</a>.
</div>
<div id="ref-Groshek2005" class="csl-entry" role="listitem">
Groshek, F., Kerfoot, E., McKenna, V., Polackwich, A. S., Gallagher, M., and Holland, P. C. (2005). <span class="nocase">Amygdala central nucleus function is necessary for learning, but not expression, of conditioned auditory orienting.</span> <em>Behav. Neurosci.</em> 119, 202–12. doi:<a href="https://doi.org/10.1037/0735-7044.119.1.202">10.1037/0735-7044.119.1.202</a>.
</div>
<div id="ref-Grossberg1989" class="csl-entry" role="listitem">
Grossberg, S., and Schmajuk, N. A. (1989). <span class="nocase">Neural dynamics of adaptive timing and temporal discrimination during associative learning</span>. <em>Neural Networks</em> 2, 79–102. Available at: <a href="http://www.sciencedirect.com/science/article/pii/0893608089900269">http://www.sciencedirect.com/science/article/pii/0893608089900269</a>.
</div>
<div id="ref-Gruber2003" class="csl-entry" role="listitem">
Gruber, A. J., Solla, S. A., Surmeier, D. J., and Houk, J. C. (2003). <span class="nocase">Modulation of striatal single units by expected reward: a spiny neuron model displaying dopamine-induced bistability.</span> <em>J. Neurophysiol.</em> 90, 1095–114. doi:<a href="https://doi.org/10.1152/jn.00618.2002">10.1152/jn.00618.2002</a>.
</div>
<div id="ref-Haber2003" class="csl-entry" role="listitem">
Haber, S. N. (2003). <a href="https://www.ncbi.nlm.nih.gov/pubmed/14729134">The primate basal ganglia: Parallel and integrative networks.</a> <em>J Chem Neuroanat</em> 26, 317–330.
</div>
<div id="ref-Haber2000" class="csl-entry" role="listitem">
Haber, S. N., Fudge, J. L., and McFarland, N. R. (2000). <span class="nocase">Striatonigrostriatal pathways in primates form an ascending spiral from the shell to the dorsolateral striatum.</span> <em>J. Neurosci.</em> 20, 2369–82. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/10704511">http://www.ncbi.nlm.nih.gov/pubmed/10704511</a>.
</div>
<div id="ref-Haber2010" class="csl-entry" role="listitem">
Haber, S. N., and Knutson, B. (2010). <span class="nocase">The reward circuit: linking primate anatomy and human imaging.</span> <em>Neuropsychopharmacology</em> 35, 4–26. doi:<a href="https://doi.org/10.1038/npp.2009.129">10.1038/npp.2009.129</a>.
</div>
<div id="ref-Hallanger1988" class="csl-entry" role="listitem">
Hallanger, A. E., and Wainer, B. H. (1988). <span class="nocase">Ascending projections from the pedunculopontine tegmental nucleus and the adjacent mesopontine tegmentum in the rat.</span> <em>J. Comp. Neurol.</em> 274, 483–515. doi:<a href="https://doi.org/10.1002/cne.902740403">10.1002/cne.902740403</a>.
</div>
<div id="ref-Hazy2010" class="csl-entry" role="listitem">
Hazy, T. E., Frank, M. J., and O’Reilly, R. C. (2010). <span class="nocase">Neural mechanisms of acquired phasic dopamine responses in learning.</span> <em>Neurosci. Biobehav. Rev.</em> 34, 701–20. doi:<a href="https://doi.org/10.1016/j.neubiorev.2009.11.019">10.1016/j.neubiorev.2009.11.019</a>.
</div>
<div id="ref-Hikosaka2008" class="csl-entry" role="listitem">
Hikosaka, O., Sesack, S. R., Lecourtier, L., and Shepard, P. D. (2008). <span class="nocase">Habenula: crossroad between the basal ganglia and the limbic system.</span> <em>J. Neurosci.</em> 28, 11825–11829. doi:<a href="https://doi.org/10.1523/JNEUROSCI.3463-08.2008">10.1523/JNEUROSCI.3463-08.2008</a>.
</div>
<div id="ref-Hochreiter1997" class="csl-entry" role="listitem">
Hochreiter, S., and Schmidhuber, J. (1997). <span class="nocase">Long short-term memory.</span> <em>Neural Comput.</em> 9, 1735–80. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/9377276">http://www.ncbi.nlm.nih.gov/pubmed/9377276</a>.
</div>
<div id="ref-Holland2004" class="csl-entry" role="listitem">
Holland, P. C., and Gallagher, M. (2004). <span class="nocase">Amygdala–frontal interactions and reward expectancy</span>. <em>Curr. Opin. Neurobiol.</em> 14, 148–155. Available at: <a href="http://www.sciencedirect.com/science/article/pii/S095943880400039X">http://www.sciencedirect.com/science/article/pii/S095943880400039X</a>.
</div>
<div id="ref-Hollerman1998" class="csl-entry" role="listitem">
Hollerman, J. R., and Schultz, W. (1998). <span class="nocase">Dopamine neurons report an error in the temporal prediction of reward during learning.</span> <em>Nat. Neurosci.</em> 1, 304–9. doi:<a href="https://doi.org/10.1038/1124">10.1038/1124</a>.
</div>
<div id="ref-Hong2008" class="csl-entry" role="listitem">
Hong, S., and Hikosaka, O. (2008). <span class="nocase">The globus pallidus sends reward-related signals to the lateral habenula.</span> <em>Neuron</em> 60, 720–9. doi:<a href="https://doi.org/10.1016/j.neuron.2008.09.035">10.1016/j.neuron.2008.09.035</a>.
</div>
<div id="ref-Hong2011" class="csl-entry" role="listitem">
Hong, S., Jhou, T. C., Smith, M., Saleem, K. S., and Hikosaka, O. (2011). <span class="nocase">Negative reward signals from the lateral habenula to dopamine neurons are mediated by rostromedial tegmental nucleus in primates.</span> <em>J. Neurosci.</em> 31, 11457–11471. Available at: <a href="http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.1384-11.2011">http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.1384-11.2011</a>.
</div>
<div id="ref-Horvitz2000" class="csl-entry" role="listitem">
Horvitz, J. C. (2000). <span class="nocase">Mesolimbocortical and nigrostriatal dopamine responses to salient non-reward events.</span> <em>Neuroscience</em> 96, 651–6. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/10727783">http://www.ncbi.nlm.nih.gov/pubmed/10727783</a>.
</div>
<div id="ref-Humphries2009" class="csl-entry" role="listitem">
Humphries, M. D., Lepora, N., Wood, R., and Gurney, K. (2009). <span class="nocase">Capturing dopaminergic modulation and bimodal membrane behaviour of striatal medium spiny neurons in accurate, reduced models.</span> <em>Front. Comput. Neurosci.</em> 3, 26. doi:<a href="https://doi.org/10.3389/neuro.10.026.2009">10.3389/neuro.10.026.2009</a>.
</div>
<div id="ref-Humphries2010" class="csl-entry" role="listitem">
Humphries, M. D., and Prescott, T. J. (2010). <span class="nocase">The ventral basal ganglia, a selection mechanism at the crossroads of space, strategy, and reward.</span> <em>Prog. Neurobiol.</em> 90, 385–417. doi:<a href="https://doi.org/10.1016/j.pneurobio.2009.11.003">10.1016/j.pneurobio.2009.11.003</a>.
</div>
<div id="ref-Ito2006" class="csl-entry" role="listitem">
Ito, R., Robbins, T. W., McNaughton, B. L., and Everitt, B. J. (2006). <span class="nocase">Selective excitotoxic lesions of the hippocampus and basolateral amygdala have dissociable effects on appetitive cue and place conditioning based on path integration in a novel Y-maze procedure.</span> <em>Eur. J. Neurosci.</em> 23, 3071–80. doi:<a href="https://doi.org/10.1111/j.1460-9568.2006.04883.x">10.1111/j.1460-9568.2006.04883.x</a>.
</div>
<div id="ref-Izhikevich2007" class="csl-entry" role="listitem">
Izhikevich, E. M. (2007). <span class="nocase">Solving the distal reward problem through linkage of STDP and dopamine signaling.</span> <em>Cereb. Cortex</em> 17, 2443–52. doi:<a href="https://doi.org/10.1093/cercor/bhl152">10.1093/cercor/bhl152</a>.
</div>
<div id="ref-Jhou2009" class="csl-entry" role="listitem">
Jhou, T. C., Fields, H. L., Baxter, M. G., Saper, C. B., and Holland, P. C. (2009). <span class="nocase">The rostromedial tegmental nucleus (RMTg), a GABAergic afferent to midbrain dopamine neurons, encodes aversive stimuli and inhibits motor responses.</span> <em>Neuron</em> 61, 786–800. doi:<a href="https://doi.org/10.1016/j.neuron.2009.02.001">10.1016/j.neuron.2009.02.001</a>.
</div>
<div id="ref-Judice-Daher2013" class="csl-entry" role="listitem">
Judice-Daher, D. M., and Bueno, J. L. O. (2013). <span class="nocase">Lesions of the nucleus accumbens disrupt reinforcement omission effects in rats.</span> <em>Behav. Brain Res.</em> 252, 439–43. doi:<a href="https://doi.org/10.1016/j.bbr.2013.06.028">10.1016/j.bbr.2013.06.028</a>.
</div>
<div id="ref-Kirkpatrick2013" class="csl-entry" role="listitem">
Kirkpatrick, K. (2013). <span class="nocase">Interactions of timing and prediction error learning.</span> <em>Behav. Processes</em>. doi:<a href="https://doi.org/10.1016/j.beproc.2013.08.005">10.1016/j.beproc.2013.08.005</a>.
</div>
<div id="ref-Kobayashi2007" class="csl-entry" role="listitem">
Kobayashi, Y., and Okada, K.-I. (2007). <span class="nocase">Reward prediction error computation in the pedunculopontine tegmental nucleus neurons.</span> <em>Ann. N. Y. Acad. Sci.</em> 1104, 310–23. doi:<a href="https://doi.org/10.1196/annals.1390.003">10.1196/annals.1390.003</a>.
</div>
<div id="ref-Komura2001" class="csl-entry" role="listitem">
Komura, Y., Tamura, R., Uwano, T., Nishijo, H., Kaga, K., and Ono, T. (2001). <span class="nocase">Retrospective and prospective coding for predicted reward in the sensory thalamus.</span> <em>Nature</em> 412, 546–9. doi:<a href="https://doi.org/10.1038/35087595">10.1038/35087595</a>.
</div>
<div id="ref-Koo2004" class="csl-entry" role="listitem">
Koo, J. W., Han, J.-S., and Kim, J. J. (2004). <span class="nocase">Selective neurotoxic lesions of basolateral and central nuclei of the amygdala produce differential effects on fear conditioning.</span> <em>J. Neurosci.</em> 24, 7654–62. doi:<a href="https://doi.org/10.1523/JNEUROSCI.1644-04.2004">10.1523/JNEUROSCI.1644-04.2004</a>.
</div>
<div id="ref-Krichmar2013" class="csl-entry" role="listitem">
Krichmar, J. L. (2013). <span class="nocase">A neurorobotic platform to test the influence of neuromodulatory signaling on anxious and curious behavior.</span> <em>Front. Neurorobot.</em> 7, 1. doi:<a href="https://doi.org/10.3389/fnbot.2013.00001">10.3389/fnbot.2013.00001</a>.
</div>
<div id="ref-Lammel2012" class="csl-entry" role="listitem">
Lammel, S., Lim, B. K., Ran, C., Huang, K. W., Betley, M. J., Tye, K. M., et al. (2012). <span class="nocase">Input-specific control of reward and aversion in the ventral tegmental area.</span> <em>Nature</em> 491, 212–7. doi:<a href="https://doi.org/10.1038/nature11527">10.1038/nature11527</a>.
</div>
<div id="ref-Lardeux2009" class="csl-entry" role="listitem">
Lardeux, S., Pernaud, R., Paleressompoulle, D., and Baunez, C. (2009). <span class="nocase">Beyond the reward pathway: coding reward magnitude and error in the rat subthalamic nucleus.</span> <em>J. Neurophysiol.</em> 102, 2526–37. doi:<a href="https://doi.org/10.1152/jn.91009.2008">10.1152/jn.91009.2008</a>.
</div>
<div id="ref-Lavezzi2011" class="csl-entry" role="listitem">
Lavezzi, H. N., and Zahm, D. S. (2011). <span class="nocase">The mesopontine rostromedial tegmental nucleus: an integrative modulator of the reward system.</span> <em>Basal Ganglia</em> 1, 191–200. doi:<a href="https://doi.org/10.1016/j.baga.2011.08.003">10.1016/j.baga.2011.08.003</a>.
</div>
<div id="ref-LeDoux2000" class="csl-entry" role="listitem">
LeDoux, J. E. (2000). <span class="nocase">Emotion circuits in the brain.</span> <em>Annu. Rev. Neurosci.</em> 23, 155–84. doi:<a href="https://doi.org/10.1146/annurev.neuro.23.1.155">10.1146/annurev.neuro.23.1.155</a>.
</div>
<div id="ref-Lee2011" class="csl-entry" role="listitem">
Lee, H. J., Wheeler, D. S., and Holland, P. C. (2011). <span class="nocase">Interactions between amygdala central nucleus and the ventral tegmental area in the acquisition of conditioned cue-directed behavior in rats.</span> <em>Eur. J. Neurosci.</em> 33, 1876–84. doi:<a href="https://doi.org/10.1111/j.1460-9568.2011.07680.x">10.1111/j.1460-9568.2011.07680.x</a>.
</div>
<div id="ref-Leung1993" class="csl-entry" role="listitem">
Leung, L. S., and Yim, C. Y. (1993). <span class="nocase">Rhythmic delta-frequency activities in the nucleus accumbens of anesthetized and freely moving rats.</span> <em>Can. J. Physiol. Pharmacol.</em> 71, 311–20. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/8104675">http://www.ncbi.nlm.nih.gov/pubmed/8104675</a>.
</div>
<div id="ref-Ljungberg1992" class="csl-entry" role="listitem">
Ljungberg, T., Apicella, P., and Schultz, W. (1992). <span class="nocase">Responses of monkey dopamine neurons during learning of behavioral reactions.</span> <em>J. Neurophysiol.</em> 67, 145–63. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/1552316">http://www.ncbi.nlm.nih.gov/pubmed/1552316</a>.
</div>
<div id="ref-Lokwan1999" class="csl-entry" role="listitem">
Lokwan, S. J., Overton, P. G., Berry, M. S., and Clark, D. (1999). <span class="nocase">Stimulation of the pedunculopontine tegmental nucleus in the rat produces burst firing in A9 dopaminergic neurons.</span> <em>Neuroscience</em> 92, 245–54. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/10392847">http://www.ncbi.nlm.nih.gov/pubmed/10392847</a>.
</div>
<div id="ref-Ludvig2007" class="csl-entry" role="listitem">
Ludvig, E. A., Conover, K., and Shizgal, P. (2007). <span class="nocase">The effects of reinforcer magnitude on timing in rats.</span> <em>J. Exp. Anal. Behav.</em> 87, 201–18. Available at: <a href="http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1832167\&amp;tool=pmcentrez\&amp;rendertype=abstract">http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1832167\&amp;tool=pmcentrez\&amp;rendertype=abstract</a>.
</div>
<div id="ref-Ludvig2008" class="csl-entry" role="listitem">
Ludvig, E. A., Sutton, R. S., and Kehoe, E. J. (2008). <span class="nocase">Stimulus representation and the timing of reward-prediction errors in models of the dopamine system.</span> <em>Neural Comput.</em> 20, 3034–54. doi:<a href="https://doi.org/10.1162/neco.2008.11-07-654">10.1162/neco.2008.11-07-654</a>.
</div>
<div id="ref-Ludvig2009" class="csl-entry" role="listitem">
Ludvig, E. A., Sutton, R. S., Verbeek, E., and Kehoe, E. J. (2009). <span class="nocase">A computational model of hippocampal function in trace conditioning</span>. <em>Adv. Neural Inf. Process. Syst.</em> 21, 993—–1000.
</div>
<div id="ref-Lustig2005" class="csl-entry" role="listitem">
Lustig, C., Matell, M. S., and Meck, W. H. (2005). <span class="nocase">Not "just" a coincidence: Frontal-striatal interactions in working memory and interval timing</span>. <em>Memory</em> 3/4, 441–448. Available at: <a href="http://www.bibsonomy.org/bibtex/2ccb5a59033ebd0fad86dc4267f1547dc/brian.mingus">http://www.bibsonomy.org/bibtex/2ccb5a59033ebd0fad86dc4267f1547dc/brian.mingus</a>.
</div>
<div id="ref-Luzardo2013" class="csl-entry" role="listitem">
Luzardo, A., Ludvig, E. A., and Rivest, F. (2013). <span class="nocase">An adaptive drift-diffusion model of interval timing dynamics</span>. <em>Behav. Processes</em> 95, 90–99. Available at: <a href="http://www.sciencedirect.com/science/article/pii/S0376635713000247">http://www.sciencedirect.com/science/article/pii/S0376635713000247</a>.
</div>
<div id="ref-Maren2004" class="csl-entry" role="listitem">
Maren, S., and Quirk, G. J. (2004). <span class="nocase">Neuronal signalling of fear memory.</span> <em>Nat. Rev. Neurosci.</em> 5, 844–52. doi:<a href="https://doi.org/10.1038/nrn1535">10.1038/nrn1535</a>.
</div>
<div id="ref-Martin-Soelch2007" class="csl-entry" role="listitem">
Martin-Soelch, C., Linthicum, J., and Ernst, M. (2007). <span class="nocase">Appetitive conditioning: neural bases and implications for psychopathology.</span> <em>Neurosci. Biobehav. Rev.</em> 31, 426–40. doi:<a href="https://doi.org/10.1016/j.neubiorev.2006.11.002">10.1016/j.neubiorev.2006.11.002</a>.
</div>
<div id="ref-Matell2000" class="csl-entry" role="listitem">
Matell, M. S., and Meck, W. H. (2000). <span class="nocase">Neuropsychological mechanisms of interval timing behavior</span>. <em>BioEssays</em> 22, 94–103. Available at: <a href="http://www.bibsonomy.org/bibtex/23ff219ef9d6fd6b6214587ad1254f7ed/brian.mingus">http://www.bibsonomy.org/bibtex/23ff219ef9d6fd6b6214587ad1254f7ed/brian.mingus</a>.
</div>
<div id="ref-Matell2004" class="csl-entry" role="listitem">
Matell, M. S., and Meck, W. H. (2004). <span class="nocase">Cortico-striatal circuits and interval timing: coincidence detection of oscillatory processes</span>. <em>Cogn. Brain Res.</em> 21, 139–170. Available at: <a href="http://www.sciencedirect.com/science/article/pii/S0926641004001697">http://www.sciencedirect.com/science/article/pii/S0926641004001697</a>.
</div>
<div id="ref-Matsumoto2007" class="csl-entry" role="listitem">
Matsumoto, M., and Hikosaka, O. (2007). <span class="nocase">Lateral habenula as a source of negative reward signals in dopamine neurons.</span> <em>Nature</em> 447, 1111–5. doi:<a href="https://doi.org/10.1038/nature05860">10.1038/nature05860</a>.
</div>
<div id="ref-Matsumoto2009" class="csl-entry" role="listitem">
Matsumoto, M., and Hikosaka, O. (2009). <span class="nocase">Two types of dopamine neuron distinctly convey positive and negative motivational signals.</span> <em>Nature</em> 459, 837–41. doi:<a href="https://doi.org/10.1038/nature08028">10.1038/nature08028</a>.
</div>
<div id="ref-McClure2003" class="csl-entry" role="listitem">
McClure, S. M., Berns, G. S., and Montague, P. R. (2003). <span class="nocase">Temporal prediction errors in a passive learning task activate human striatum.</span> <em>Neuron</em> 38, 339–46. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/12718866">http://www.ncbi.nlm.nih.gov/pubmed/12718866</a>.
</div>
<div id="ref-McDannald2004" class="csl-entry" role="listitem">
McDannald, M., Kerfoot, E., Gallagher, M., and Holland, P. C. (2004). <span class="nocase">Amygdala central nucleus function is necessary for learning but not expression of conditioned visual orienting.</span> <em>Eur. J. Neurosci.</em> 20, 240–8. doi:<a href="https://doi.org/10.1111/j.0953-816X.2004.03458.x">10.1111/j.0953-816X.2004.03458.x</a>.
</div>
<div id="ref-McGinty2009" class="csl-entry" role="listitem">
McGinty, V. B., and Grace, A. A. (2009). <span class="nocase">Activity-dependent depression of medial prefrontal cortex inputs to accumbens neurons by the basolateral amygdala.</span> <em>Neuroscience</em> 162, 1429–36. doi:<a href="https://doi.org/10.1016/j.neuroscience.2009.05.028">10.1016/j.neuroscience.2009.05.028</a>.
</div>
<div id="ref-Meck2006" class="csl-entry" role="listitem">
Meck, W. H. (2006). <span class="nocase">Neuroanatomical localization of an internal clock: a functional link between mesolimbic, nigrostriatal, and mesocortical dopaminergic systems.</span> <em>Brain Res.</em> 1109, 93–107. doi:<a href="https://doi.org/10.1016/j.brainres.2006.06.031">10.1016/j.brainres.2006.06.031</a>.
</div>
<div id="ref-Mirenowicz1994" class="csl-entry" role="listitem">
Mirenowicz, J., and Schultz, W. (1994). <a href="https://www.ncbi.nlm.nih.gov/pubmed/7983508">Importance of unpredictability for reward responses in primate dopamine neurons.</a> <em>J Neurophysiol</em> 72, 1024–1027.
</div>
<div id="ref-Montague1996" class="csl-entry" role="listitem">
Montague, P. R., Dayan, P., and Sejnowski, T. J. (1996). <span class="nocase">A framework for mesencephalic dopamine systems based on predictive Hebbian learning.</span> <em>J. Neurosci.</em> 16, 1936–47. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/8774460">http://www.ncbi.nlm.nih.gov/pubmed/8774460</a>.
</div>
<div id="ref-Morris2006" class="csl-entry" role="listitem">
Morris, R. W., and Bouton, M. E. (2006). <span class="nocase">Effect of unconditioned stimulus magnitude on the emergence of conditioned responding.</span> <em>J. Exp. Psychol. Anim. Behav. Process.</em> 32, 371–85. doi:<a href="https://doi.org/10.1037/0097-7403.32.4.371">10.1037/0097-7403.32.4.371</a>.
</div>
<div id="ref-Muller2007" class="csl-entry" role="listitem">
Muller, J. F., Mascagni, F., and McDonald, A. J. (2007). <span class="nocase">Postsynaptic targets of somatostatin-containing interneurons in the rat basolateral amygdala</span>. <em>J. Comp. Neurol.</em> 500, 513–529. doi:<a href="https://doi.org/10.1002/cne.21185">10.1002/cne.21185</a>.
</div>
<div id="ref-Murray2007" class="csl-entry" role="listitem">
Murray, E. A. (2007). <span class="nocase">The amygdala, reward and emotion.</span> <em>Trends Cogn. Sci.</em> 11, 489–97. doi:<a href="https://doi.org/10.1016/j.tics.2007.08.013">10.1016/j.tics.2007.08.013</a>.
</div>
<div id="ref-Nakamura1986" class="csl-entry" role="listitem">
Nakamura, K., and Ono, T. (1986). <span class="nocase">Lateral hypothalamus neuron involvement in integration of natural and artificial rewards and cue signals.</span> <em>J. Neurophysiol.</em> 55, 163–81. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/3512788">http://www.ncbi.nlm.nih.gov/pubmed/3512788</a>.
</div>
<div id="ref-Nicola2007" class="csl-entry" role="listitem">
Nicola, S. M. (2007). <span class="nocase">The nucleus accumbens as part of a basal ganglia action selection circuit.</span> <em>Psychopharmacology (Berl).</em> 191, 521–50. doi:<a href="https://doi.org/10.1007/s00213-006-0510-4">10.1007/s00213-006-0510-4</a>.
</div>
<div id="ref-Nishijo2008" class="csl-entry" role="listitem">
Nishijo, H., Hori, E., Tazumi, T., and Ono, T. (2008). <span class="nocase">Neural correlates to both emotion and cognitive functions in the monkey amygdala.</span> <em>Behav. Brain Res.</em> 188, 14–23. doi:<a href="https://doi.org/10.1016/j.bbr.2007.10.013">10.1016/j.bbr.2007.10.013</a>.
</div>
<div id="ref-Nishijo2000" class="csl-entry" role="listitem">
Nishijo, H., Ono, T., Uwano, T., Kondoh, T., and Torii, K. (2000). <span class="nocase">Hypothalamic and amygdalar neuronal responses to various tastant solutions during ingestive behavior in rats.</span> <em>J. Nutr.</em> 130, 954S–9S. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/10736360">http://www.ncbi.nlm.nih.gov/pubmed/10736360</a>.
</div>
<div id="ref-Niv2007" class="csl-entry" role="listitem">
Niv, Y., Daw, N. D., Joel, D., and Dayan, P. (2007). <span class="nocase">Tonic dopamine: opportunity costs and the control of response vigor.</span> <em>Psychopharmacology (Berl).</em> 191, 507–20. doi:<a href="https://doi.org/10.1007/s00213-006-0502-4">10.1007/s00213-006-0502-4</a>.
</div>
<div id="ref-ODoherty2003" class="csl-entry" role="listitem">
O’Doherty, J. P., Dayan, P., Friston, K., Critchley, H., and Dolan, R. J. (2003). <span class="nocase">Temporal difference models and reward-related learning in the human brain.</span> <em>Neuron</em> 38, 329–37. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/12718865">http://www.ncbi.nlm.nih.gov/pubmed/12718865</a>.
</div>
<div id="ref-ODonnell1995" class="csl-entry" role="listitem">
O’Donnell, P., and Grace, A. A. (1995). <span class="nocase">Synaptic interactions among excitatory afferents to nucleus accumbens neurons: hippocampal gating of prefrontal cortical input.</span> <em>J. Neurosci.</em> 15, 3622–39. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/7751934">http://www.ncbi.nlm.nih.gov/pubmed/7751934</a>.
</div>
<div id="ref-OReilly2006" class="csl-entry" role="listitem">
O’Reilly, R. C., and Frank, M. J. (2006). <span class="nocase">Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia.</span> <em>Neural Comput.</em> 18, 283–328. doi:<a href="https://doi.org/10.1162/089976606775093909">10.1162/089976606775093909</a>.
</div>
<div id="ref-OReilly2007" class="csl-entry" role="listitem">
O’Reilly, R. C., Frank, M. J., Hazy, T. E., and Watz, B. (2007). <span>PVLV:</span> The primary value and learned value pavlovian learning algorithm. <em>Behav Neurosci</em> 121, 31–49.
</div>
<div id="ref-Oja1982" class="csl-entry" role="listitem">
Oja, E. (1982). <span class="nocase">A simplified neuron model as a principal component analyzer.</span> <em>J. Math. Biol.</em> 15, 267–73. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/7153672">http://www.ncbi.nlm.nih.gov/pubmed/7153672</a>.
</div>
<div id="ref-Ono1995" class="csl-entry" role="listitem">
Ono, T., Nishijo, H., and Uwano, T. (1995). <span class="nocase">Amygdala role in conditioned associative learning</span>. <em>Prog. Neurobiol.</em> 46, 401–422. Available at: <a href="http://www.sciencedirect.com/science/article/pii/030100829500008J">http://www.sciencedirect.com/science/article/pii/030100829500008J</a>.
</div>
<div id="ref-Oprisan2011" class="csl-entry" role="listitem">
Oprisan, S. A., and Buhusi, C. V. (2011). <span class="nocase">Modeling pharmacological clock and memory patterns of interval timing in a striatal beat-frequency model with realistic, noisy neurons.</span> <em>Front. Integr. Neurosci.</em> 5, 52. doi:<a href="https://doi.org/10.3389/fnint.2011.00052">10.3389/fnint.2011.00052</a>.
</div>
<div id="ref-Pan2005a" class="csl-entry" role="listitem">
Pan, W.-X., and Hyland, B. I. (2005). <span class="nocase">Pedunculopontine tegmental nucleus controls conditioned responses of midbrain dopamine neurons in behaving rats.</span> <em>J. Neurosci.</em> 25, 4725–32. doi:<a href="https://doi.org/10.1523/JNEUROSCI.0277-05.2005">10.1523/JNEUROSCI.0277-05.2005</a>.
</div>
<div id="ref-Pan2005" class="csl-entry" role="listitem">
Pan, W.-X., Schmidt, R., Wickens, J. R., and Hyland, B. I. (2005). Dopamine cells respond to predicted events during classical conditioning: Evidence for eligibility traces in the reward-learning network. <em>J Neurosci</em> 25, 6235–6242. doi:<a href="https://doi.org/10.1523/JNEUROSCI.1478-05.2005">10.1523/JNEUROSCI.1478-05.2005</a>.
</div>
<div id="ref-Pape2010" class="csl-entry" role="listitem">
Pape, H.-C., and Pare, D. (2010). <span class="nocase">Plastic synaptic networks of the amygdala for the acquisition, expression, and extinction of conditioned fear.</span> <em>Physiol. Rev.</em> 90, 419–63. doi:<a href="https://doi.org/10.1152/physrev.00037.2009">10.1152/physrev.00037.2009</a>.
</div>
<div id="ref-Rao2010" class="csl-entry" role="listitem">
Rao, R. P. N. (2010). <span class="nocase">Decision making under uncertainty: a neural model based on partially observable markov decision processes.</span> <em>Front. Comput. Neurosci.</em> 4, 146. doi:<a href="https://doi.org/10.3389/fncom.2010.00146">10.3389/fncom.2010.00146</a>.
</div>
<div id="ref-Raybuck2013" class="csl-entry" role="listitem">
Raybuck, J. D., and Lattal, K. M. (2013). <span class="nocase">Bridging the interval: Theory and Neurobiology of Trace Conditioning.</span> <em>Behav. Processes</em>. doi:<a href="https://doi.org/10.1016/j.beproc.2013.08.016">10.1016/j.beproc.2013.08.016</a>.
</div>
<div id="ref-Redgrave2008" class="csl-entry" role="listitem">
Redgrave, P., Gurney, K., and Reynolds, J. (2008). <span class="nocase">What is reinforced by phasic dopamine signals?</span> <em>Brain Res. Rev.</em> 58, 322–39. doi:<a href="https://doi.org/10.1016/j.brainresrev.2007.10.007">10.1016/j.brainresrev.2007.10.007</a>.
</div>
<div id="ref-Reutimann2004" class="csl-entry" role="listitem">
Reutimann, J., Yakovlev, V., Fusi, S., and Senn, W. (2004). <span class="nocase">Climbing neuronal activity as an event-based cortical representation of time.</span> <em>J. Neurosci.</em> 24, 3295–303. doi:<a href="https://doi.org/10.1523/JNEUROSCI.4098-03.2004">10.1523/JNEUROSCI.4098-03.2004</a>.
</div>
<div id="ref-Reynolds2002" class="csl-entry" role="listitem">
Reynolds, J. N. J., and Wickens, J. R. (2002). <span class="nocase">Dopamine-dependent plasticity of corticostriatal synapses</span>. <em>Neural Networks</em> 15, 507–521. Available at: <a href="http://www.sciencedirect.com/science/article/pii/S089360800200045X">http://www.sciencedirect.com/science/article/pii/S089360800200045X</a>.
</div>
<div id="ref-Rivest2010" class="csl-entry" role="listitem">
Rivest, F., Kalaska, J. F., and Bengio, Y. (2010). <span class="nocase">Alternative time representation in dopamine models.</span> <em>J. Comput. Neurosci.</em> 28, 107–30. doi:<a href="https://doi.org/10.1007/s10827-009-0191-1">10.1007/s10827-009-0191-1</a>.
</div>
<div id="ref-Rivest2013" class="csl-entry" role="listitem">
Rivest, F., Kalaska, J. F., and Bengio, Y. (2013). <span class="nocase">Conditioning and time representation in long short-term memory networks.</span> <em>Biol. Cybern.</em> doi:<a href="https://doi.org/10.1007/s00422-013-0575-1">10.1007/s00422-013-0575-1</a>.
</div>
<div id="ref-Robbins1996" class="csl-entry" role="listitem">
Robbins, T. W., and Everitt, B. J. (1996). <span class="nocase">Neurobehavioural mechanisms of reward and motivation.</span> <em>Curr. Opin. Neurobiol.</em> 6, 228–36. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/8725965">http://www.ncbi.nlm.nih.gov/pubmed/8725965</a>.
</div>
<div id="ref-Rose2009" class="csl-entry" role="listitem">
Rose, J., Schmidt, R., Grabemann, M., and Güntürkün, O. (2009). <span class="nocase">Theory meets pigeons: the influence of reward-magnitude on discrimination-learning.</span> <em>Behav. Brain Res.</em> 198, 125–9. doi:<a href="https://doi.org/10.1016/j.bbr.2008.10.038">10.1016/j.bbr.2008.10.038</a>.
</div>
<div id="ref-Sah2003" class="csl-entry" role="listitem">
Sah, P., Faber, E. S. L., Lopez De Armentia, M., and Power, J. (2003). <span class="nocase">The amygdaloid complex: anatomy and physiology.</span> <em>Physiol. Rev.</em> 83, 803–34. doi:<a href="https://doi.org/10.1152/physrev.00002.2003">10.1152/physrev.00002.2003</a>.
</div>
<div id="ref-Samejima2007" class="csl-entry" role="listitem">
Samejima, K., and Doya, K. (2007). <span class="nocase">Multiple representations of belief states and action values in corticobasal ganglia loops.</span> <em>Ann. N. Y. Acad. Sci.</em> 1104, 213–28. doi:<a href="https://doi.org/10.1196/annals.1390.024">10.1196/annals.1390.024</a>.
</div>
<div id="ref-Schroll2012" class="csl-entry" role="listitem">
Schroll, H., Vitay, J., and Hamker, F. H. (2012). <span class="nocase">Working memory and response selection: a computational account of interactions among cortico-basalganglio-thalamic loops.</span> <em>Neural Netw.</em> 26, 59–74. doi:<a href="https://doi.org/10.1016/j.neunet.2011.10.008">10.1016/j.neunet.2011.10.008</a>.
</div>
<div id="ref-Schultz1993" class="csl-entry" role="listitem">
Schultz, W., Apicella, P., and Ljungberg, T. (1993). <span class="nocase">Responses of monkey dopamine neurons to reward and conditioned stimuli during successive steps of learning a delayed response task.</span> <em>J. Neurosci.</em> 13, 900–13. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/8441015">http://www.ncbi.nlm.nih.gov/pubmed/8441015</a>.
</div>
<div id="ref-Schultz1992" class="csl-entry" role="listitem">
Schultz, W., Apicella, P., Scarnati, E., and Ljungberg, T. (1992). <span class="nocase">Neuronal activity in monkey ventral striatum related to the expectation of reward.</span> <em>J. Neurosci.</em> 12, 4595–610. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/1464759">http://www.ncbi.nlm.nih.gov/pubmed/1464759</a>.
</div>
<div id="ref-Schultz1997" class="csl-entry" role="listitem">
Schultz, W., Dayan, P., and Montague, P. R. (1997). <span class="nocase">A neural substrate of prediction and reward.</span> <em>Science</em> 275, 1593–9. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/9054347">http://www.ncbi.nlm.nih.gov/pubmed/9054347</a>.
</div>
<div id="ref-Seamans2004" class="csl-entry" role="listitem">
Seamans, J. K., and Yang, C. R. (2004). <span class="nocase">The principal features and mechanisms of dopamine modulation in the prefrontal cortex.</span> <em>Prog. Neurobiol.</em> 74, 1–58. doi:<a href="https://doi.org/10.1016/j.pneurobio.2004.05.006">10.1016/j.pneurobio.2004.05.006</a>.
</div>
<div id="ref-Semba1992" class="csl-entry" role="listitem">
Semba, K., and Fibiger, H. C. (1992). <span class="nocase">Afferent connections of the laterodorsal and the pedunculopontine tegmental nuclei in the rat: a retro- and antero-grade transport and immunohistochemical study.</span> <em>J. Comp. Neurol.</em> 323, 387–410. doi:<a href="https://doi.org/10.1002/cne.903230307">10.1002/cne.903230307</a>.
</div>
<div id="ref-Sesack2010" class="csl-entry" role="listitem">
Sesack, S. R., and Grace, A. A. (2010). <span class="nocase">Cortico-Basal Ganglia reward network: microcircuitry.</span> <em>Neuropsychopharmacology</em> 35, 27–47. doi:<a href="https://doi.org/10.1038/npp.2009.93">10.1038/npp.2009.93</a>.
</div>
<div id="ref-Shen2008" class="csl-entry" role="listitem">
Shen, W., Flajolet, M., Greengard, P., and Surmeier, D. J. (2008). <span class="nocase">Dichotomous dopaminergic control of striatal synaptic plasticity.</span> <em>Science</em> 321, 848–51. doi:<a href="https://doi.org/10.1126/science.1160575">10.1126/science.1160575</a>.
</div>
<div id="ref-Simen2011" class="csl-entry" role="listitem">
Simen, P., Balci, F., Souza, L. de, Cohen, J. D., and Holmes, P. (2011). <span class="nocase">A model of interval timing by neural integration.</span> <em>J. Neurosci.</em> 31, 9238–53. doi:<a href="https://doi.org/10.1523/JNEUROSCI.3121-10.2011">10.1523/JNEUROSCI.3121-10.2011</a>.
</div>
<div id="ref-Singh2011" class="csl-entry" role="listitem">
Singh, T., McDannald, M. A., Takahashi, Y. K., Haney, R. Z., Cooch, N. K., Lucantonio, F., et al. (2011). <span class="nocase">The role of the nucleus accumbens in knowing when to respond.</span> <em>Learn. Mem.</em> 18, 85–7. doi:<a href="https://doi.org/10.1101/lm.2008111">10.1101/lm.2008111</a>.
</div>
<div id="ref-Smith2009" class="csl-entry" role="listitem">
Smith, K. S., Tindell, A. J., Aldridge, J. W., and Berridge, K. C. (2009). <span class="nocase">Ventral pallidum roles in reward and motivation.</span> <em>Behav. Brain Res.</em> 196, 155–167. doi:<a href="https://doi.org/10.1016/j.bbr.2008.09.038">10.1016/j.bbr.2008.09.038</a>.
</div>
<div id="ref-Sporns2002" class="csl-entry" role="listitem">
Sporns, O., and Alexander, W. H. (2002). <span class="nocase">Neuromodulation and plasticity in an autonomous robot.</span> <em>Neural Netw.</em> 15, 761–74. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/12371525">http://www.ncbi.nlm.nih.gov/pubmed/12371525</a>.
</div>
<div id="ref-Stopper2011" class="csl-entry" role="listitem">
Stopper, C. M., and Floresco, S. B. (2011). <span class="nocase">Contributions of the nucleus accumbens and its subregions to different aspects of risk-based decision making.</span> <em>Cogn. Affect. Behav. Neurosci.</em> 11, 97–112. doi:<a href="https://doi.org/10.3758/s13415-010-0015-9">10.3758/s13415-010-0015-9</a>.
</div>
<div id="ref-Suri1999" class="csl-entry" role="listitem">
Suri, R. E., and Schultz, W. (1999). A neural network model with dopamine-like reinforcement signal that learns a spatial delayed response task. <em>Neuroscience</em> 91, 871–90.
</div>
<div id="ref-Suri2001" class="csl-entry" role="listitem">
Suri, R. E., and Schultz, W. (2001). <span class="nocase">Temporal difference model reproduces anticipatory neural activity.</span> <em>Neural Comput.</em> 13, 841–62. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/11255572">http://www.ncbi.nlm.nih.gov/pubmed/11255572</a>.
</div>
<div id="ref-Sutton1981" class="csl-entry" role="listitem">
Sutton, R. S., and Barto, A. G. (1981). <span class="nocase">Toward a modern theory of adaptive networks: expectation and prediction.</span> <em>Psychol. Rev.</em> 88, 135–70. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/7291377">http://www.ncbi.nlm.nih.gov/pubmed/7291377</a>.
</div>
<div id="ref-Sutton1998" class="csl-entry" role="listitem">
Sutton, R. S., and Barto, A. G. (1998). <em><span class="nocase">Reinforcement learning: An introduction</span></em>. MIT press.
</div>
<div id="ref-Tachibana2012" class="csl-entry" role="listitem">
Tachibana, Y., and Hikosaka, O. (2012). <span class="nocase">The primate ventral pallidum encodes expected reward value and regulates motor action.</span> <em>Neuron</em> 76, 826–37. doi:<a href="https://doi.org/10.1016/j.neuron.2012.09.030">10.1016/j.neuron.2012.09.030</a>.
</div>
<div id="ref-Tan2008" class="csl-entry" role="listitem">
Tan, C. O., and Bullock, D. (2008). <span class="nocase">A local circuit model of learned striatal and dopamine cell responses under probabilistic schedules of reward.</span> <em>J. Neurosci.</em> 28, 10062–74. doi:<a href="https://doi.org/10.1523/JNEUROSCI.0259-08.2008">10.1523/JNEUROSCI.0259-08.2008</a>.
</div>
<div id="ref-Tanaka2000" class="csl-entry" role="listitem">
Tanaka, K. (2000). <span class="nocase">Mechanisms of visual object recognition studied in monkeys.</span> <em>Spat. Vis.</em> 13, 147–63. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/11198228">http://www.ncbi.nlm.nih.gov/pubmed/11198228</a>.
</div>
<div id="ref-Thompson2009" class="csl-entry" role="listitem">
Thompson, R. F., and Steinmetz, J. E. (2009). <span class="nocase">The role of the cerebellum in classical conditioning of discrete behavioral responses.</span> <em>Neuroscience</em> 162, 732–55. doi:<a href="https://doi.org/10.1016/j.neuroscience.2009.01.041">10.1016/j.neuroscience.2009.01.041</a>.
</div>
<div id="ref-Tindell2004" class="csl-entry" role="listitem">
Tindell, A. J., Berridge, K. C., and Aldridge, J. W. (2004). <span class="nocase">Ventral pallidal representation of pavlovian cues and reward: population and rate codes.</span> <em>J. Neurosci.</em> 24, 1058–69. doi:<a href="https://doi.org/10.1523/JNEUROSCI.1437-03.2004">10.1523/JNEUROSCI.1437-03.2004</a>.
</div>
<div id="ref-Tobler2005" class="csl-entry" role="listitem">
Tobler, P. N., Fiorillo, C. D., and Schultz, W. (2005). <span class="nocase">Adaptive coding of reward value by dopamine neurons.</span> <em>Science</em> 307, 1642–5. doi:<a href="https://doi.org/10.1126/science.1105370">10.1126/science.1105370</a>.
</div>
<div id="ref-Turrigiano2008" class="csl-entry" role="listitem">
Turrigiano, G. G. (2008). <span class="nocase">The self-tuning neuron: synaptic scaling of excitatory synapses.</span> <em>Cell</em> 135, 422–35. doi:<a href="https://doi.org/10.1016/j.cell.2008.10.008">10.1016/j.cell.2008.10.008</a>.
</div>
<div id="ref-Tye2010" class="csl-entry" role="listitem">
Tye, K. M., Cone, J. J., Schairer, W. W., and Janak, P. H. (2010). <span class="nocase">Amygdala neural encoding of the absence of reward during extinction.</span> <em>J. Neurosci.</em> 30, 116–25. doi:<a href="https://doi.org/10.1523/JNEUROSCI.4240-09.2010">10.1523/JNEUROSCI.4240-09.2010</a>.
</div>
<div id="ref-Usuda1998" class="csl-entry" role="listitem">
Usuda, I., Tanaka, K., and Chiba, T. (1998). <span class="nocase">Efferent projections of the nucleus accumbens in the rat with special reference to subdivision of the nucleus: biotinylated dextran amine study.</span> <em>Brain Res.</em> 797, 73–93. Available at: <a href="http://www.ncbi.nlm.nih.gov/pubmed/9630528">http://www.ncbi.nlm.nih.gov/pubmed/9630528</a>.
</div>
<div id="ref-Vitay2010" class="csl-entry" role="listitem">
Vitay, J., and Hamker, F. H. (2010). <span class="nocase">A computational model of Basal Ganglia and its role in memory retrieval in rewarded visual memory tasks.</span> <em>Front. Comput. Neurosci.</em> 4. doi:<a href="https://doi.org/10.3389/fncom.2010.00013">10.3389/fncom.2010.00013</a>.
</div>
<div id="ref-Walker2008" class="csl-entry" role="listitem">
Walker, A. G., and Steinmetz, J. E. (2008). <span class="nocase">Hippocampal lesions in rats differentially affect long- and short-trace eyeblink conditioning.</span> <em>Physiol. Behav.</em> 93, 570–8. doi:<a href="https://doi.org/10.1016/j.physbeh.2007.10.018">10.1016/j.physbeh.2007.10.018</a>.
</div>
<div id="ref-Winstanley2005" class="csl-entry" role="listitem">
Winstanley, C. A., Baunez, C., Theobald, D. E. H., and Robbins, T. W. (2005). <span class="nocase">Lesions to the subthalamic nucleus decrease impulsive choice but impair autoshaping in rats: the importance of the basal ganglia in Pavlovian conditioning and impulse control.</span> <em>Eur. J. Neurosci.</em> 21, 3107–16. doi:<a href="https://doi.org/10.1111/j.1460-9568.2005.04143.x">10.1111/j.1460-9568.2005.04143.x</a>.
</div>
<div id="ref-Wolf2005" class="csl-entry" role="listitem">
Wolf, J. A., Moyer, J. T., Lazarewicz, M. T., Contreras, D., Benoit-Marand, M., O’Donnell, P., et al. (2005). <span class="nocase">NMDA/AMPA ratio impacts state transitions and entrainment to oscillations in a computational model of the nucleus accumbens medium spiny projection neuron.</span> <em>J. Neurosci.</em> 25, 9080–95. doi:<a href="https://doi.org/10.1523/JNEUROSCI.2220-05.2005">10.1523/JNEUROSCI.2220-05.2005</a>.
</div>
<div id="ref-Wolf2004" class="csl-entry" role="listitem">
Wolf, M. E., Sun, X., Mangiavacchi, S., and Chao, S. Z. (2004). <span class="nocase">Psychomotor stimulants and neuronal plasticity.</span> <em>Neuropharmacology</em> 47 Suppl 1, 61–79. doi:<a href="https://doi.org/10.1016/j.neuropharm.2004.07.006">10.1016/j.neuropharm.2004.07.006</a>.
</div>
<div id="ref-Wu2013" class="csl-entry" role="listitem">
Wu, G.-Y., Yao, J., Hu, B., Zhang, H.-M., Li, Y.-D., Li, X., et al. (2013). <span class="nocase">Reevaluating the role of the hippocampus in delay eyeblink conditioning.</span> <em>PLoS One</em> 8, e71249. doi:<a href="https://doi.org/10.1371/journal.pone.0071249">10.1371/journal.pone.0071249</a>.
</div>
<div id="ref-Zahm1990" class="csl-entry" role="listitem">
Zahm, D. S., and Heimer, L. (1990). <span class="nocase">Two transpallidal pathways originating in the rat nucleus accumbens.</span> <em>J. Comp. Neurol.</em> 302, 437–46. doi:<a href="https://doi.org/10.1002/cne.903020302">10.1002/cne.903020302</a>.
</div>
</div>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="./4-NN.html" class="pagination-link" aria-label="Working memory and response selection: A computational account of interactions among cortico-basal ganglio-thalamic loops">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Working memory and response selection: A computational account of interactions among cortico-basal ganglio-thalamic loops</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./6-FINI.html" class="pagination-link" aria-label="ANNarchy: a code generation approach to neural simulations on parallel hardware">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">ANNarchy: a code generation approach to neural simulations on parallel hardware</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>